[32mINFO    [0m jubilant:_juju.py:237 cli: juju add-model --no-switch jubilant-7976c98f
[32mINFO    [0m jubilant:_juju.py:237 cli: juju deploy --model jubilant-7976c98f ./haproxy_amd64.charm haproxy --base ubuntu@24.04
[32mINFO    [0m jubilant:_juju.py:237 cli: juju deploy --model jubilant-7976c98f self-signed-certificates self-signed-certificates --channel 1/edge
[32mINFO    [0m jubilant:_juju.py:237 cli: juju config --model jubilant-7976c98f haproxy external-hostname=haproxy.internal
[32mINFO    [0m jubilant:_juju.py:237 cli: juju integrate --model jubilant-7976c98f haproxy:certificates self-signed-certificates:certificates
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
+ .model.name = 'jubilant-7976c98f'
+ .model.type = 'iaas'
+ .model.controller = 'github-pr-77b50-lxd'
+ .model.cloud = 'localhost'
+ .model.version = '3.6.11'
+ .model.region = 'localhost'
+ .model.model_status.current = 'available'
+ .machines['0'].juju_status.current = 'pending'
+ .machines['0'].instance_id = 'pending'
+ .machines['0'].machine_status.current = 'allocating'
+ .machines['0'].machine_status.message = 'Creating container'
+ .machines['0'].modification_status.current = 'idle'
+ .machines['0'].base.name = 'ubuntu'
+ .machines['0'].base.channel = '24.04'
+ .machines['0'].constraints = 'arch=amd64'
+ .machines['1'].juju_status.current = 'pending'
+ .machines['1'].instance_id = 'pending'
+ .machines['1'].machine_status.current = 'allocating'
+ .machines['1'].machine_status.message = 'Creating container'
+ .machines['1'].modification_status.current = 'idle'
+ .machines['1'].base.name = 'ubuntu'
+ .machines['1'].base.channel = '24.04'
+ .machines['1'].constraints = 'arch=amd64'
+ .apps['haproxy'].charm = 'local:haproxy-0'
+ .apps['haproxy'].charm_origin = 'local'
+ .apps['haproxy'].charm_name = 'haproxy'
+ .apps['haproxy'].charm_rev = 0
+ .apps['haproxy'].exposed = False
+ .apps['haproxy'].base.name = 'ubuntu'
+ .apps['haproxy'].base.channel = '24.04'
+ .apps['haproxy'].app_status.current = 'waiting'
+ .apps['haproxy'].app_status.message = 'waiting for machine'
+ .apps['haproxy'].relations['certificates'][0].related_app = 'self-signed-certificates'
+ .apps['haproxy'].relations['certificates'][0].interface = 'tls-certificates'
+ .apps['haproxy'].relations['certificates'][0].scope = 'global'
+ .apps['haproxy'].relations['haproxy-peers'][0].related_app = 'haproxy'
+ .apps['haproxy'].relations['haproxy-peers'][0].interface = 'haproxy-peers'
+ .apps['haproxy'].relations['haproxy-peers'][0].scope = 'global'
+ .apps['haproxy'].units['haproxy/0'].workload_status.current = 'waiting'
+ .apps['haproxy'].units['haproxy/0'].workload_status.message = 'waiting for machine'
+ .apps['haproxy'].units['haproxy/0'].juju_status.current = 'allocating'
+ .apps['haproxy'].units['haproxy/0'].machine = '0'
+ .apps['haproxy'].endpoint_bindings[''] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['certificates'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['cos-agent'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['ha'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['haproxy-peers'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['haproxy-route'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['haproxy-route-tcp'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['ingress'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['ingress-per-unit'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['receive-ca-certs'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['reverseproxy'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['website'] = 'alpha'
+ .apps['self-signed-certificates'].charm = 'self-signed-certificates'
+ .apps['self-signed-certificates'].charm_origin = 'charmhub'
+ .apps['self-signed-certificates'].charm_name = 'self-signed-certificates'
+ .apps['self-signed-certificates'].charm_rev = 416
+ .apps['self-signed-certificates'].exposed = False
+ .apps['self-signed-certificates'].base.name = 'ubuntu'
+ .apps['self-signed-certificates'].base.channel = '24.04'
+ .apps['self-signed-certificates'].charm_channel = '1/edge'
+ .apps['self-signed-certificates'].app_status.current = 'waiting'
+ .apps['self-signed-certificates'].app_status.message = 'waiting for machine'
+ .apps['self-signed-certificates'].relations['certificates'][0].related_app = 'haproxy'
+ .apps['self-signed-certificates'].relations['certificates'][0].interface = 'tls-certificates'
+ .apps['self-signed-certificates'].relations['certificates'][0].scope = 'global'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.current = 'waiting'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.message = 'waiting for machine'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.current = 'allocating'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].machine = '1'
+ .apps['self-signed-certificates'].endpoint_bindings[''] = 'alpha'
+ .apps['self-signed-certificates'].endpoint_bindings['certificates'] = 'alpha'
+ .apps['self-signed-certificates'].endpoint_bindings['send-ca-cert'] = 'alpha'
+ .apps['self-signed-certificates'].endpoint_bindings['tracing'] = 'alpha'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['0'].instance_id = 'pending'
- .machines['0'].machine_status.current = 'allocating'
- .machines['0'].machine_status.message = 'Creating container'
- .machines['0'].modification_status.current = 'idle'
+ .machines['0'].instance_id = 'juju-9a7f5d-0'
+ .machines['0'].machine_status.current = 'running'
+ .machines['0'].machine_status.message = 'Container started'
+ .machines['0'].modification_status.current = 'applied'
+ .machines['0'].hardware = 'arch=amd64 cores=0 mem=0M availability-zone=github-runner virt-type=container'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
+ .machines['0'].dns_name = '10.127.120.177'
+ .machines['0'].ip_addresses[0] = '10.127.120.177'
- .machines['0'].machine_status.message = 'Container started'
+ .machines['0'].machine_status.message = 'Running'
+ .apps['haproxy'].units['haproxy/0'].public_address = '10.127.120.177'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['1'].instance_id = 'pending'
- .machines['1'].machine_status.current = 'allocating'
- .machines['1'].machine_status.message = 'Creating container'
- .machines['1'].modification_status.current = 'idle'
+ .machines['1'].instance_id = 'juju-9a7f5d-1'
+ .machines['1'].machine_status.current = 'running'
+ .machines['1'].machine_status.message = 'Container started'
+ .machines['1'].modification_status.current = 'applied'
+ .machines['1'].hardware = 'arch=amd64 cores=0 mem=0M availability-zone=github-runner virt-type=container'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
+ .machines['1'].dns_name = '10.127.120.236'
+ .machines['1'].ip_addresses[0] = '10.127.120.236'
- .machines['1'].machine_status.message = 'Container started'
+ .machines['1'].machine_status.message = 'Running'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].public_address = '10.127.120.236'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['0'].juju_status.current = 'pending'
+ .machines['0'].juju_status.current = 'started'
+ .machines['0'].juju_status.version = '3.6.11'
+ .machines['0'].hostname = 'juju-9a7f5d-0'
+ .machines['0'].network_interfaces['eth0'].ip_addresses[0] = '10.127.120.177'
+ .machines['0'].network_interfaces['eth0'].mac_address = '00:16:3e:d1:17:90'
+ .machines['0'].network_interfaces['eth0'].is_up = True
+ .machines['0'].network_interfaces['eth0'].gateway = '10.127.120.1'
+ .machines['0'].network_interfaces['eth0'].space = 'alpha'
- .apps['haproxy'].app_status.message = 'waiting for machine'
+ .apps['haproxy'].app_status.message = 'agent initialising'
- .apps['haproxy'].units['haproxy/0'].workload_status.message = 'waiting for machine'
+ .apps['haproxy'].units['haproxy/0'].workload_status.message = 'agent initialising'
+ .apps['haproxy'].units['haproxy/0'].juju_status.version = '3.6.11'
+ .apps['haproxy'].units['haproxy/0'].leader = True
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['1'].juju_status.current = 'pending'
+ .machines['1'].juju_status.current = 'started'
+ .machines['1'].juju_status.version = '3.6.11'
+ .machines['1'].hostname = 'juju-9a7f5d-1'
+ .machines['1'].network_interfaces['eth0'].ip_addresses[0] = '10.127.120.236'
+ .machines['1'].network_interfaces['eth0'].mac_address = '00:16:3e:0e:c9:49'
+ .machines['1'].network_interfaces['eth0'].is_up = True
+ .machines['1'].network_interfaces['eth0'].gateway = '10.127.120.1'
+ .machines['1'].network_interfaces['eth0'].space = 'alpha'
- .apps['self-signed-certificates'].app_status.message = 'waiting for machine'
+ .apps['self-signed-certificates'].app_status.message = 'agent initialising'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.message = 'waiting for machine'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.message = 'agent initialising'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.version = '3.6.11'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].leader = True
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .apps['haproxy'].app_status.current = 'waiting'
- .apps['haproxy'].app_status.message = 'agent initialising'
+ .apps['haproxy'].app_status.current = 'maintenance'
+ .apps['haproxy'].app_status.message = 'installing charm software'
- .apps['haproxy'].units['haproxy/0'].workload_status.current = 'waiting'
- .apps['haproxy'].units['haproxy/0'].workload_status.message = 'agent initialising'
- .apps['haproxy'].units['haproxy/0'].juju_status.current = 'allocating'
+ .apps['haproxy'].units['haproxy/0'].workload_status.current = 'maintenance'
+ .apps['haproxy'].units['haproxy/0'].workload_status.message = 'installing charm software'
+ .apps['haproxy'].units['haproxy/0'].juju_status.current = 'executing'
+ .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running install hook'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .apps['self-signed-certificates'].app_status.current = 'waiting'
- .apps['self-signed-certificates'].app_status.message = 'agent initialising'
+ .apps['self-signed-certificates'].app_status.current = 'maintenance'
+ .apps['self-signed-certificates'].app_status.message = 'installing charm software'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.current = 'waiting'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.message = 'agent initialising'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.current = 'allocating'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.current = 'maintenance'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.message = 'installing charm software'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.current = 'executing'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.message = 'running install hook'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .apps['self-signed-certificates'].app_status.current = 'maintenance'
- .apps['self-signed-certificates'].app_status.message = 'installing charm software'
+ .apps['self-signed-certificates'].app_status.current = 'active'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.current = 'maintenance'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.message = 'installing charm software'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.current = 'active'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.message = 'running install hook'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.message = 'running leader-elected hook'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.message = 'running leader-elected hook'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.message = 'running config-changed hook'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .apps['haproxy'].app_status.current = 'maintenance'
- .apps['haproxy'].app_status.message = 'installing charm software'
+ .apps['haproxy'].app_status.current = 'active'
- .apps['haproxy'].units['haproxy/0'].workload_status.current = 'maintenance'
- .apps['haproxy'].units['haproxy/0'].workload_status.message = 'installing charm software'
+ .apps['haproxy'].units['haproxy/0'].workload_status.current = 'active'
+ .apps['haproxy'].units['haproxy/0'].open_ports[0] = '80/tcp'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.current = 'executing'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.message = 'running config-changed hook'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.current = 'idle'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running install hook'
+ .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running certificates-relation-created hook'
- .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.current = 'idle'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.current = 'executing'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.message = 'running certificates-relation-changed hook for haproxy/0'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running certificates-relation-created hook'
+ .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running config-changed hook'
[32mINFO    [0m jubilant:_juju.py:237 cli: juju deploy --model jubilant-7976c98f any-charm ingress-per-unit-requirer-any --channel beta --config 'src-overwrite={"any_charm.py": "# Copyright 2025 Canonical Ltd.\n# See LICENSE file for licensing details.\n# pylint: disable=duplicate-code,import-error,too-few-public-methods\n\n\"\"\"Ingress per unit requirer any charm.\"\"\"\nimport pathlib\n\nimport apt\nimport ops\nfrom any_charm_base import AnyCharmBase\nfrom ingress_per_unit import IngressPerUnitReadyForUnitEvent, IngressPerUnitRequirer\n\n\nclass AnyCharm(AnyCharmBase):\n    \"\"\"Any charm that uses the ingress per unit requirer interface.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the charm.\n\n        Args:\n            args: Positional arguments.\n            kwargs: Keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.ingress_per_unit = IngressPerUnitRequirer(\n            self, port=80, relation_name=\"require-ingress-per-unit\", strip_prefix=True\n        )\n        self.framework.observe(self.on.install, self.start_server)\n        self.framework.observe(self.ingress_per_unit.on.ready_for_unit, self._on_ingress_ready)\n\n    def start_server(self, _: ops.InstallEvent):\n        \"\"\"Start the server.\"\"\"\n        apt.update()\n        apt.add_package(package_names=\"apache2\")\n        www_dir = pathlib.Path(\"/var/www/html\")\n        file_path = www_dir / \"ok\"\n        file_path.parent.mkdir(exist_ok=True)\n        file_path.write_text(\"ok!\")\n\n    def _on_ingress_ready(self, _: IngressPerUnitReadyForUnitEvent):\n        \"\"\"Relation changed handler.\"\"\"\n        self.unit.status = ops.ActiveStatus(\"Server Ready\")\n", "ingress_per_unit.py": "# Copyright 2022 Canonical Ltd.\n# See LICENSE file for licensing details.\n\nr\"\"\"# Interface Library for ingress_per_unit.\n\nThis library wraps relation endpoints using the `ingress_per_unit` interface\nand provides a Python API for both requesting and providing per-unit\ningress.\n\n## Getting Started\n\nTo get started using the library, you just need to fetch the library using `charmcraft`.\n\n```shell\ncharmcraft fetch-lib charms.traefik_k8s.v1.ingress_per_unit\n```\n\nAdd the `jsonschema` dependency to the `requirements.txt` of your charm.\n\n```yaml\nrequires:\n    ingress:\n        interface: ingress_per_unit\n        limit: 1\n```\n\nThen, to initialise the library:\n\n```python\nfrom charms.traefik_k8s.v1.ingress_per_unit import (IngressPerUnitRequirer,\n  IngressPerUnitReadyForUnitEvent, IngressPerUnitRevokedForUnitEvent)\n\nclass SomeCharm(CharmBase):\n  def __init__(self, *args):\n    # ...\n    self.ingress_per_unit = IngressPerUnitRequirer(self, port=80)\n    # The following event is triggered when the ingress URL to be used\n    # by this unit of `SomeCharm` is ready (or changes).\n    self.framework.observe(\n        self.ingress_per_unit.on.ready_for_unit, self._on_ingress_ready\n    )\n    self.framework.observe(\n        self.ingress_per_unit.on.revoked_for_unit, self._on_ingress_revoked\n    )\n\n    def _on_ingress_ready(self, event: IngressPerUnitReadyForUnitEvent):\n        # event.url is the same as self.ingress_per_unit.url\n        logger.info(\"This unit'"'"'s ingress URL: %s\", event.url)\n\n    def _on_ingress_revoked(self, event: IngressPerUnitRevokedForUnitEvent):\n        logger.info(\"This unit no longer has ingress\")\n```\n\nIf you wish to be notified also (or instead) when another unit'"'"'s ingress changes\n(e.g. if you'"'"'re the leader and you'"'"'re doing things with your peers'"'"' ingress),\nyou can pass `listen_to = \"all-units\" | \"both\"` to `IngressPerUnitRequirer`\nand observe `self.ingress_per_unit.on.ready` and `self.ingress_per_unit.on.revoked`.\n\"\"\"\n\nimport logging\nimport socket\nimport typing\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport yaml\nfrom ops.charm import CharmBase, RelationEvent\nfrom ops.framework import (\n    EventSource,\n    Object,\n    ObjectEvents,\n    StoredDict,\n    StoredList,\n    StoredState,\n)\nfrom ops.model import Application, ModelError, Relation, Unit\n\n# The unique Charmhub library identifier, never change it\nLIBID = \"7ef06111da2945ed84f4f5d4eb5b353a\"\n\n# Increment this major API version when introducing breaking changes\nLIBAPI = 1\n\n# Increment this PATCH version before using `charmcraft publish-lib` or reset\n# to 0 if you are raising the major API version\nLIBPATCH = 20\n\nlog = logging.getLogger(__name__)\n\ntry:\n    import jsonschema\n\n    DO_VALIDATION = True\nexcept ModuleNotFoundError:\n    log.warning(\n        \"The `ingress_per_unit` library needs the `jsonschema` package to be able \"\n        \"to do runtime data validation; without it, it will still work but validation \"\n        \"will be disabled. \\n\"\n        \"It is recommended to add `jsonschema` to the '"'"'requirements.txt'"'"' of your charm, \"\n        \"which will enable this feature.\"\n    )\n    DO_VALIDATION = False\n\n# LIBRARY GLOBS\nRELATION_INTERFACE = \"ingress_per_unit\"\nDEFAULT_RELATION_NAME = RELATION_INTERFACE.replace(\"_\", \"-\")\n\nINGRESS_REQUIRES_UNIT_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"model\": {\"type\": \"string\"},\n        \"name\": {\"type\": \"string\"},\n        \"host\": {\"type\": \"string\"},\n        \"port\": {\"type\": \"string\"},\n        \"mode\": {\"type\": \"string\"},\n        \"strip-prefix\": {\"type\": \"string\"},\n        \"redirect-https\": {\"type\": \"string\"},\n        \"scheme\": {\"type\": \"string\"},\n    },\n    \"required\": [\"model\", \"name\", \"host\", \"port\"],\n}\nINGRESS_PROVIDES_APP_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"ingress\": {\n            \"type\": \"object\",\n            \"patternProperties\": {\n                \"\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"url\": {\"type\": \"string\"},\n                    },\n                    \"required\": [\"url\"],\n                }\n            },\n        }\n    },\n    \"required\": [\"ingress\"],\n}\n\n# TYPES\ntry:\n    from typing import Literal, TypedDict  # type: ignore\nexcept ImportError:\n    from typing_extensions import Literal, TypedDict  # py35 compat\n\n\n# Model of the data a unit implementing the requirer will need to provide.\nRequirerData = TypedDict(\n    \"RequirerData\",\n    {\n        \"model\": str,\n        \"name\": str,\n        \"host\": str,\n        \"port\": int,\n        \"mode\": Optional[Literal[\"tcp\", \"http\"]],\n        \"strip-prefix\": Optional[bool],\n        \"redirect-https\": Optional[bool],\n        \"scheme\": Optional[Literal[\"http\", \"https\"]],\n    },\n    total=False,\n)\n\n\nRequirerUnitData = Dict[Unit, \"RequirerData\"]\nKeyValueMapping = Dict[str, str]\nProviderApplicationData = Dict[str, KeyValueMapping]\n\n\ndef _type_convert_stored(obj):\n    \"\"\"Convert Stored* to their appropriate types, recursively.\"\"\"\n    if isinstance(obj, StoredList):\n        return list(map(_type_convert_stored, obj))\n    if isinstance(obj, StoredDict):\n        rdict: Dict[Any, Any] = {}\n        for k in obj.keys():\n            rdict[k] = _type_convert_stored(obj[k])\n        return rdict\n    return obj\n\n\ndef _validate_data(data, schema):\n    \"\"\"Checks whether `data` matches `schema`.\n\n    Will raise DataValidationError if the data is not valid, else return None.\n    \"\"\"\n    if not DO_VALIDATION:\n        return\n    try:\n        jsonschema.validate(instance=data, schema=schema)  # pyright: ignore[reportUnboundVariable]\n    except jsonschema.ValidationError as e:  # pyright: ignore[reportUnboundVariable]\n        raise DataValidationError(data, schema) from e\n\n\n# EXCEPTIONS\nclass DataValidationError(RuntimeError):\n    \"\"\"Raised when data validation fails on IPU relation data.\"\"\"\n\n\nclass RelationException(RuntimeError):\n    \"\"\"Base class for relation exceptions from this library.\n\n    Attributes:\n        relation: The Relation which caused the exception.\n        entity: The Application or Unit which caused the exception.\n    \"\"\"\n\n    def __init__(self, relation: Relation, entity: Union[Application, Unit]):\n        super().__init__(relation)\n        self.args = (\n            \"There is an error with the relation {}:{} with {}\".format(\n                relation.name, relation.id, entity.name\n            ),\n        )\n        self.relation = relation\n        self.entity = entity\n\n\nclass RelationDataMismatchError(RelationException):\n    \"\"\"Data from different units do not match where they should.\"\"\"\n\n\nclass RelationPermissionError(RelationException):\n    \"\"\"Ingress is requested to do something for which it lacks permissions.\"\"\"\n\n    def __init__(self, relation: Relation, entity: Union[Application, Unit], message: str):\n        super(RelationPermissionError, self).__init__(relation, entity)\n        self.args = (\n            \"Unable to write data to relation '"'"'{}:{}'"'"' with {}: {}\".format(\n                relation.name, relation.id, entity.name, message\n            ),\n        )\n\n\nclass _IngressPerUnitBase(Object):\n    \"\"\"Base class for IngressPerUnit interface classes.\"\"\"\n\n    def __init__(self, charm: CharmBase, relation_name: str = DEFAULT_RELATION_NAME):\n        \"\"\"Constructor for _IngressPerUnitBase.\n\n        Args:\n            charm: The charm that is instantiating the instance.\n            relation_name: The name of the relation name to bind to\n                (defaults to \"ingress-per-unit\").\n        \"\"\"\n        super().__init__(charm, relation_name)\n        self.charm: CharmBase = charm\n\n        self.relation_name = relation_name\n        self.app = self.charm.app\n        self.unit = self.charm.unit\n\n        observe = self.framework.observe\n        rel_events = charm.on[relation_name]\n        observe(rel_events.relation_created, self._handle_relation)\n        observe(rel_events.relation_joined, self._handle_relation)\n        observe(rel_events.relation_changed, self._handle_relation)\n        observe(rel_events.relation_departed, self._handle_relation)\n        observe(rel_events.relation_broken, self._handle_relation_broken)\n        observe(charm.on.leader_elected, self._handle_upgrade_or_leader)  # type: ignore\n        observe(charm.on.upgrade_charm, self._handle_upgrade_or_leader)  # type: ignore\n\n    @property\n    def relations(self):\n        \"\"\"The list of Relation instances associated with this relation_name.\"\"\"\n        return list(self.charm.model.relations[self.relation_name])\n\n    def _handle_relation(self, event):\n        \"\"\"Subclasses should implement this method to handle a relation update.\"\"\"\n        pass\n\n    def _handle_relation_broken(self, event):\n        \"\"\"Subclasses should implement this method to handle a relation breaking.\"\"\"\n        pass\n\n    def _handle_upgrade_or_leader(self, event):\n        \"\"\"Subclasses should implement this method to handle upgrades or leadership change.\"\"\"\n        pass\n\n    def is_ready(self, relation: Optional[Relation] = None) -> bool:\n        \"\"\"Checks whether the given relation is ready.\n\n        A relation is ready if the remote side has sent valid data.\n        \"\"\"\n        if relation is None:\n            return any(map(self.is_ready, self.relations))\n        if relation.app is None:\n            # No idea why, but this happened once.\n            return False\n        if not relation.app.name:  # type: ignore\n            # Juju doesn'"'"'t provide JUJU_REMOTE_APP during relation-broken\n            # hooks. See https://github.com/canonical/operator/issues/693\n            return False\n        return True\n\n\nclass IngressDataReadyEvent(RelationEvent):\n    \"\"\"Event triggered when the requirer has provided valid ingress data.\n\n    Also emitted when the data has changed.\n    If you receive this, you should handle it as if the data being\n    provided was new.\n    \"\"\"\n\n\nclass IngressDataRemovedEvent(RelationEvent):\n    \"\"\"Event triggered when a requirer has wiped its ingress data.\n\n    Also emitted when the requirer data has become incomplete or invalid.\n    If you receive this, you should handle it as if the remote unit no longer\n    wishes to receive ingress.\n    \"\"\"\n\n\nclass IngressEndpointsUpdatedEvent(RelationEvent):\n    \"\"\"Event triggered when the proxied endpoints change.\"\"\"\n\n\nclass IngressPerUnitProviderEvents(ObjectEvents):\n    \"\"\"Container for events for IngressPerUnit.\"\"\"\n\n    data_provided = EventSource(IngressDataReadyEvent)\n    data_removed = EventSource(IngressDataRemovedEvent)\n    endpoints_updated = EventSource(IngressEndpointsUpdatedEvent)\n\n\nclass IngressPerUnitProvider(_IngressPerUnitBase):\n    \"\"\"Implementation of the provider of ingress_per_unit.\"\"\"\n\n    on = IngressPerUnitProviderEvents()  # type: ignore\n\n    def _handle_relation(self, event):\n        relation = event.relation\n        try:\n            self.validate(relation)\n        except RelationDataMismatchError as e:\n            self.on.data_removed.emit(relation=relation, app=relation.app)  # type: ignore\n            log.warning(\n                \"relation data mismatch: {} \" \"data_removed ingress for {}.\".format(e, relation)\n            )\n            return\n\n        if self.is_ready(relation):\n            self.on.data_provided.emit(relation=relation, app=relation.app)  # type: ignore\n        else:\n            self.on.data_removed.emit(relation=relation, app=relation.app)  # type: ignore\n\n    def _handle_relation_broken(self, event):\n        # relation broken -> we revoke in any case\n        self.on.data_removed.emit(relation=event.relation, app=event.relation.app)  # type: ignore\n\n    def is_ready(self, relation: Optional[Relation] = None) -> bool:\n        \"\"\"Checks whether the given relation is ready.\n\n        Or any relation if not specified.\n        A given relation is ready if SOME remote side has sent valid data.\n        \"\"\"\n        if relation is None:\n            return any(map(self.is_ready, self.relations))\n\n        if not super().is_ready(relation):\n            return False\n\n        try:\n            requirer_units_data = self._requirer_units_data(relation)\n        except Exception:\n            log.exception(\"Cannot fetch ingress data for the '"'"'{}'"'"' relation\".format(relation))\n            return False\n\n        return any(requirer_units_data.values())\n\n    def validate(self, relation: Relation):\n        \"\"\"Checks whether the given relation is failed.\n\n        Or any relation if not specified.\n        \"\"\"\n        # verify that all remote units (requirer'"'"'s side) publish the same model.\n        # We do not validate the port because, in case of changes to the configuration\n        # of the charm or a new version of the charmed workload, e.g. over an upgrade,\n        # the remote port may be different among units.\n        expected_model = None  # It may be none for units that have not yet written data\n\n        remote_units_data = self._requirer_units_data(relation)\n        for remote_unit, remote_unit_data in remote_units_data.items():\n            if \"model\" in remote_unit_data:\n                remote_model = remote_unit_data[\"model\"]\n                if not expected_model:\n                    expected_model = remote_model\n                elif expected_model != remote_model:\n                    raise RelationDataMismatchError(relation, remote_unit)\n\n    def is_unit_ready(self, relation: Relation, unit: Unit) -> bool:\n        \"\"\"Report whether the given unit has shared data in its unit data bag.\"\"\"\n        # sanity check: this should not occur in production, but it may happen\n        # during testing: cfr https://github.com/canonical/traefik-k8s-operator/issues/39\n        assert unit in relation.units, (\n            \"attempting to get ready state \" \"for unit that does not belong to relation\"\n        )\n        try:\n            self._get_requirer_unit_data(relation, unit)\n        except (KeyError, DataValidationError):\n            return False\n        return True\n\n    def get_data(self, relation: Relation, unit: Unit) -> \"RequirerData\":\n        \"\"\"Fetch the data shared by the specified unit on the relation (Requirer side).\"\"\"\n        return self._get_requirer_unit_data(relation, unit)\n\n    def publish_url(self, relation: Relation, unit_name: str, url: str):\n        \"\"\"Place the ingress url in the application data bag for the units on the requirer side.\n\n        Assumes that this unit is leader.\n        \"\"\"\n        assert self.unit.is_leader(), \"only leaders can do this\"\n\n        raw_data = relation.data[self.app].get(\"ingress\", None)\n        data = yaml.safe_load(raw_data) if raw_data else {}\n        ingress = {\"ingress\": data}\n\n        # we ensure that the application databag has the shape we think it\n        # should have; to catch any inconsistencies early on.\n        try:\n            _validate_data(ingress, INGRESS_PROVIDES_APP_SCHEMA)\n        except DataValidationError as e:\n            log.error(\n                \"unable to publish url to {}: corrupted application databag ({})\".format(\n                    unit_name, e\n                )\n            )\n            return\n\n        # we update the data with a new url\n        data[unit_name] = {\"url\": url}\n\n        # we validate the data **again**, to ensure that we respected the schema\n        # and did not accidentally corrupt our own databag.\n        _validate_data(ingress, INGRESS_PROVIDES_APP_SCHEMA)\n        relation.data[self.app][\"ingress\"] = yaml.safe_dump(data)\n\n        self.on.endpoints_updated.emit(relation=relation, app=relation.app)\n\n    def wipe_ingress_data(self, relation):\n        \"\"\"Remove all published ingress data.\n\n        Assumes that this unit is leader.\n        \"\"\"\n        assert self.unit.is_leader(), \"only leaders can do this\"\n        try:\n            relation.data\n        except ModelError as e:\n            log.warning(\n                \"error {} accessing relation data for {!r}. \"\n                \"Probably a ghost of a dead relation is still \"\n                \"lingering around.\".format(e, relation.name)\n            )\n            return\n        del relation.data[self.app][\"ingress\"]\n        self.on.endpoints_updated.emit(relation=relation, app=relation.app)\n\n    def _requirer_units_data(self, relation: Relation) -> RequirerUnitData:\n        \"\"\"Fetch and validate the requirer'"'"'s units databag.\"\"\"\n        if not relation.app or not relation.app.name:\n            # Handle edge case where remote app name can be missing, e.g.,\n            # relation_broken events.\n            # FIXME https://github.com/canonical/traefik-k8s-operator/issues/34\n            return {}\n\n        remote_units = [unit for unit in relation.units if unit.app is not self.app]\n\n        requirer_units_data = {}\n        for remote_unit in remote_units:\n            try:\n                remote_data = self._get_requirer_unit_data(relation, remote_unit)\n            except KeyError:\n                # this remote unit didn'"'"'t share data yet\n                log.warning(\"Remote unit {} not ready.\".format(remote_unit.name))\n                continue\n            except DataValidationError as e:\n                # this remote unit sent invalid data.\n                log.error(\"Remote unit {} sent invalid data ({}).\".format(remote_unit.name, e))\n                continue\n\n            remote_data[\"port\"] = int(remote_data[\"port\"])\n            requirer_units_data[remote_unit] = remote_data\n        return requirer_units_data\n\n    def _get_requirer_unit_data(self, relation: Relation, remote_unit: Unit) -> RequirerData:  # type: ignore\n        \"\"\"Fetch and validate the requirer unit data for this unit.\n\n        For convenience, we convert '"'"'port'"'"' to integer.\n        \"\"\"\n        if not relation.app or not relation.app.name:\n            # Handle edge case where remote app name can be missing, e.g.,\n            # relation_broken events.\n            # FIXME https://github.com/canonical/traefik-k8s-operator/issues/34\n            return {}\n\n        databag = relation.data[remote_unit]\n        remote_data: Dict[str, Union[int, str]] = {}\n        for k in (\n            \"port\",\n            \"host\",\n            \"model\",\n            \"name\",\n            \"mode\",\n            \"strip-prefix\",\n            \"redirect-https\",\n            \"scheme\",\n        ):\n            v = databag.get(k)\n            if v is not None:\n                remote_data[k] = v\n        _validate_data(remote_data, INGRESS_REQUIRES_UNIT_SCHEMA)\n        remote_data[\"port\"] = int(remote_data[\"port\"])\n        remote_data[\"strip-prefix\"] = bool(remote_data.get(\"strip-prefix\", \"false\") == \"true\")\n        remote_data[\"redirect-https\"] = bool(remote_data.get(\"redirect-https\", \"false\") == \"true\")\n        return typing.cast(RequirerData, remote_data)\n\n    def _provider_app_data(self, relation: Relation) -> ProviderApplicationData:\n        \"\"\"Fetch and validate the provider'"'"'s app databag.\"\"\"\n        if not relation.app or not relation.app.name:\n            # Handle edge case where remote app name can be missing, e.g.,\n            # relation_broken events.\n            # FIXME https://github.com/canonical/traefik-k8s-operator/issues/34\n            return {}\n\n        # we start by looking at the provider'"'"'s app databag\n        if self.unit.is_leader():\n            # only leaders can read their app'"'"'s data\n            data = relation.data[self.app].get(\"ingress\")\n            if not data:\n                return {}\n\n            deserialized = yaml.safe_load(data)\n            _validate_data({\"ingress\": deserialized}, INGRESS_PROVIDES_APP_SCHEMA)\n            return deserialized\n\n        return {}\n\n    @property\n    def proxied_endpoints(self) -> dict:\n        \"\"\"The ingress settings provided to units by this provider.\n\n        For example, when this IngressPerUnitProvider has provided the\n        `http://foo.bar/my-model.my-app-1` and\n        `http://foo.bar/my-model.my-app-2` URLs to the two units of the\n        my-app application, the returned dictionary will be:\n\n        ```\n        {\n            \"my-app/1\": {\n                \"url\": \"http://foo.bar/my-model.my-app-1\"\n            },\n            \"my-app/2\": {\n                \"url\": \"http://foo.bar/my-model.my-app-2\"\n            }\n        }\n        ```\n        \"\"\"\n        results = {}\n\n        for ingress_relation in self.relations:\n            provider_app_data = self._provider_app_data(ingress_relation)\n            results.update(provider_app_data)\n\n        return results\n\n\nclass _IPUEvent(RelationEvent):\n    __args__: Tuple[str, ...] = ()\n    __optional_kwargs__: Dict[str, Any] = {}\n\n    @classmethod\n    def __attrs__(cls):\n        return cls.__args__ + tuple(cls.__optional_kwargs__.keys())\n\n    def __init__(self, handle, relation, *args, **kwargs):\n        super().__init__(handle, relation, app=relation.app)\n\n        if not len(self.__args__) == len(args):\n            raise TypeError(\"expected {} args, got {}\".format(len(self.__args__), len(args)))\n\n        for attr, obj in zip(self.__args__, args):\n            setattr(self, attr, obj)\n        for attr, default in self.__optional_kwargs__.items():\n            obj = kwargs.get(attr, default)\n            setattr(self, attr, obj)\n\n    def snapshot(self):\n        dct = super().snapshot()\n        for attr in self.__attrs__():\n            obj = getattr(self, attr)\n            try:\n                dct[attr] = obj\n            except ValueError as e:\n                raise ValueError(\n                    \"cannot automagically serialize {}: \"\n                    \"override this method and do it \"\n                    \"manually.\".format(obj)\n                ) from e\n        return dct\n\n    def restore(self, snapshot) -> None:\n        super().restore(snapshot)\n        for attr, obj in snapshot.items():\n            setattr(self, attr, obj)\n\n\nclass IngressPerUnitReadyEvent(_IPUEvent):\n    \"\"\"Ingress is ready (or has changed) for some unit.\n\n    Attrs:\n        `unit_name`: name of the unit for which ingress has been\n            provided/has changed.\n        `url`: the (new) url for that unit.\n    \"\"\"\n\n    __args__ = (\"unit_name\", \"url\")\n    if typing.TYPE_CHECKING:\n        unit_name = \"\"\n        url = \"\"\n\n\nclass IngressPerUnitReadyForUnitEvent(_IPUEvent):\n    \"\"\"Ingress is ready (or has changed) for this unit.\n\n    Is only fired on the unit(s) for which ingress has been provided or\n    has changed.\n    Attrs:\n        `url`: the (new) url for this unit.\n    \"\"\"\n\n    __args__ = (\"url\",)\n    if typing.TYPE_CHECKING:\n        url = \"\"\n\n\nclass IngressPerUnitRevokedEvent(_IPUEvent):\n    \"\"\"Ingress is revoked (or has changed) for some unit.\n\n    Attrs:\n        `unit_name`: the name of the unit whose ingress has been revoked.\n            this could be \"THIS\" unit, or a peer.\n    \"\"\"\n\n    __args__ = (\"unit_name\",)\n\n    if typing.TYPE_CHECKING:\n        unit_name = \"\"\n\n\nclass IngressPerUnitRevokedForUnitEvent(RelationEvent):\n    \"\"\"Ingress is revoked (or has changed) for this unit.\n\n    Is only fired on the unit(s) for which ingress has changed.\n    \"\"\"\n\n\nclass IngressPerUnitRequirerEvents(ObjectEvents):\n    \"\"\"Container for IUP events.\"\"\"\n\n    ready = EventSource(IngressPerUnitReadyEvent)\n    revoked = EventSource(IngressPerUnitRevokedEvent)\n    ready_for_unit = EventSource(IngressPerUnitReadyForUnitEvent)\n    revoked_for_unit = EventSource(IngressPerUnitRevokedForUnitEvent)\n\n\nclass IngressPerUnitRequirer(_IngressPerUnitBase):\n    \"\"\"Implementation of the requirer of ingress_per_unit.\"\"\"\n\n    on: IngressPerUnitRequirerEvents = IngressPerUnitRequirerEvents()\n    # used to prevent spurious urls to be sent out if the event we'"'"'re currently\n    # handling is a relation-broken one.\n    _stored = StoredState()\n\n    def __init__(\n        self,\n        charm: CharmBase,\n        relation_name: str = DEFAULT_RELATION_NAME,\n        *,\n        host: Optional[str] = None,\n        port: Optional[int] = None,\n        mode: Literal[\"tcp\", \"http\"] = \"http\",\n        listen_to: Literal[\"only-this-unit\", \"all-units\", \"both\"] = \"only-this-unit\",\n        strip_prefix: bool = False,\n        redirect_https: bool = False,\n        # FIXME: now that `provide_ingress_requirements` takes a scheme, this arg can be changed to\n        #  str type in v2.\n        scheme: typing.Callable[[], str] = lambda: \"http\",\n    ):\n        \"\"\"Constructor for IngressPerUnitRequirer.\n\n        The request args can be used to specify the ingress properties when the\n        instance is created. If any are set, at least `port` is required, and\n        they will be sent to the ingress provider as soon as it is available.\n        All request args must be given as keyword args.\n\n        Args:\n            charm: the charm that is instantiating the library.\n            relation_name: the name of the relation name to bind to\n                (defaults to \"ingress-per-unit\"; relation must be of interface\n                type \"ingress_per_unit\" and have \"limit: 1\").\n            host: Hostname to be used by the ingress provider to address the\n                requirer unit; if unspecified, the FQDN of the unit will be\n                used instead.\n            port: port to be used by the ingress provider to address the\n                    requirer unit.\n            mode: mode to be used between \"tcp\" and \"http\".\n            listen_to: Choose which events should be fired on this unit:\n                \"only-this-unit\": this unit will only be notified when ingress\n                  is ready/revoked for this unit.\n                \"all-units\": this unit will be notified when ingress is\n                  ready/revoked for any unit of this application, including\n                  itself.\n                \"all\": this unit will receive both event types (which means it\n                  will be notified *twice* of changes to this unit'"'"'s ingress!).\n            strip_prefix: remove prefixes from the URL path.\n            redirect_https: redirect incoming requests to HTTPS\n            scheme: callable returning the scheme to use when constructing the ingress url.\n        \"\"\"\n        super().__init__(charm, relation_name)\n        self._stored.set_default(current_urls=None)  # type: ignore\n\n        # if instantiated with a port, and we are related, then\n        # we immediately publish our ingress data  to speed up the process.\n        self._host = host\n        self._port = port\n        self._mode = mode\n        self._strip_prefix = strip_prefix\n        self._redirect_https = redirect_https\n        self._get_scheme = scheme\n\n        self.listen_to = listen_to\n\n        self.framework.observe(\n            self.charm.on[self.relation_name].relation_changed, self._handle_relation\n        )\n        self.framework.observe(\n            self.charm.on[self.relation_name].relation_broken, self._handle_relation\n        )\n\n    def _handle_relation(self, event: RelationEvent):\n        # we calculate the diff between the urls we were aware of\n        # before and those we know now\n        previous_urls = self._stored.current_urls or {}  # type: ignore\n\n        # since ops 2.10, breaking relations won'"'"'t show up in self.model.relations, so we'"'"'re safe\n        # in assuming all relations that are there are alive and well.\n        current_urls = self._urls_from_relation_data\n        self._stored.current_urls = current_urls  # type: ignore\n\n        removed = previous_urls.keys() - current_urls.keys()  # type: ignore\n        changed = {a for a in current_urls if current_urls[a] != previous_urls.get(a)}  # type: ignore\n\n        this_unit_name = self.unit.name\n        # do not use self.relation in this context because if\n        # the event is relation-broken, self.relation might be None\n        relation = event.relation\n        if self.listen_to in {\"only-this-unit\", \"both\"}:\n            if this_unit_name in changed:\n                self.on.ready_for_unit.emit(relation, current_urls[this_unit_name])  # type: ignore\n\n            if this_unit_name in removed:\n                self.on.revoked_for_unit.emit(relation=relation, app=relation.app)  # type: ignore\n\n        if self.listen_to in {\"all-units\", \"both\"}:\n            for unit_name in changed:\n                self.on.ready.emit(relation, unit_name, current_urls[unit_name])  # type: ignore\n\n            for unit_name in removed:\n                self.on.revoked.emit(relation, unit_name)  # type: ignore\n\n        self._publish_auto_data()\n\n    def _handle_upgrade_or_leader(self, event):\n        self._publish_auto_data()\n\n    def _publish_auto_data(self):\n        if self._port:\n            self.provide_ingress_requirements(host=self._host, port=self._port)\n\n    @property\n    def relation(self) -> Optional[Relation]:\n        \"\"\"The established Relation instance, or None if still unrelated.\"\"\"\n        return self.relations[0] if self.relations else None\n\n    def is_ready(self) -> bool:\n        \"\"\"Checks whether the given relation is ready.\n\n        Or any relation if not specified.\n        A given relation is ready if the remote side has sent valid data.\n        \"\"\"\n        if not self.relation:\n            return False\n        if super().is_ready(self.relation) is False:\n            return False\n        return bool(self.url)\n\n    def provide_ingress_requirements(\n        self, *, scheme: Optional[str] = None, host: Optional[str] = None, port: int\n    ):\n        \"\"\"Publishes the data that Traefik needs to provide ingress.\n\n        Args:\n            scheme: Scheme to be used; if unspecified, use the one used by __init__.\n            host: Hostname to be used by the ingress provider to address the\n             requirer unit; if unspecified, FQDN will be used instead\n            port: the port of the service (required)\n        \"\"\"\n        # This public method may be used at various points of the charm lifecycle, possibly when\n        # the ingress relation is not yet there.\n        # Abort if there is no relation (instead of requiring the caller to guard against it).\n        if not self.relation:\n            return\n\n        if not host:\n            host = socket.getfqdn()\n\n        if not scheme:\n            # If scheme was not provided, use the one given to the constructor.\n            scheme = self._get_scheme()\n\n        data = {\n            \"model\": self.model.name,\n            \"name\": self.unit.name,\n            \"host\": host,\n            \"port\": str(port),\n            \"mode\": self._mode,\n            \"scheme\": scheme,\n        }\n\n        if self._strip_prefix:\n            data[\"strip-prefix\"] = \"true\"\n\n        if self._redirect_https:\n            data[\"redirect-https\"] = \"true\"\n\n        _validate_data(data, INGRESS_REQUIRES_UNIT_SCHEMA)\n        self.relation.data[self.unit].update(data)\n\n    @property\n    def _urls_from_relation_data(self) -> Dict[str, str]:\n        \"\"\"The full ingress URLs to reach every unit.\n\n        May return an empty dict if the URLs aren'"'"'t available yet.\n        \"\"\"\n        relation = self.relation\n        if not relation:\n            return {}\n\n        if not relation.app or not relation.app.name:  # type: ignore\n            # FIXME Workaround for https://github.com/canonical/operator/issues/693\n            # We must be in a relation_broken hook\n            return {}\n        assert isinstance(relation.app, Application)  # type guard\n\n        try:\n            raw = relation.data.get(relation.app, {}).get(\"ingress\")\n        except ModelError as e:\n            log.debug(\n                \"Error {} attempting to read remote app data; \"\n                \"probably we are in a relation_departed hook\".format(e)\n            )\n            return {}\n\n        if not raw:\n            # remote side didn'"'"'t send yet\n            return {}\n\n        data = yaml.safe_load(raw)\n        _validate_data({\"ingress\": data}, INGRESS_PROVIDES_APP_SCHEMA)\n\n        return {unit_name: unit_data[\"url\"] for unit_name, unit_data in data.items()}\n\n    @property\n    def urls(self) -> Dict[str, str]:\n        \"\"\"The full ingress URLs to reach every unit.\n\n        May return an empty dict if the URLs aren'"'"'t available yet.\n        \"\"\"\n        current_urls = self._urls_from_relation_data\n        return current_urls\n\n    @property\n    def url(self) -> Optional[str]:\n        \"\"\"The full ingress URL to reach the current unit.\n\n        May return None if the URL isn'"'"'t available yet.\n        \"\"\"\n        urls = self.urls\n        if not urls:\n            return None\n        return urls.get(self.charm.unit.name)\n", "apt.py": "# Copyright 2021 Canonical Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Abstractions for the system'"'"'s Debian/Ubuntu package information and repositories.\n\nThis module contains abstractions and wrappers around Debian/Ubuntu-style repositories and\npackages, in order to easily provide an idiomatic and Pythonic mechanism for adding packages and/or\nrepositories to systems for use in machine charms.\n\nA sane default configuration is attainable through nothing more than instantiation of the\nappropriate classes. `DebianPackage` objects provide information about the architecture, version,\nname, and status of a package.\n\n`DebianPackage` will try to look up a package either from `dpkg -L` or from `apt-cache` when\nprovided with a string indicating the package name. If it cannot be located, `PackageNotFoundError`\nwill be returned, as `apt` and `dpkg` otherwise return `100` for all errors, and a meaningful error\nmessage if the package is not known is desirable.\n\nTo install packages with convenience methods:\n\n```python\ntry:\n    # Run `apt-get update`\n    apt.update()\n    apt.add_package(\"zsh\")\n    apt.add_package([\"vim\", \"htop\", \"wget\"])\nexcept PackageError as e:\n    logger.error(\"could not install package. Reason: %s\", e.message)\n````\n\nThe convenience methods don'"'"'t raise `PackageNotFoundError`. If any packages aren'"'"'t found in\nthe cache, `apt.add_package` raises `PackageError` with a message '"'"'Failed to install\npackages: foo, bar'"'"'.\n\nTo find details of a specific package:\n\n```python\ntry:\n    vim = apt.DebianPackage.from_system(\"vim\")\n\n    # To find from the apt cache only\n    # apt.DebianPackage.from_apt_cache(\"vim\")\n\n    # To find from installed packages only\n    # apt.DebianPackage.from_installed_package(\"vim\")\n\n    vim.ensure(PackageState.Latest)\n    logger.info(\"updated vim to version: %s\", vim.fullversion)\nexcept PackageNotFoundError:\n    logger.error(\"a specified package not found in package cache or on system\")\nexcept PackageError as e:\n    logger.error(\"could not install package. Reason: %s\", e.message)\n```\n\n\n`RepositoryMapping` will return a dict-like object containing enabled system repositories\nand their properties (available groups, baseuri. gpg key). This class can add, disable, or\nmanipulate repositories. Items can be retrieved as `DebianRepository` objects.\n\nIn order to add a new repository with explicit details for fields, a new `DebianRepository`\ncan be added to `RepositoryMapping`\n\n`RepositoryMapping` provides an abstraction around the existing repositories on the system,\nand can be accessed and iterated over like any `Mapping` object, to retrieve values by key,\niterate, or perform other operations.\n\nKeys are constructed as `{repo_type}-{}-{release}` in order to uniquely identify a repository.\n\nRepositories can be added with explicit values through a Python constructor.\n\nExample:\n```python\nrepositories = apt.RepositoryMapping()\n\nif \"deb-example.com-focal\" not in repositories:\n    repositories.add(DebianRepository(enabled=True, repotype=\"deb\",\n                     uri=\"https://example.com\", release=\"focal\", groups=[\"universe\"]))\n```\n\nAlternatively, any valid `sources.list` line may be used to construct a new\n`DebianRepository`.\n\nExample:\n```python\nrepositories = apt.RepositoryMapping()\n\nif \"deb-us.archive.ubuntu.com-xenial\" not in repositories:\n    line = \"deb http://us.archive.ubuntu.com/ubuntu xenial main restricted\"\n    repo = DebianRepository.from_repo_line(line)\n    repositories.add(repo)\n```\n\nDependencies:\nNote that this module requires `opentelemetry-api`, which is already included into\nyour charm'"'"'s virtual environment via `ops >= 2.21`.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport fileinput\nimport glob\nimport logging\nimport os\nimport re\nimport subprocess\nimport typing\nfrom enum import Enum\nfrom subprocess import PIPE, CalledProcessError, check_output\nfrom typing import Any, Iterable, Iterator, Literal, Mapping\nfrom urllib.parse import urlparse\n\nimport opentelemetry.trace\n\nlogger = logging.getLogger(__name__)\ntracer = opentelemetry.trace.get_tracer(__name__)\n\n# The unique Charmhub library identifier, never change it\nLIBID = \"7c3dbc9c2ad44a47bd6fcb25caa270e5\"\n\n# Increment this major API version when introducing breaking changes\nLIBAPI = 0\n\n# Increment this PATCH version before using `charmcraft publish-lib` or reset\n# to 0 if you are raising the major API version\nLIBPATCH = 19\n\nPYDEPS = [\"opentelemetry-api\"]\n\n\nVALID_SOURCE_TYPES = (\"deb\", \"deb-src\")\nOPTIONS_MATCHER = re.compile(r\"\\[.*?\\]\")\n_GPG_KEY_DIR = \"/etc/apt/trusted.gpg.d/\"\n\n\nclass Error(Exception):\n    \"\"\"Base class of most errors raised by this library.\"\"\"\n\n    def __repr__(self):\n        \"\"\"Represent the Error.\"\"\"\n        return f\"<{type(self).__module__}.{type(self).__name__} {self.args}>\"\n\n    @property\n    def name(self):\n        \"\"\"Return a string representation of the model plus class.\"\"\"\n        return f\"<{type(self).__module__}.{type(self).__name__}>\"\n\n    @property\n    def message(self):\n        \"\"\"Return the message passed as an argument.\"\"\"\n        return self.args[0]\n\n\nclass PackageError(Error):\n    \"\"\"Raised when there'"'"'s an error installing or removing a package.\n\n    Additionally, `apt.add_package` raises `PackageError` if any packages aren'"'"'t found in\n    the cache.\n    \"\"\"\n\n\nclass PackageNotFoundError(Error):\n    \"\"\"Raised by `DebianPackage` methods if a requested package is not found.\"\"\"\n\n\nclass PackageState(Enum):\n    \"\"\"A class to represent possible package states.\"\"\"\n\n    Present = \"present\"\n    Absent = \"absent\"\n    Latest = \"latest\"\n    Available = \"available\"\n\n\nclass DebianPackage:\n    \"\"\"Represents a traditional Debian package and its utility functions.\n\n    `DebianPackage` wraps information and functionality around a known package, whether installed\n    or available. The version, epoch, name, and architecture can be easily queried and compared\n    against other `DebianPackage` objects to determine the latest version or to install a specific\n    version.\n\n    The representation of this object as a string mimics the output from `dpkg` for familiarity.\n\n    Installation and removal of packages is handled through the `state` property or `ensure`\n    method, with the following options:\n\n        apt.PackageState.Absent\n        apt.PackageState.Available\n        apt.PackageState.Present\n        apt.PackageState.Latest\n\n    When `DebianPackage` is initialized, the state of a given `DebianPackage` object will be set to\n    `Available`, `Present`, or `Latest`, with `Absent` implemented as a convenience for removal\n    (though it operates essentially the same as `Available`).\n    \"\"\"\n\n    def __init__(\n        self, name: str, version: str, epoch: str, arch: str, state: PackageState\n    ) -> None:\n        self._name = name\n        self._arch = arch\n        self._state = state\n        self._version = Version(version, epoch)\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"Equality for comparison.\n\n        Args:\n          other: a `DebianPackage` object for comparison\n\n        Returns:\n          A boolean reflecting equality\n        \"\"\"\n        return isinstance(other, self.__class__) and (\n            self._name,\n            self._version.number,\n        ) == (other._name, other._version.number)\n\n    def __hash__(self):\n        \"\"\"Return a hash of this package.\"\"\"\n        return hash((self._name, self._version.number))\n\n    def __repr__(self):\n        \"\"\"Represent the package.\"\"\"\n        return f\"<{self.__module__}.{type(self).__name__}: {self.__dict__}>\"\n\n    def __str__(self):\n        \"\"\"Return a human-readable representation of the package.\"\"\"\n        return (\n            f\"<{type(self).__name__}: {self._name}-{self._version}.{self._arch} -- {self._state}>\"\n        )\n\n    @staticmethod\n    def _apt(\n        command: str,\n        package_names: str | list[str],\n        optargs: list[str] | None = None,\n    ) -> None:\n        \"\"\"Wrap package management commands for Debian/Ubuntu systems.\n\n        Args:\n          command: the command given to `apt-get`\n          package_names: a package name or list of package names to operate on\n          optargs: an (Optional) list of additional arguments\n\n        Raises:\n          PackageError if an error is encountered\n        \"\"\"\n        optargs = optargs if optargs is not None else []\n        if isinstance(package_names, str):\n            package_names = [package_names]\n        _cmd = [\"apt-get\", \"-y\", *optargs, command, *package_names]\n        try:\n            env = os.environ.copy()\n            env[\"DEBIAN_FRONTEND\"] = \"noninteractive\"\n            with tracer.start_as_current_span(_cmd[0]) as span:\n                span.set_attribute(\"argv\", _cmd)\n                subprocess.run(_cmd, capture_output=True, check=True, text=True, env=env)\n        except CalledProcessError as e:\n            raise PackageError(\n                f\"Could not {command} package(s) {package_names}: {e.stderr}\"\n            ) from None\n\n    def _add(self) -> None:\n        \"\"\"Add a package to the system.\"\"\"\n        self._apt(\n            \"install\",\n            f\"{self.name}={self.version}\",\n            optargs=[\"--option=Dpkg::Options::=--force-confold\"],\n        )\n\n    def _remove(self) -> None:\n        \"\"\"Remove a package from the system. Implementation-specific.\"\"\"\n        return self._apt(\"remove\", f\"{self.name}={self.version}\")\n\n    @property\n    def name(self) -> str:\n        \"\"\"Returns the name of the package.\"\"\"\n        return self._name\n\n    def ensure(self, state: PackageState):\n        \"\"\"Ensure that a package is in a given state.\n\n        Args:\n          state: a `PackageState` to reconcile the package to\n\n        Raises:\n          PackageError from the underlying call to apt\n        \"\"\"\n        if self._state is not state:\n            if state not in (PackageState.Present, PackageState.Latest):\n                self._remove()\n            else:\n                self._add()\n        self._state = state\n\n    @property\n    def present(self) -> bool:\n        \"\"\"Returns whether or not a package is present.\"\"\"\n        return self._state in (PackageState.Present, PackageState.Latest)\n\n    @property\n    def latest(self) -> bool:\n        \"\"\"Returns whether the package is the most recent version.\"\"\"\n        return self._state is PackageState.Latest\n\n    @property\n    def state(self) -> PackageState:\n        \"\"\"Returns the current package state.\"\"\"\n        return self._state\n\n    @state.setter\n    def state(self, state: PackageState) -> None:\n        \"\"\"Set the package state to a given value.\n\n        Args:\n          state: a `PackageState` to reconcile the package to\n\n        Raises:\n          PackageError from the underlying call to apt\n        \"\"\"\n        if state in (PackageState.Latest, PackageState.Present):\n            self._add()\n        else:\n            self._remove()\n        self._state = state\n\n    @property\n    def version(self) -> Version:\n        \"\"\"Returns the version for a package.\"\"\"\n        return self._version\n\n    @property\n    def epoch(self) -> str:\n        \"\"\"Returns the epoch for a package. May be unset.\"\"\"\n        return self._version.epoch\n\n    @property\n    def arch(self) -> str:\n        \"\"\"Returns the architecture for a package.\"\"\"\n        return self._arch\n\n    @property\n    def fullversion(self) -> str:\n        \"\"\"Returns the name+epoch for a package.\"\"\"\n        return f\"{self._version}.{self._arch}\"\n\n    @staticmethod\n    def _get_epoch_from_version(version: str) -> tuple[str, str]:\n        \"\"\"Pull the epoch, if any, out of a version string.\"\"\"\n        epoch_matcher = re.compile(r\"^((?P<epoch>\\d+):)?(?P<version>.*)\")\n        result = epoch_matcher.search(version)\n        assert result is not None\n        matches = result.groupdict()\n        return matches.get(\"epoch\", \"\"), matches[\"version\"]\n\n    @classmethod\n    def from_system(\n        cls, package: str, version: str | None = \"\", arch: str | None = \"\"\n    ) -> DebianPackage:\n        \"\"\"Locates a package, either on the system or known to apt, and serializes the information.\n\n        Args:\n            package: a string representing the package\n            version: an optional string if a specific version is requested\n            arch: an optional architecture, defaulting to `dpkg --print-architecture`. If an\n                architecture is not specified, this will be used for selection.\n\n        \"\"\"\n        try:\n            return DebianPackage.from_installed_package(package, version, arch)\n        except PackageNotFoundError:\n            logger.debug(\n                \"package '"'"'%s'"'"' is not currently installed or has the wrong architecture.\", package\n            )\n\n        # Ok, try `apt-cache ...`\n        try:\n            return DebianPackage.from_apt_cache(package, version, arch)\n        except (PackageNotFoundError, PackageError):\n            # If we get here, it'"'"'s not known to the systems.\n            # This seems unnecessary, but virtually all `apt` commands have a return code of `100`,\n            # and providing meaningful error messages without this is ugly.\n            arch_str = f\".{arch}\" if arch else \"\"\n            raise PackageNotFoundError(\n                f\"Package '"'"'{package}{arch_str}'"'"' \"\n                \"could not be found on the system or in the apt cache!\"\n            ) from None\n\n    @classmethod\n    def from_installed_package(\n        cls, package: str, version: str | None = \"\", arch: str | None = \"\"\n    ) -> DebianPackage:\n        \"\"\"Check whether the package is already installed and return an instance.\n\n        Args:\n            package: a string representing the package\n            version: an optional string if a specific version is requested\n            arch: an optional architecture, defaulting to `dpkg --print-architecture`.\n                If an architecture is not specified, this will be used for selection.\n        \"\"\"\n        system_arch = check_output(\n            [\"dpkg\", \"--print-architecture\"], universal_newlines=True\n        ).strip()\n        arch = arch if arch else system_arch\n\n        # Regexps are a really terrible way to do this. Thanks dpkg\n        output = \"\"\n        try:\n            output = check_output([\"dpkg\", \"-l\", package], stderr=PIPE, universal_newlines=True)\n        except CalledProcessError:\n            raise PackageNotFoundError(f\"Package is not installed: {package}\") from None\n\n        # Pop off the output from `dpkg -l'"'"' because there'"'"'s no flag to\n        # omit it`\n        lines = str(output).splitlines()[5:]\n\n        dpkg_matcher = re.compile(\n            r\"\"\"\n        ^(?P<package_status>\\w+?)\\s+\n        (?P<package_name>.*?)(?P<throwaway_arch>:\\w+?)?\\s+\n        (?P<version>.*?)\\s+\n        (?P<arch>\\w+?)\\s+\n        (?P<description>.*)\n        \"\"\",\n            re.VERBOSE,\n        )\n\n        for line in lines:\n            result = dpkg_matcher.search(line)\n            if result is None:\n                logger.warning(\"dpkg matcher could not parse line: %s\", line)\n                continue\n            matches = result.groupdict()\n            package_status = matches[\"package_status\"]\n\n            if not package_status.endswith(\"i\"):\n                logger.debug(\n                    \"package '"'"'%s'"'"' in dpkg output but not installed, status: '"'"'%s'"'"'\",\n                    package,\n                    package_status,\n                )\n                break\n\n            epoch, split_version = DebianPackage._get_epoch_from_version(matches[\"version\"])\n            pkg = DebianPackage(\n                name=matches[\"package_name\"],\n                version=split_version,\n                epoch=epoch,\n                arch=matches[\"arch\"],\n                state=PackageState.Present,\n            )\n            if (pkg.arch == \"all\" or pkg.arch == arch) and (\n                version == \"\" or str(pkg.version) == version\n            ):\n                return pkg\n\n        # If we didn'"'"'t find it, fail through\n        raise PackageNotFoundError(f\"Package {package}.{arch} is not installed!\")\n\n    @classmethod\n    def from_apt_cache(\n        cls, package: str, version: str | None = \"\", arch: str | None = \"\"\n    ) -> DebianPackage:\n        \"\"\"Check whether the package is already installed and return an instance.\n\n        Args:\n            package: a string representing the package\n            version: an optional string if a specific version is requested\n            arch: an optional architecture, defaulting to `dpkg --print-architecture`.\n                If an architecture is not specified, this will be used for selection.\n        \"\"\"\n        cmd = [\"dpkg\", \"--print-architecture\"]\n        with tracer.start_as_current_span(cmd[0]) as span:\n            span.set_attribute(\"argv\", cmd)\n            system_arch = check_output(cmd, universal_newlines=True).strip()\n        arch = arch if arch else system_arch\n\n        # Regexps are a really terrible way to do this. Thanks dpkg\n        keys = (\"Package\", \"Architecture\", \"Version\")\n\n        cmd = [\"apt-cache\", \"show\", package]\n        try:\n            with tracer.start_as_current_span(cmd[0]) as span:\n                span.set_attribute(\"argv\", cmd)\n                output = check_output(cmd, stderr=PIPE, universal_newlines=True)\n        except CalledProcessError as e:\n            raise PackageError(f\"Could not list packages in apt-cache: {e.stderr}\") from None\n\n        pkg_groups = output.strip().split(\"\\n\\n\")\n        keys = (\"Package\", \"Architecture\", \"Version\")\n\n        for pkg_raw in pkg_groups:\n            lines = str(pkg_raw).splitlines()\n            vals: dict[str, str] = {}\n            for line in lines:\n                if line.startswith(keys):\n                    items = line.split(\":\", 1)\n                    vals[items[0]] = items[1].strip()\n                else:\n                    continue\n\n            epoch, split_version = DebianPackage._get_epoch_from_version(vals[\"Version\"])\n            pkg = DebianPackage(\n                name=vals[\"Package\"],\n                version=split_version,\n                epoch=epoch,\n                arch=vals[\"Architecture\"],\n                state=PackageState.Available,\n            )\n\n            if (pkg.arch == \"all\" or pkg.arch == arch) and (\n                version == \"\" or str(pkg.version) == version\n            ):\n                return pkg\n\n        # If we didn'"'"'t find it, fail through\n        raise PackageNotFoundError(f\"Package {package}.{arch} is not in the apt cache!\")\n\n\nclass Version:\n    \"\"\"An abstraction around package versions.\n\n    This seems like it should be strictly unnecessary, except that `apt_pkg` is not usable inside a\n    venv, and wedging version comparisons into `DebianPackage` would overcomplicate it.\n\n    This class implements the algorithm found here:\n    https://www.debian.org/doc/debian-policy/ch-controlfields.html#version\n    \"\"\"\n\n    def __init__(self, version: str, epoch: str):\n        self._version = version\n        self._epoch = epoch or \"\"\n\n    def __repr__(self):\n        \"\"\"Represent the package.\"\"\"\n        return f\"<{self.__module__}.{type(self).__name__}: {self.__dict__}>\"\n\n    def __str__(self):\n        \"\"\"Return human-readable representation of the package.\"\"\"\n        epoch = f\"{self._epoch}:\" if self._epoch else \"\"\n        return f\"{epoch}{self._version}\"\n\n    @property\n    def epoch(self):\n        \"\"\"Returns the epoch for a package. May be empty.\"\"\"\n        return self._epoch\n\n    @property\n    def number(self) -> str:\n        \"\"\"Returns the version number for a package.\"\"\"\n        return self._version\n\n    def _get_parts(self, version: str) -> tuple[str, str]:\n        \"\"\"Separate the version into component upstream and Debian pieces.\"\"\"\n        try:\n            version.rindex(\"-\")\n        except ValueError:\n            # No hyphens means no Debian version\n            return version, \"0\"\n\n        upstream, debian = version.rsplit(\"-\", 1)\n        return upstream, debian\n\n    def _listify(self, revision: str) -> list[str | int]:\n        \"\"\"Split a revision string into a list.\n\n        This list is comprised of  alternating between strings and numbers,\n        padded on either end to always be \"str, int, str, int...\" and\n        always be of even length.  This allows us to trivially implement the\n        comparison algorithm described.\n        \"\"\"\n        result: list[str | int] = []\n        while revision:\n            rev_1, remains = self._get_alphas(revision)\n            rev_2, remains = self._get_digits(remains)\n            result.extend([rev_1, rev_2])\n            revision = remains\n        return result\n\n    def _get_alphas(self, revision: str) -> tuple[str, str]:\n        \"\"\"Return a tuple of the first non-digit characters of a revision.\"\"\"\n        # get the index of the first digit\n        for i, char in enumerate(revision):\n            if char.isdigit():\n                if i == 0:\n                    return \"\", revision\n                return revision[0:i], revision[i:]\n        # string is entirely alphas\n        return revision, \"\"\n\n    def _get_digits(self, revision: str) -> tuple[int, str]:\n        \"\"\"Return a tuple of the first integer characters of a revision.\"\"\"\n        # If the string is empty, return (0,'"'"''"'"')\n        if not revision:\n            return 0, \"\"\n        # get the index of the first non-digit\n        for i, char in enumerate(revision):\n            if not char.isdigit():\n                if i == 0:\n                    return 0, revision\n                return int(revision[0:i]), revision[i:]\n        # string is entirely digits\n        return int(revision), \"\"\n\n    def _dstringcmp(self, a: str, b: str) -> Literal[-1, 0, 1]:\n        \"\"\"Debian package version string section lexical sort algorithm.\n\n        The lexical comparison is a comparison of ASCII values modified so\n        that all the letters sort earlier than all the non-letters and so that\n        a tilde sorts before anything, even the end of a part.\n        \"\"\"\n        if a == b:\n            return 0\n        try:\n            for i, char in enumerate(a):\n                if char == b[i]:\n                    continue\n                # \"a tilde sorts before anything, even the end of a part\"\n                # (emptyness)\n                if char == \"~\":\n                    return -1\n                if b[i] == \"~\":\n                    return 1\n                # \"all the letters sort earlier than all the non-letters\"\n                if char.isalpha() and not b[i].isalpha():\n                    return -1\n                if not char.isalpha() and b[i].isalpha():\n                    return 1\n                # otherwise lexical sort\n                if ord(char) > ord(b[i]):\n                    return 1\n                if ord(char) < ord(b[i]):\n                    return -1\n        except IndexError:\n            # a is longer than b but otherwise equal, greater unless there are tildes\n            # FIXME: type checker thinks \"char\" is possibly unbound as it'"'"'s a loop variable\n            #        but it won'"'"'t be since the IndexError can only occur inside the loop\n            #        -- I'"'"'d like to refactor away this `try ... except` anyway\n            if char == \"~\":  # pyright: ignore[reportPossiblyUnboundVariable]\n                return -1\n            return 1\n        # if we get here, a is shorter than b but otherwise equal, so check for tildes...\n        if b[len(a)] == \"~\":\n            return 1\n        return -1\n\n    def _compare_revision_strings(self, first: str, second: str) -> Literal[-1, 0, 1]:\n        \"\"\"Compare two debian revision strings.\"\"\"\n        if first == second:\n            return 0\n\n        # listify pads results so that we will always be comparing ints to ints\n        # and strings to strings (at least until we fall off the end of a list)\n        first_list = self._listify(first)\n        second_list = self._listify(second)\n        if first_list == second_list:\n            return 0\n        try:\n            for i, item in enumerate(first_list):\n                # explicitly raise IndexError if we'"'"'ve fallen off the edge of list2\n                if i >= len(second_list):\n                    raise IndexError\n                other = second_list[i]\n                # if the items are equal, next\n                if item == other:\n                    continue\n                # numeric comparison\n                if isinstance(item, int):\n                    assert isinstance(other, int)\n                    if item > other:\n                        return 1\n                    if item < other:\n                        return -1\n                else:\n                    # string comparison\n                    assert isinstance(other, str)\n                    return self._dstringcmp(item, other)\n        except IndexError:\n            # rev1 is longer than rev2 but otherwise equal, hence greater\n            # ...except for goddamn tildes\n            # FIXME: bug?? we return 1 in both cases\n            # FIXME: first_list[len(second_list)] should be a string\n            #        why are we indexing to 0 twice?\n            if first_list[len(second_list)][0][0] == \"~\":  # type: ignore\n                return 1\n            return 1\n        # rev1 is shorter than rev2 but otherwise equal, hence lesser\n        # ...except for goddamn tildes\n        # FIXME: bug?? we return -1 in both cases\n        # FIXME: first_list[len(second_list)] should be a string, why are we indexing to 0 twice?\n        if second_list[len(first_list)][0][0] == \"~\":  # type: ignore\n            return -1\n        return -1\n\n    def _compare_version(self, other: Version) -> Literal[-1, 0, 1]:\n        if (self.number, self.epoch) == (other.number, other.epoch):\n            return 0\n\n        if self.epoch < other.epoch:\n            return -1\n        if self.epoch > other.epoch:\n            return 1\n\n        # If none of these are true, follow the algorithm\n        upstream_version, debian_version = self._get_parts(self.number)\n        other_upstream_version, other_debian_version = self._get_parts(other.number)\n\n        upstream_cmp = self._compare_revision_strings(upstream_version, other_upstream_version)\n        if upstream_cmp != 0:\n            return upstream_cmp\n\n        debian_cmp = self._compare_revision_strings(debian_version, other_debian_version)\n        if debian_cmp != 0:\n            return debian_cmp\n\n        return 0\n\n    def __lt__(self, other: Version) -> bool:\n        \"\"\"Less than magic method impl.\"\"\"\n        return self._compare_version(other) < 0\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"Equality magic method impl.\"\"\"\n        if not isinstance(other, Version):\n            return False\n        return self._compare_version(other) == 0\n\n    def __gt__(self, other: Version) -> bool:\n        \"\"\"Greater than magic method impl.\"\"\"\n        return self._compare_version(other) > 0\n\n    def __le__(self, other: Version) -> bool:\n        \"\"\"Less than or equal to magic method impl.\"\"\"\n        return self.__eq__(other) or self.__lt__(other)\n\n    def __ge__(self, other: Version) -> bool:\n        \"\"\"Greater than or equal to magic method impl.\"\"\"\n        return self.__gt__(other) or self.__eq__(other)\n\n    def __ne__(self, other: object) -> bool:\n        \"\"\"Not equal to magic method impl.\"\"\"\n        return not self.__eq__(other)\n\n\n@typing.overload\ndef add_package(\n    package_names: str,\n    version: str | None = \"\",\n    arch: str | None = \"\",\n    update_cache: bool = False,\n) -> DebianPackage: ...\n@typing.overload\ndef add_package(\n    package_names: list[str],\n    version: str | None = \"\",\n    arch: str | None = \"\",\n    update_cache: bool = False,\n) -> DebianPackage | list[DebianPackage]: ...\ndef add_package(\n    package_names: str | list[str],\n    version: str | None = \"\",\n    arch: str | None = \"\",\n    update_cache: bool = False,\n) -> DebianPackage | list[DebianPackage]:\n    \"\"\"Add a package or list of packages to the system.\n\n    Args:\n        package_names: single package name, or list of package names\n        name: the name(s) of the package(s)\n        version: an (Optional) version as a string. Defaults to the latest known\n        arch: an optional architecture for the package\n        update_cache: whether or not to run `apt-get update` prior to operating\n\n    Raises:\n        TypeError if no package name is given, or explicit version is set for multiple packages\n        PackageError: if packages fail to install, including if any packages aren'"'"'t found in the\n            cache\n    \"\"\"\n    cache_refreshed = False\n    if update_cache:\n        update()\n        cache_refreshed = True\n\n    package_names = [package_names] if isinstance(package_names, str) else package_names\n    if not package_names:\n        raise TypeError(\"Expected at least one package name to add, received zero!\")\n\n    if len(package_names) != 1 and version:\n        raise TypeError(\n            \"Explicit version should not be set if more than one package is being added!\"\n        )\n\n    succeeded: list[DebianPackage] = []\n    retry: list[str] = []\n    failed: list[str] = []\n\n    for p in package_names:\n        pkg, _ = _add(p, version, arch)\n        if isinstance(pkg, DebianPackage):\n            succeeded.append(pkg)\n        elif cache_refreshed:\n            logger.warning(\"failed to locate and install/update '"'"'%s'"'"'\", pkg)\n            failed.append(p)\n        else:\n            logger.warning(\"failed to locate and install/update '"'"'%s'"'"', will retry later\", pkg)\n            retry.append(p)\n\n    if retry:\n        logger.info(\"updating the apt-cache and retrying installation of failed packages.\")\n        update()\n\n        for p in retry:\n            pkg, _ = _add(p, version, arch)\n            if isinstance(pkg, DebianPackage):\n                succeeded.append(pkg)\n            else:\n                failed.append(p)\n\n    if failed:\n        raise PackageError(f\"Failed to install packages: {'"'"', '"'"'.join(failed)}\")\n\n    return succeeded[0] if len(succeeded) == 1 else succeeded\n\n\ndef _add(\n    name: str,\n    version: str | None = \"\",\n    arch: str | None = \"\",\n) -> tuple[DebianPackage, Literal[True]] | tuple[str, Literal[False]]:\n    \"\"\"Add a package to the system.\n\n    Args:\n        name: the name(s) of the package(s)\n        version: an (Optional) version as a string. Defaults to the latest known\n        arch: an optional architecture for the package\n\n    Returns: a tuple of `DebianPackage` if found, or a :str: if it is not, and\n        a boolean indicating success\n    \"\"\"\n    try:\n        pkg = DebianPackage.from_system(name, version, arch)\n        pkg.ensure(state=PackageState.Present)\n        return pkg, True\n    except PackageNotFoundError:\n        return name, False\n\n\n@typing.overload\ndef remove_package(\n    package_names: str,\n) -> DebianPackage: ...\n@typing.overload\ndef remove_package(\n    package_names: list[str],\n) -> DebianPackage | list[DebianPackage]: ...\ndef remove_package(\n    package_names: str | list[str],\n) -> DebianPackage | list[DebianPackage]:\n    \"\"\"Remove package(s) from the system.\n\n    Args:\n        package_names: the name of a package\n\n    Raises:\n        TypeError: if no packages are provided\n    \"\"\"\n    packages: list[DebianPackage] = []\n\n    package_names = [package_names] if isinstance(package_names, str) else package_names\n    if not package_names:\n        raise TypeError(\"Expected at least one package name to add, received zero!\")\n\n    for p in package_names:\n        try:\n            pkg = DebianPackage.from_installed_package(p)\n            pkg.ensure(state=PackageState.Absent)\n            packages.append(pkg)\n        except PackageNotFoundError:  # noqa: PERF203\n            logger.info(\"package '"'"'%s'"'"' was requested for removal, but it was not installed.\", p)\n\n    # the list of packages will be empty when no package is removed\n    logger.debug(\"packages: '"'"'%s'"'"'\", packages)\n    return packages[0] if len(packages) == 1 else packages\n\n\ndef update() -> None:\n    \"\"\"Update the apt cache via `apt-get update`.\"\"\"\n    cmd = [\"apt-get\", \"update\", \"--error-on=any\"]\n    try:\n        with tracer.start_as_current_span(cmd[0]) as span:\n            span.set_attribute(\"argv\", cmd)\n            subprocess.run(cmd, capture_output=True, check=True)\n    except CalledProcessError as e:\n        logger.error(\n            \"%s:\\nstdout:\\n%s\\nstderr:\\n%s\",\n            \" \".join(cmd),\n            e.stdout.decode(),\n            e.stderr.decode(),\n        )\n        raise\n\n\ndef import_key(key: str) -> str:\n    \"\"\"Import an ASCII Armor key.\n\n    A Radix64 format keyid is also supported for backwards\n    compatibility. In this case Ubuntu keyserver will be\n    queried for a key via HTTPS by its keyid. This method\n    is less preferable because https proxy servers may\n    require traffic decryption which is equivalent to a\n    man-in-the-middle attack (a proxy server impersonates\n    keyserver TLS certificates and has to be explicitly\n    trusted by the system).\n\n    Args:\n        key: A GPG key in ASCII armor format, including BEGIN\n            and END markers or a keyid.\n\n    Returns:\n        The GPG key filename written.\n\n    Raises:\n        GPGKeyError if the key could not be imported\n    \"\"\"\n    key = key.strip()\n    if \"-\" in key or \"\\n\" in key:\n        # Send everything not obviously a keyid to GPG to import, as\n        # we trust its validation better than our own. eg. handling\n        # comments before the key.\n        logger.debug(\"PGP key found (looks like ASCII Armor format)\")\n        if (\n            \"-----BEGIN PGP PUBLIC KEY BLOCK-----\" in key\n            and \"-----END PGP PUBLIC KEY BLOCK-----\" in key\n        ):\n            logger.debug(\"Writing provided PGP key in the binary format\")\n            key_bytes = key.encode(\"utf-8\")\n            key_name = DebianRepository._get_keyid_by_gpg_key(key_bytes)\n            key_gpg = DebianRepository._dearmor_gpg_key(key_bytes)\n            gpg_key_filename = os.path.join(_GPG_KEY_DIR, f\"{key_name}.gpg\")\n            DebianRepository._write_apt_gpg_keyfile(\n                key_name=gpg_key_filename, key_material=key_gpg\n            )\n            return gpg_key_filename\n        else:\n            raise GPGKeyError(\"ASCII armor markers missing from GPG key\")\n    else:\n        logger.warning(\n            \"PGP key found (looks like Radix64 format). \"\n            \"SECURELY importing PGP key from keyserver; \"\n            \"full key not provided.\"\n        )\n        # as of bionic add-apt-repository uses curl with an HTTPS keyserver URL\n        # to retrieve GPG keys. `apt-key adv` command is deprecated as is\n        # apt-key in general as noted in its manpage. See lp:1433761 for more\n        # history. Instead, /etc/apt/trusted.gpg.d is used directly to drop\n        # gpg\n        key_asc = DebianRepository._get_key_by_keyid(key)\n        # write the key in GPG format so that apt-key list shows it\n        key_gpg = DebianRepository._dearmor_gpg_key(key_asc.encode(\"utf-8\"))\n        gpg_key_filename = os.path.join(_GPG_KEY_DIR, f\"{key}.gpg\")\n        DebianRepository._write_apt_gpg_keyfile(key_name=gpg_key_filename, key_material=key_gpg)\n        return gpg_key_filename\n\n\nclass InvalidSourceError(Error):\n    \"\"\"Exceptions for invalid source entries.\"\"\"\n\n\nclass GPGKeyError(Error):\n    \"\"\"Exceptions for GPG keys.\"\"\"\n\n\nclass DebianRepository:\n    \"\"\"An abstraction to represent a repository.\"\"\"\n\n    _deb822_stanza: _Deb822Stanza | None = None\n    \"\"\"set by Deb822Stanza after creating a DebianRepository\"\"\"\n\n    def __init__(\n        self,\n        enabled: bool,\n        repotype: str,\n        uri: str,\n        release: str,\n        groups: list[str],\n        filename: str = \"\",\n        gpg_key_filename: str = \"\",\n        options: dict[str, str] | None = None,\n    ):\n        self._enabled = enabled\n        self._repotype = repotype\n        self._uri = uri\n        self._release = release\n        self._groups = groups\n        self._filename = filename\n        self._gpg_key_filename = gpg_key_filename\n        self._options = options\n\n    @property\n    def enabled(self):\n        \"\"\"Return whether or not the repository is enabled.\"\"\"\n        return self._enabled\n\n    @property\n    def repotype(self):\n        \"\"\"Return whether it is binary or source.\"\"\"\n        return self._repotype\n\n    @property\n    def uri(self):\n        \"\"\"Return the URI.\"\"\"\n        return self._uri\n\n    @property\n    def release(self):\n        \"\"\"Return which Debian/Ubuntu releases it is valid for.\"\"\"\n        return self._release\n\n    @property\n    def groups(self):\n        \"\"\"Return the enabled package groups.\"\"\"\n        return self._groups\n\n    @property\n    def filename(self):\n        \"\"\"Returns the filename for a repository.\"\"\"\n        return self._filename\n\n    @filename.setter\n    def filename(self, fname: str) -> None:\n        \"\"\"Set the filename used when a repo is written back to disk.\n\n        Args:\n            fname: a filename to write the repository information to.\n        \"\"\"\n        if not fname.endswith((\".list\", \".sources\")):\n            raise InvalidSourceError(\"apt source filenames should end in .list or .sources!\")\n        self._filename = fname\n\n    @property\n    def gpg_key(self):\n        \"\"\"Returns the path to the GPG key for this repository.\"\"\"\n        if not self._gpg_key_filename and self._deb822_stanza is not None:\n            self._gpg_key_filename = self._deb822_stanza.get_gpg_key_filename()\n        return self._gpg_key_filename\n\n    @property\n    def options(self):\n        \"\"\"Returns any additional repo options which are set.\"\"\"\n        return self._options\n\n    def make_options_string(self, include_signed_by: bool = True) -> str:\n        \"\"\"Generate the complete one-line-style options string for a repository.\n\n        Combining `gpg_key`, if set (and include_signed_by is True), with any other\n        provided options to form the options section of a one-line-style definition.\n        \"\"\"\n        options = self._options if self._options else {}\n        if include_signed_by and self.gpg_key:\n            options[\"signed-by\"] = self.gpg_key\n        if not options:\n            return \"\"\n        pairs = (f\"{k}={v}\" for k, v in sorted(options.items()))\n        return \"[{}] \".format(\" \".join(pairs))\n\n    @staticmethod\n    def prefix_from_uri(uri: str) -> str:\n        \"\"\"Get a repo list prefix from the uri, depending on whether a path is set.\"\"\"\n        uridetails = urlparse(uri)\n        path = (\n            uridetails.path.lstrip(\"/\").replace(\"/\", \"-\") if uridetails.path else uridetails.netloc\n        )\n        return f\"/etc/apt/sources.list.d/{path}\"\n\n    @staticmethod\n    def from_repo_line(repo_line: str, write_file: bool | None = True) -> DebianRepository:\n        \"\"\"Instantiate a new `DebianRepository` from a `sources.list` entry line.\n\n        Args:\n            repo_line: a string representing a repository entry\n            write_file: boolean to enable writing the new repo to disk. True by default.\n                Expect it to result in an add-apt-repository call under the hood, like:\n                    add-apt-repository --no-update --sourceslist=\"$repo_line\"\n        \"\"\"\n        repo = RepositoryMapping._parse(\n            repo_line,\n            filename=\"UserInput\",  # temp filename\n        )\n        repo.filename = repo._make_filename()\n        if write_file:\n            _add_repository(repo)\n        return repo\n\n    def _make_filename(self) -> str:\n        \"\"\"Construct a filename from uri and release.\n\n        For internal use when a filename isn'"'"'t set.\n        Should match the filename written to by add-apt-repository.\n        \"\"\"\n        return \"{}-{}.list\".format(\n            DebianRepository.prefix_from_uri(self.uri),\n            self.release.replace(\"/\", \"-\"),\n        )\n\n    def disable(self) -> None:\n        \"\"\"Remove this repository by disabling it in the source file.\n\n        WARNING: This method does NOT alter the `self.enabled` flag.\n\n        WARNING: disable is currently not implemented for repositories defined\n        by a deb822 stanza. Raises a NotImplementedError in this case.\n        \"\"\"\n        if self._deb822_stanza is not None:\n            raise NotImplementedError(\n                \"Disabling a repository defined by a deb822 format source is not implemented.\"\n                \" Please raise an issue if you require this feature.\"\n            )\n        searcher = f\"{self.repotype} {self.make_options_string()}{self.uri} {self.release}\"\n        with tracer.start_as_current_span(\"disable source\") as span:\n            span.set_attribute(\"filename\", self._filename)\n            with fileinput.input(self._filename, inplace=True) as lines:\n                for line in lines:\n                    if re.match(rf\"^{re.escape(searcher)}\\s\", line):\n                        print(f\"# {line}\", end=\"\")\n                    else:\n                        print(line, end=\"\")\n\n    def import_key(self, key: str) -> None:\n        \"\"\"Import an ASCII Armor key.\n\n        A Radix64 format keyid is also supported for backwards\n        compatibility. In this case Ubuntu keyserver will be\n        queried for a key via HTTPS by its keyid. This method\n        is less preferable because https proxy servers may\n        require traffic decryption which is equivalent to a\n        man-in-the-middle attack (a proxy server impersonates\n        keyserver TLS certificates and has to be explicitly\n        trusted by the system).\n\n        Args:\n          key: A GPG key in ASCII armor format,\n                      including BEGIN and END markers or a keyid.\n\n        Raises:\n          GPGKeyError if the key could not be imported\n        \"\"\"\n        self._gpg_key_filename = import_key(key)\n\n    @staticmethod\n    def _get_keyid_by_gpg_key(key_material: bytes) -> str:\n        \"\"\"Get a GPG key fingerprint by GPG key material.\n\n        Gets a GPG key fingerprint (40-digit, 160-bit) by the ASCII armor-encoded\n        or binary GPG key material. Can be used, for example, to generate file\n        names for keys passed via charm options.\n        \"\"\"\n        # Use the same gpg command for both Xenial and Bionic\n        cmd = [\"gpg\", \"--with-colons\", \"--with-fingerprint\"]\n        with tracer.start_as_current_span(cmd[0]) as span:\n            span.set_attribute(\"argv\", cmd)\n            ps = subprocess.run(cmd, capture_output=True, input=key_material)\n            out, err = ps.stdout.decode(), ps.stderr.decode()\n        if \"gpg: no valid OpenPGP data found.\" in err:\n            raise GPGKeyError(\"Invalid GPG key material provided\")\n        # from gnupg2 docs: fpr :: Fingerprint (fingerprint is in field 10)\n        result = re.search(r\"^fpr:{9}([0-9A-F]{40}):$\", out, re.MULTILINE)\n        assert result is not None\n        return result.group(1)\n\n    @staticmethod\n    def _get_key_by_keyid(keyid: str) -> str:\n        \"\"\"Get a key via HTTPS from the Ubuntu keyserver.\n\n        Different key ID formats are supported by SKS keyservers (the longer ones\n        are more secure, see \"dead beef attack\" and https://evil32.com/). Since\n        HTTPS is used, if SSLBump-like HTTPS proxies are in place, they will\n        impersonate keyserver.ubuntu.com and generate a certificate with\n        keyserver.ubuntu.com in the CN field or in SubjAltName fields of a\n        certificate. If such proxy behavior is expected it is necessary to add the\n        CA certificate chain containing the intermediate CA of the SSLBump proxy to\n        every machine that this code runs on via ca-certs cloud-init directive (via\n        cloudinit-userdata model-config) or via other means (such as through a\n        custom charm option). Also note that DNS resolution for the hostname in a\n        URL is done at a proxy server - not at the client side.\n        8-digit (32 bit) key ID\n        https://keyserver.ubuntu.com/pks/lookup?search=0x4652B4E6\n        16-digit (64 bit) key ID\n        https://keyserver.ubuntu.com/pks/lookup?search=0x6E85A86E4652B4E6\n        40-digit key ID:\n        https://keyserver.ubuntu.com/pks/lookup?search=0x35F77D63B5CEC106C577ED856E85A86E4652B4E6\n\n        Args:\n          keyid: An 8, 16 or 40 hex digit keyid to find a key for\n\n        Returns:\n          A string containing key material for the specified GPG key id\n\n\n        Raises:\n          subprocess.CalledProcessError\n        \"\"\"\n        # options=mr - machine-readable output (disables html wrappers)\n        keyserver_url = (\n            \"https://keyserver.ubuntu.com\" \"/pks/lookup?op=get&options=mr&exact=on&search=0x{}\"\n        )\n        curl_cmd = [\"curl\", keyserver_url.format(keyid)]\n        with tracer.start_as_current_span(curl_cmd[0]) as span:\n            span.set_attribute(\"argv\", curl_cmd)\n            # use proxy server settings in order to retrieve the key\n            return check_output(curl_cmd).decode()\n\n    @staticmethod\n    def _dearmor_gpg_key(key_asc: bytes) -> bytes:\n        \"\"\"Convert a GPG key in the ASCII armor format to the binary format.\n\n        Args:\n          key_asc: A GPG key in ASCII armor format.\n\n        Returns:\n          A GPG key in binary format as a string\n\n        Raises:\n          GPGKeyError\n        \"\"\"\n        cmd = [\"gpg\", \"--dearmor\"]\n        with tracer.start_as_current_span(cmd[0]) as span:\n            span.set_attribute(\"argv\", cmd)\n            ps = subprocess.run(cmd, capture_output=True, input=key_asc)\n            out, err = ps.stdout, ps.stderr.decode()\n        if \"gpg: no valid OpenPGP data found.\" in err:\n            raise GPGKeyError(\n                \"Invalid GPG key material. Check your network setup\"\n                \" (MTU, routing, DNS) and/or proxy server settings\"\n                \" as well as destination keyserver status.\"\n            )\n        else:\n            return out\n\n    @staticmethod\n    def _write_apt_gpg_keyfile(key_name: str, key_material: bytes) -> None:\n        \"\"\"Write GPG key material into a file at a provided path.\n\n        Args:\n          key_name: A key name to use for a key file (could be a fingerprint)\n          key_material: A GPG key material (binary)\n        \"\"\"\n        with open(key_name, \"wb\") as keyf:\n            keyf.write(key_material)\n\n\ndef _repo_to_identifier(repo: DebianRepository) -> str:\n    \"\"\"Return str identifier derived from repotype, uri, and release.\n\n    Private method used to produce the identifiers used by RepositoryMapping.\n    \"\"\"\n    return f\"{repo.repotype}-{repo.uri}-{repo.release}\"\n\n\ndef _repo_to_line(repo: DebianRepository, include_signed_by: bool = True) -> str:\n    \"\"\"Return the one-per-line format repository definition.\"\"\"\n    return \"{prefix}{repotype} {options}{uri} {release} {groups}\".format(\n        prefix=\"\" if repo.enabled else \"#\",\n        repotype=repo.repotype,\n        options=repo.make_options_string(include_signed_by=include_signed_by),\n        uri=repo.uri,\n        release=repo.release,\n        groups=\" \".join(repo.groups),\n    )\n\n\nclass RepositoryMapping(Mapping[str, DebianRepository]):\n    \"\"\"An representation of known repositories.\n\n    Instantiation of `RepositoryMapping` will iterate through the\n    filesystem, parse out repository files in `/etc/apt/...`, and create\n    `DebianRepository` objects in this list.\n\n    Typical usage:\n\n        repositories = apt.RepositoryMapping()\n        repositories.add(DebianRepository(\n            enabled=True, repotype=\"deb\", uri=\"https://example.com\", release=\"focal\",\n            groups=[\"universe\"]\n        ))\n    \"\"\"\n\n    _apt_dir = \"/etc/apt\"\n    _sources_subdir = \"sources.list.d\"\n    _default_list_name = \"sources.list\"\n    _default_sources_name = \"ubuntu.sources\"\n    _last_errors: tuple[Error, ...] = ()\n\n    def __init__(self):\n        self._repository_map: dict[str, DebianRepository] = {}\n        self.default_file = os.path.join(self._apt_dir, self._default_list_name)\n        # ^ public attribute for backwards compatibility only\n        sources_dir = os.path.join(self._apt_dir, self._sources_subdir)\n        default_sources = os.path.join(sources_dir, self._default_sources_name)\n\n        # read sources.list if it exists\n        # ignore InvalidSourceError if ubuntu.sources also exists\n        # -- in this case, sources.list just contains a comment\n        if os.path.isfile(self.default_file):\n            try:\n                self.load(self.default_file)\n            except InvalidSourceError:\n                if not os.path.isfile(default_sources):\n                    raise\n\n        with tracer.start_as_current_span(\"load sources\"):\n            # read sources.list.d\n            for file in glob.iglob(os.path.join(sources_dir, \"*.list\")):\n                self.load(file)\n            for file in glob.iglob(os.path.join(sources_dir, \"*.sources\")):\n                self.load_deb822(file)\n\n    def __contains__(self, key: Any) -> bool:\n        \"\"\"Magic method for checking presence of repo in mapping.\n\n        Checks against the string names used to identify repositories.\n        \"\"\"\n        return key in self._repository_map\n\n    def __len__(self) -> int:\n        \"\"\"Return number of repositories in map.\"\"\"\n        return len(self._repository_map)\n\n    def __iter__(self) -> Iterator[DebianRepository]:  # pyright: ignore[reportIncompatibleMethodOverride]\n        \"\"\"Return iterator for RepositoryMapping.\n\n        Iterates over the DebianRepository values rather than the string names.\n        FIXME: this breaks the expectations of the Mapping abstract base class\n            for example when it provides methods like keys and items\n        \"\"\"\n        return iter(self._repository_map.values())\n\n    def __getitem__(self, repository_uri: str) -> DebianRepository:\n        \"\"\"Return a given `DebianRepository`.\"\"\"\n        return self._repository_map[repository_uri]\n\n    def __setitem__(self, repository_uri: str, repository: DebianRepository) -> None:\n        \"\"\"Add a `DebianRepository` to the cache.\"\"\"\n        self._repository_map[repository_uri] = repository\n\n    def load_deb822(self, filename: str) -> None:\n        \"\"\"Load a deb822 format repository source file into the cache.\n\n        In contrast to one-line-style, the deb822 format specifies a repository\n        using a multi-line stanza. Stanzas are separated by whitespace,\n        and each definition consists of lines that are either key: value pairs,\n        or continuations of the previous value.\n\n        Read more about the deb822 format here:\n            https://manpages.ubuntu.com/manpages/noble/en/man5/sources.list.5.html\n        For instance, ubuntu 24.04 (noble) lists its sources using deb822 style in:\n            /etc/apt/sources.list.d/ubuntu.sources\n        \"\"\"\n        with open(filename) as f:\n            repos, errors = self._parse_deb822_lines(f, filename=filename)\n        for repo in repos:\n            self._repository_map[_repo_to_identifier(repo)] = repo\n        if errors:\n            self._last_errors = tuple(errors)\n            logger.debug(\n                \"the following %d error(s) were encountered when reading deb822 sources:\\n%s\",\n                len(errors),\n                \"\\n\".join(str(e) for e in errors),\n            )\n        if repos:\n            logger.info(\"parsed %d apt package repositories from %s\", len(repos), filename)\n        else:\n            raise InvalidSourceError(f\"all repository lines in '"'"'{filename}'"'"' were invalid!\")\n\n    @classmethod\n    def _parse_deb822_lines(\n        cls,\n        lines: Iterable[str],\n        filename: str = \"\",\n    ) -> tuple[list[DebianRepository], list[InvalidSourceError]]:\n        \"\"\"Parse lines from a deb822 file into a list of repos and a list of errors.\n\n        The semantics of `_parse_deb822_lines` slightly different to `_parse`:\n            `_parse` reads a commented out line as an entry that is not enabled\n            `_parse_deb822_lines` strips out comments entirely when parsing a file into stanzas,\n                instead only reading the '"'"'Enabled'"'"' key to determine if an entry is enabled\n        \"\"\"\n        repos: list[DebianRepository] = []\n        errors: list[InvalidSourceError] = []\n        for numbered_lines in _iter_deb822_stanzas(lines):\n            try:\n                stanza = _Deb822Stanza(numbered_lines=numbered_lines, filename=filename)\n            except InvalidSourceError as e:  # noqa: PERF203\n                errors.append(e)\n            else:\n                repos.extend(stanza.repos)\n        return repos, errors\n\n    def load(self, filename: str):\n        \"\"\"Load a one-line-style format repository source file into the cache.\n\n        Args:\n          filename: the path to the repository file\n        \"\"\"\n        parsed: list[int] = []\n        skipped: list[int] = []\n        with open(filename) as f:\n            for n, line in enumerate(f, start=1):  # 1 indexed line numbers\n                try:\n                    repo = self._parse(line, filename)\n                except InvalidSourceError:  # noqa: PERF203\n                    skipped.append(n)\n                else:\n                    repo_identifier = _repo_to_identifier(repo)\n                    self._repository_map[repo_identifier] = repo\n                    parsed.append(n)\n                    logger.debug(\"parsed repo: '"'"'%s'"'"'\", repo_identifier)\n\n        if skipped:\n            skip_list = \", \".join(str(s) for s in skipped)\n            logger.debug(\"skipped the following lines in file '"'"'%s'"'"': %s\", filename, skip_list)\n\n        if parsed:\n            logger.info(\"parsed %d apt package repositories from %s\", len(parsed), filename)\n        else:\n            raise InvalidSourceError(f\"all repository lines in '"'"'{filename}'"'"' were invalid!\")\n\n    @staticmethod\n    def _parse(line: str, filename: str) -> DebianRepository:\n        \"\"\"Parse a line in a sources.list file.\n\n        Args:\n          line: a single line from `load` to parse\n          filename: the filename being read\n\n        Raises:\n          InvalidSourceError if the source type is unknown\n        \"\"\"\n        enabled = True\n        repotype = uri = release = gpg_key = \"\"\n        options = {}\n        groups = []\n\n        line = line.strip()\n        if line.startswith(\"#\"):\n            enabled = False\n            line = line[1:]\n\n        # Check for \"#\" in the line and treat a part after it as a comment then strip it off.\n        i = line.find(\"#\")\n        if i > 0:\n            line = line[:i]\n\n        # Split a source into substrings to initialize a new repo.\n        source = line.strip()\n        if source:\n            # Match any repo options, and get a dict representation.\n            for v in re.findall(OPTIONS_MATCHER, source):\n                opts = dict(o.split(\"=\") for o in v.strip(\"[]\").split())\n                # Extract the '"'"'signed-by'"'"' option for the gpg_key\n                gpg_key = opts.pop(\"signed-by\", \"\")\n                options = opts\n\n            # Remove any options from the source string and split the string into chunks\n            source = re.sub(OPTIONS_MATCHER, \"\", source)\n            chunks = source.split()\n\n            # Check we'"'"'ve got a valid list of chunks\n            if len(chunks) < 3 or chunks[0] not in VALID_SOURCE_TYPES:\n                raise InvalidSourceError(\"An invalid sources line was found in %s!\", filename)\n\n            repotype = chunks[0]\n            uri = chunks[1]\n            release = chunks[2]\n            groups = chunks[3:]\n\n            return DebianRepository(\n                enabled, repotype, uri, release, groups, filename, gpg_key, options\n            )\n        else:\n            raise InvalidSourceError(\"An invalid sources line was found in %s!\", filename)\n\n    def add(  # noqa: D417  # undocumented-param: default_filename intentionally undocumented\n        self, repo: DebianRepository, default_filename: bool | None = False\n    ) -> None:\n        \"\"\"Add a new repository to the system using add-apt-repository.\n\n        Args:\n            repo: a DebianRepository object\n                if repo.enabled is falsey, will return without adding the repository\n        Raises:\n            CalledProcessError: if there'"'"'s an error running apt-add-repository\n\n        WARNING: Does not associate the repository with a signing key.\n        Use `import_key` to add a signing key globally.\n\n        WARNING: if repo.enabled is falsey, will return without adding the repository\n\n        WARNING: Don'"'"'t forget to call `apt.update` before installing any packages!\n        Or call `apt.add_package` with `update_cache=True`.\n\n        WARNING: the default_filename keyword argument is provided for backwards compatibility\n        only. It is not used, and was not used in the previous revision of this library.\n        \"\"\"\n        if not repo.enabled:\n            logger.warning(\n                (\n                    \"Returning from RepositoryMapping.add(repo=%s) without adding the repo\"\n                    \" because repo.enabled is %s\"\n                ),\n                repo,\n                repo.enabled,\n            )\n            return\n        _add_repository(repo)\n        self._repository_map[_repo_to_identifier(repo)] = repo\n\n    def disable(self, repo: DebianRepository) -> None:\n        \"\"\"Remove a repository by disabling it in the source file.\n\n        WARNING: disable is currently not implemented for repositories defined\n        by a deb822 stanza, and will raise a NotImplementedError if called on one.\n\n        WARNING: This method does NOT alter the `.enabled` flag on the DebianRepository.\n        \"\"\"\n        repo.disable()\n        self._repository_map[_repo_to_identifier(repo)] = repo\n        # ^ adding to map on disable seems like a bug, but this is the previous behaviour\n\n\ndef _add_repository(\n    repo: DebianRepository,\n    remove: bool = False,\n    update_cache: bool = False,\n) -> None:\n    line = _repo_to_line(repo, include_signed_by=False)\n    key_file = repo.gpg_key\n    if key_file and not remove and not os.path.exists(key_file):\n        msg = (\n            \"Adding repository '"'"'%s'"'"' with add-apt-repository.\"\n            \" Key file '"'"'%s'"'"' does not exist.\"\n            \" Ensure it is imported correctly to use this repository.\"\n        )\n        logger.warning(msg, line, key_file)\n    cmd = [\n        \"add-apt-repository\",\n        \"--yes\",\n        \"--sourceslist=\" + line,\n    ]\n    if remove:\n        cmd.append(\"--remove\")\n    if not update_cache:\n        cmd.append(\"--no-update\")\n    logger.info(\"%s\", cmd)\n    try:\n        with tracer.start_as_current_span(cmd[0]) as span:\n            span.set_attribute(\"argv\", cmd)\n            subprocess.run(cmd, check=True, capture_output=True)\n    except CalledProcessError as e:\n        logger.error(\n            \"subprocess.run(%s):\\nstdout:\\n%s\\nstderr:\\n%s\",\n            cmd,\n            e.stdout.decode(),\n            e.stderr.decode(),\n        )\n        raise\n\n\nclass _Deb822Stanza:\n    \"\"\"Representation of a stanza from a deb822 source file.\n\n    May define multiple DebianRepository objects.\n    \"\"\"\n\n    def __init__(self, numbered_lines: list[tuple[int, str]], filename: str = \"\"):\n        self._filename = filename\n        self._numbered_lines = numbered_lines\n        if not numbered_lines:\n            self._repos = ()\n            self._gpg_key_filename = \"\"\n            self._gpg_key_from_stanza = None\n            return\n        options, line_numbers = _deb822_stanza_to_options(numbered_lines)\n        repos, gpg_key_info = _deb822_options_to_repos(\n            options, line_numbers=line_numbers, filename=filename\n        )\n        for repo in repos:\n            repo._deb822_stanza = self\n        self._repos = repos\n        self._gpg_key_filename, self._gpg_key_from_stanza = gpg_key_info\n\n    @property\n    def repos(self) -> tuple[DebianRepository, ...]:\n        \"\"\"The repositories defined by this deb822 stanza.\"\"\"\n        return self._repos\n\n    def get_gpg_key_filename(self) -> str:\n        \"\"\"Return the path to the GPG key for this stanza.\n\n        Import the key first, if the key itself was provided in the stanza.\n        Return an empty string if no filename or key was provided.\n        \"\"\"\n        if self._gpg_key_filename:\n            return self._gpg_key_filename\n        if self._gpg_key_from_stanza is None:\n            return \"\"\n        # a gpg key was provided in the stanza\n        # and we haven'"'"'t already imported it\n        self._gpg_key_filename = import_key(self._gpg_key_from_stanza)\n        return self._gpg_key_filename\n\n\nclass MissingRequiredKeyError(InvalidSourceError):\n    \"\"\"Missing a required value in a source file.\"\"\"\n\n    def __init__(self, message: str = \"\", *, file: str, line: int | None, key: str) -> None:\n        super().__init__(message, file, line, key)\n        self.file = file\n        self.line = line\n        self.key = key\n\n\nclass BadValueError(InvalidSourceError):\n    \"\"\"Bad value for an entry in a source file.\"\"\"\n\n    def __init__(\n        self,\n        message: str = \"\",\n        *,\n        file: str,\n        line: int | None,\n        key: str,\n        value: str,\n    ) -> None:\n        super().__init__(message, file, line, key, value)\n        self.file = file\n        self.line = line\n        self.key = key\n        self.value = value\n\n\ndef _iter_deb822_stanzas(lines: Iterable[str]) -> Iterator[list[tuple[int, str]]]:\n    \"\"\"Given lines from a deb822 format file, yield a stanza of lines.\n\n    Args:\n        lines: an iterable of lines from a deb822 sources file\n\n    Yields:\n        lists of numbered lines (a tuple of line number and line) that make up\n        a deb822 stanza, with comments stripped out (but accounted for in line numbering)\n    \"\"\"\n    current_stanza: list[tuple[int, str]] = []\n    for n, line in enumerate(lines, start=1):  # 1 indexed line numbers\n        if not line.strip():  # blank lines separate stanzas\n            if current_stanza:\n                yield current_stanza\n                current_stanza = []\n            continue\n        content, _delim, _comment = line.partition(\"#\")\n        if content.strip():  # skip (potentially indented) comment line\n            current_stanza.append((n, content.rstrip()))  # preserve indent\n    if current_stanza:\n        yield current_stanza\n\n\ndef _deb822_stanza_to_options(\n    lines: Iterable[tuple[int, str]],\n) -> tuple[dict[str, str], dict[str, int]]:\n    \"\"\"Turn numbered lines into a dict of options and a dict of line numbers.\n\n    Args:\n        lines: an iterable of numbered lines (a tuple of line number and line)\n\n    Returns:\n        a dictionary of option names to (potentially multiline) values, and\n        a dictionary of option names to starting line number\n    \"\"\"\n    parts: dict[str, list[str]] = {}\n    line_numbers: dict[str, int] = {}\n    current = None\n    for n, line in lines:\n        assert \"#\" not in line  # comments should be stripped out\n        if line.startswith(\" \"):  # continuation of previous key'"'"'s value\n            assert current is not None\n            parts[current].append(line.rstrip())  # preserve indent\n            continue\n        raw_key, _, raw_value = line.partition(\":\")\n        current = raw_key.strip()\n        parts[current] = [raw_value.strip()]\n        line_numbers[current] = n\n    options = {k: \"\\n\".join(v) for k, v in parts.items()}\n    return options, line_numbers\n\n\ndef _deb822_options_to_repos(\n    options: dict[str, str], line_numbers: Mapping[str, int] = {}, filename: str = \"\"\n) -> tuple[tuple[DebianRepository, ...], tuple[str, str | None]]:\n    \"\"\"Return a collections of DebianRepository objects defined by this deb822 stanza.\n\n    Args:\n        options: a dictionary of deb822 field names to string options\n        line_numbers: a dictionary of field names to line numbers (for error messages)\n        filename: the file the options were read from (for repository object and errors)\n\n    Returns:\n        a tuple of `DebianRepository`s, and\n        a tuple of the gpg key filename and optional in-stanza provided key itself\n\n    Raises:\n      InvalidSourceError if any options are malformed or required options are missing\n    \"\"\"\n    # Enabled\n    enabled_field = options.pop(\"Enabled\", \"yes\")\n    if enabled_field == \"yes\":\n        enabled = True\n    elif enabled_field == \"no\":\n        enabled = False\n    else:\n        raise BadValueError(\n            \"Must be one of yes or no (default: yes).\",\n            file=filename,\n            line=line_numbers.get(\"Enabled\"),\n            key=\"Enabled\",\n            value=enabled_field,\n        )\n    # Signed-By\n    gpg_key_file = options.pop(\"Signed-By\", \"\")\n    gpg_key_from_stanza: str | None = None\n    if \"\\n\" in gpg_key_file:\n        # actually a literal multi-line gpg-key rather than a filename\n        gpg_key_from_stanza = gpg_key_file\n        gpg_key_file = \"\"\n    # Types\n    try:\n        repotypes = options.pop(\"Types\").split()\n        uris = options.pop(\"URIs\").split()\n        suites = options.pop(\"Suites\").split()\n    except KeyError as e:\n        [key] = e.args\n        raise MissingRequiredKeyError(\n            key=key,\n            line=min(line_numbers.values()) if line_numbers else None,\n            file=filename,\n        ) from e\n    # Components\n    # suite can specify an exact path, in which case the components must be omitted\n    # and suite must end with a slash (/).\n    # If suite does not specify an exact path, at least one component must be present.\n    # https://manpages.ubuntu.com/manpages/noble/man5/sources.list.5.html\n    components: list[str]\n    if len(suites) == 1 and suites[0].endswith(\"/\"):\n        if \"Components\" in options:\n            msg = (\n                \"Since '"'"'Suites'"'"' (line {suites_line}) specifies\"\n                \" a path relative to  '"'"'URIs'"'"' (line {uris_line}),\"\n                \" '"'"'Components'"'"' must be  omitted.\"\n            ).format(\n                suites_line=line_numbers.get(\"Suites\"),\n                uris_line=line_numbers.get(\"URIs\"),\n            )\n            raise BadValueError(\n                msg,\n                file=filename,\n                line=line_numbers.get(\"Components\"),\n                key=\"Components\",\n                value=options[\"Components\"],\n            )\n        components = []\n    else:\n        if \"Components\" not in options:\n            msg = (\n                \"Since '"'"'Suites'"'"' (line {suites_line}) does not specify\"\n                \" a path relative to  '"'"'URIs'"'"' (line {uris_line}),\"\n                \" '"'"'Components'"'"' must be  present in this stanza.\"\n            ).format(\n                suites_line=line_numbers.get(\"Suites\"),\n                uris_line=line_numbers.get(\"URIs\"),\n            )\n            raise MissingRequiredKeyError(\n                msg,\n                file=filename,\n                line=min(line_numbers.values()) if line_numbers else None,\n                key=\"Components\",\n            )\n        components = options.pop(\"Components\").split()\n    repos = tuple(\n        DebianRepository(\n            enabled=enabled,\n            repotype=repotype,\n            uri=uri,\n            release=suite,\n            groups=components,\n            filename=filename,\n            gpg_key_filename=gpg_key_file,\n            options=options,\n        )\n        for repotype in repotypes\n        for uri in uris\n        for suite in suites\n    )\n    return repos, (gpg_key_file, gpg_key_from_stanza)\n"}' --config 'python-packages=pydantic<2.0' --num-units 2
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
+ .model.name = 'jubilant-7976c98f'
+ .model.type = 'iaas'
+ .model.controller = 'github-pr-77b50-lxd'
+ .model.cloud = 'localhost'
+ .model.version = '3.6.11'
+ .model.region = 'localhost'
+ .model.model_status.current = 'available'
+ .machines['0'].juju_status.current = 'started'
+ .machines['0'].juju_status.version = '3.6.11'
+ .machines['0'].hostname = 'juju-9a7f5d-0'
+ .machines['0'].dns_name = '10.127.120.177'
+ .machines['0'].ip_addresses[0] = '10.127.120.177'
+ .machines['0'].instance_id = 'juju-9a7f5d-0'
+ .machines['0'].machine_status.current = 'running'
+ .machines['0'].machine_status.message = 'Running'
+ .machines['0'].modification_status.current = 'applied'
+ .machines['0'].base.name = 'ubuntu'
+ .machines['0'].base.channel = '24.04'
+ .machines['0'].network_interfaces['eth0'].ip_addresses[0] = '10.127.120.177'
+ .machines['0'].network_interfaces['eth0'].mac_address = '00:16:3e:d1:17:90'
+ .machines['0'].network_interfaces['eth0'].is_up = True
+ .machines['0'].network_interfaces['eth0'].gateway = '10.127.120.1'
+ .machines['0'].network_interfaces['eth0'].space = 'alpha'
+ .machines['0'].constraints = 'arch=amd64'
+ .machines['0'].hardware = 'arch=amd64 cores=0 mem=0M availability-zone=github-runner virt-type=container'
+ .machines['1'].juju_status.current = 'started'
+ .machines['1'].juju_status.version = '3.6.11'
+ .machines['1'].hostname = 'juju-9a7f5d-1'
+ .machines['1'].dns_name = '10.127.120.236'
+ .machines['1'].ip_addresses[0] = '10.127.120.236'
+ .machines['1'].instance_id = 'juju-9a7f5d-1'
+ .machines['1'].machine_status.current = 'running'
+ .machines['1'].machine_status.message = 'Running'
+ .machines['1'].modification_status.current = 'applied'
+ .machines['1'].base.name = 'ubuntu'
+ .machines['1'].base.channel = '24.04'
+ .machines['1'].network_interfaces['eth0'].ip_addresses[0] = '10.127.120.236'
+ .machines['1'].network_interfaces['eth0'].mac_address = '00:16:3e:0e:c9:49'
+ .machines['1'].network_interfaces['eth0'].is_up = True
+ .machines['1'].network_interfaces['eth0'].gateway = '10.127.120.1'
+ .machines['1'].network_interfaces['eth0'].space = 'alpha'
+ .machines['1'].constraints = 'arch=amd64'
+ .machines['1'].hardware = 'arch=amd64 cores=0 mem=0M availability-zone=github-runner virt-type=container'
+ .machines['2'].juju_status.current = 'pending'
+ .machines['2'].instance_id = 'pending'
+ .machines['2'].machine_status.current = 'pending'
+ .machines['2'].modification_status.current = 'idle'
+ .machines['2'].base.name = 'ubuntu'
+ .machines['2'].base.channel = '22.04'
+ .machines['2'].constraints = 'arch=amd64'
+ .machines['3'].juju_status.current = 'pending'
+ .machines['3'].instance_id = 'pending'
+ .machines['3'].machine_status.current = 'pending'
+ .machines['3'].modification_status.current = 'idle'
+ .machines['3'].base.name = 'ubuntu'
+ .machines['3'].base.channel = '22.04'
+ .machines['3'].constraints = 'arch=amd64'
+ .apps['haproxy'].charm = 'local:haproxy-0'
+ .apps['haproxy'].charm_origin = 'local'
+ .apps['haproxy'].charm_name = 'haproxy'
+ .apps['haproxy'].charm_rev = 0
+ .apps['haproxy'].exposed = False
+ .apps['haproxy'].base.name = 'ubuntu'
+ .apps['haproxy'].base.channel = '24.04'
+ .apps['haproxy'].app_status.current = 'active'
+ .apps['haproxy'].relations['certificates'][0].related_app = 'self-signed-certificates'
+ .apps['haproxy'].relations['certificates'][0].interface = 'tls-certificates'
+ .apps['haproxy'].relations['certificates'][0].scope = 'global'
+ .apps['haproxy'].relations['haproxy-peers'][0].related_app = 'haproxy'
+ .apps['haproxy'].relations['haproxy-peers'][0].interface = 'haproxy-peers'
+ .apps['haproxy'].relations['haproxy-peers'][0].scope = 'global'
+ .apps['haproxy'].units['haproxy/0'].workload_status.current = 'active'
+ .apps['haproxy'].units['haproxy/0'].juju_status.current = 'executing'
+ .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running start hook'
+ .apps['haproxy'].units['haproxy/0'].juju_status.version = '3.6.11'
+ .apps['haproxy'].units['haproxy/0'].leader = True
+ .apps['haproxy'].units['haproxy/0'].machine = '0'
+ .apps['haproxy'].units['haproxy/0'].open_ports[0] = '80/tcp'
+ .apps['haproxy'].units['haproxy/0'].public_address = '10.127.120.177'
+ .apps['haproxy'].endpoint_bindings[''] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['certificates'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['cos-agent'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['ha'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['haproxy-peers'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['haproxy-route'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['haproxy-route-tcp'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['ingress'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['ingress-per-unit'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['receive-ca-certs'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['reverseproxy'] = 'alpha'
+ .apps['haproxy'].endpoint_bindings['website'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].charm = 'any-charm'
+ .apps['ingress-per-unit-requirer-any'].charm_origin = 'charmhub'
+ .apps['ingress-per-unit-requirer-any'].charm_name = 'any-charm'
+ .apps['ingress-per-unit-requirer-any'].charm_rev = 92
+ .apps['ingress-per-unit-requirer-any'].exposed = False
+ .apps['ingress-per-unit-requirer-any'].base.name = 'ubuntu'
+ .apps['ingress-per-unit-requirer-any'].base.channel = '22.04'
+ .apps['ingress-per-unit-requirer-any'].charm_channel = 'latest/beta'
+ .apps['ingress-per-unit-requirer-any'].app_status.current = 'waiting'
+ .apps['ingress-per-unit-requirer-any'].app_status.message = 'waiting for machine'
+ .apps['ingress-per-unit-requirer-any'].relations['peer-any'][0].related_app = 'ingress-per-unit-requirer-any'
+ .apps['ingress-per-unit-requirer-any'].relations['peer-any'][0].interface = 'peer-any'
+ .apps['ingress-per-unit-requirer-any'].relations['peer-any'][0].scope = 'global'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/0'].workload_status.current = 'waiting'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/0'].workload_status.message = 'waiting for machine'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/0'].juju_status.current = 'allocating'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/0'].machine = '2'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/1'].workload_status.current = 'waiting'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/1'].workload_status.message = 'waiting for machine'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/1'].juju_status.current = 'allocating'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/1'].machine = '3'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings[''] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['dns-record'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['ingress'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['ldap'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['nginx-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['peer-any'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-aar'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-agent-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-airbyte-server'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-alertmanager-dispatch'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-alertmanager-remote-configuration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-anbox-stream-gateway'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-any'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-apache-vhost-config'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-apache-website'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-arangodb'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-auth-proxy'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-autoscaling'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-aws-iam'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-aws-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-azure-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-barbican-hsm'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-barbican-secrets'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-baremetal'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-bgp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-bind-rndc'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-block-storage'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cassandra'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-catalogue'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceilometer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-admin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-bootstrap'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-iscsi-admin-access'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-mds'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-osd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-radosgw'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ceph-rbd-mirror'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-certificate-transfer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cinder'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cinder-backend'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cinder-backup'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cinder-ceph-key'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cinder-gw'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cinder-nedge'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cloudflared-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-config-server'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-container-runtime'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-containerd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-containers'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cos-agent'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-cos-k8s-tokens'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-dashboard-plugin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-db'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-db2'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-designate'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-dex-oidc-config'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-dns-record'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-dns-transfer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-docker-registry'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-dockerhost'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-elastic-beats'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-elasticsearch'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-elasticsearch-datastore'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ephemeral-backend'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-etcd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-etcd-proxy'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-event-service'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-external-cloud-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-external-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-fiveg-core-gnb'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-fiveg-n2'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-fluentbit'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-forward-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ftn-compiler'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-gcp-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-generic-ip-port-user-pass'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-giraph'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-glance'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-glance-backend'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-glance-simplestreams-sync'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-glauth-auxiliary'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-gnocchi'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-grafana-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-grafana-cloud-config'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-grafana-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-grafana-datasource'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-grafana-datasource-exchange'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-grafana-metadata'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-grafana-source'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-guacd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-hacluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-haproxy-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-heat-plugin-subordinate'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-http'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-http-proxy'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-httpd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-hydra-endpoints'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-influxdb-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-infoblox'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ingress'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ingress-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ingress-per-unit'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-irc-bridge'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-istio-gateway-info'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-java'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-jenkins-agent-v0'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-jenkins-extension'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-jenkins-slave'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-k8s-cluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-k8s-service'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kafka'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kafka-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kapacitor'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-karma-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-keystone'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-keystone-admin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-keystone-credentials'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-keystone-domain-backend'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-keystone-fid-service-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-keystone-middleware'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-keystone-notifications'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kratos-info'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kube-control'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kube-dns'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kubeflow-dashboard-links'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kubernetes-cni'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-kubernetes-info'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-landscape-hosted'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ldap'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-livepatch-pro-airgapped-server'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-lldp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-loadbalancer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-local-monitors'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-login-ui-endpoints'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-logs'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-logstash-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-loki-push-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-lte-core'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-lxd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-lxd-bgp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-lxd-dns'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-lxd-https'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-lxd-metrics'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-magma-orchestrator'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-manila-plugin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-matrix-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-memcache'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-midonet'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mongodb'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mongodb-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-monitor'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-monitors'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mount'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-munin-node'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mysql'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mysql-async'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mysql-async-replication'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mysql-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mysql-monitor'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mysql-root'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mysql-router'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-mysql-shared'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nats'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nedge'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-neutron-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-neutron-load-balancer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-neutron-plugin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-neutron-plugin-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-neutron-plugin-api-subordinate'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nginx-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nova'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nova-ceilometer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nova-cell'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nova-compute'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nova-vgpu'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nova-vmware'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nrpe'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-nrpe-external-master'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ntp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-oathkeeper-info'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-oauth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-object-storage'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-odl-controller-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-oidc-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-openfga'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-opensearch-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-openstack-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-openstack-loadbalancer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ovsdb'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ovsdb-cluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ovsdb-cms'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ovsdb-manager'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ovsdb-subordinate'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-pacemaker-remote'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-parca-scrape'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-parca-store'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-peer-cluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-pgsql'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-placement'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-pod-defaults'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-postfix-metrics'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-postgresql-async'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-postgresql-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-prolog-epilog'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-prometheus'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-prometheus-manual'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-prometheus-remote-write'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-prometheus-rules'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-prometheus-scrape'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-public-address'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-quantum'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-rabbitmq'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-radosgw-multisite'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-radosgw-user'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ranger-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-redis'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-register-application'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-rest'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-s3'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-saml'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-script-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-sdcore-config'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-sdn-plugin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-secrets'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-sentry-metrics'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-service-control'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-service-mesh'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-shards'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-slurmd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-slurmdbd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-slurmrestd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-smtp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-squid-auth-helper'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ssl-termination'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-statistics'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-stun-server'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-swift'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-swift-global-cluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-swift-gw'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-swift-proxy'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-syslog'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-telegraf-exec'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-temporal'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-thruk-agent'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-tls-certificates'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-tokens'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-tracing'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-traefik-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-trino-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-ubuntu'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-udldap-userdata'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-untrusted-container-runtime'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-user-group'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-vault-autounseal'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-vault-kv'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-vsd-rest-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-vsphere-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-web-publish'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-websso-fid-service-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-websso-trusted-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-xlc-compiler'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-zookeeper'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['provide-zuul'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['redis'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-aar'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-agent-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-airbyte-server'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-alertmanager-dispatch'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-alertmanager-remote-configuration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-anbox-stream-gateway'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-any'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-apache-vhost-config'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-apache-website'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-arangodb'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-auth-proxy'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-autoscaling'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-aws-iam'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-aws-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-azure-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-barbican-hsm'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-barbican-secrets'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-baremetal'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-bgp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-bind-rndc'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-block-storage'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cassandra'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-catalogue'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceilometer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-admin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-bootstrap'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-iscsi-admin-access'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-mds'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-osd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-radosgw'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ceph-rbd-mirror'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-certificate-transfer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cinder'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cinder-backend'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cinder-backup'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cinder-ceph-key'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cinder-gw'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cinder-nedge'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cloudflared-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-config-server'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-container-runtime'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-containerd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-containers'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cos-agent'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-cos-k8s-tokens'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-dashboard-plugin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-db'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-db2'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-designate'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-dex-oidc-config'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-dns-record'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-dns-transfer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-docker-registry'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-dockerhost'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-elastic-beats'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-elasticsearch'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-elasticsearch-datastore'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ephemeral-backend'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-etcd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-etcd-proxy'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-event-service'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-external-cloud-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-external-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-fiveg-core-gnb'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-fiveg-n2'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-fluentbit'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-forward-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ftn-compiler'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-gcp-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-generic-ip-port-user-pass'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-giraph'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-glance'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-glance-backend'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-glance-simplestreams-sync'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-glauth-auxiliary'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-gnocchi'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-grafana-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-grafana-cloud-config'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-grafana-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-grafana-datasource'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-grafana-datasource-exchange'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-grafana-metadata'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-grafana-source'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-guacd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-hacluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-haproxy-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-haproxy-route-tcp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-heat-plugin-subordinate'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-http'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-http-proxy'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-httpd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-hydra-endpoints'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-influxdb-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-infoblox'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ingress'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ingress-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ingress-per-unit'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-irc-bridge'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-istio-gateway-info'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-java'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-jenkins-agent-v0'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-jenkins-extension'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-jenkins-slave'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-k8s-cluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-k8s-service'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kafka'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kafka-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kapacitor'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-karma-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-keystone'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-keystone-admin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-keystone-credentials'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-keystone-domain-backend'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-keystone-fid-service-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-keystone-middleware'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-keystone-notifications'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kratos-info'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kube-control'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kube-dns'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kubeflow-dashboard-links'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kubernetes-cni'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-kubernetes-info'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-landscape-hosted'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ldap'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-livepatch-pro-airgapped-server'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-lldp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-loadbalancer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-local-monitors'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-login-ui-endpoints'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-logs'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-logstash-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-loki-push-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-lte-core'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-lxd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-lxd-bgp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-lxd-dns'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-lxd-https'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-lxd-metrics'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-magma-orchestrator'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-manila-plugin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-matrix-auth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-memcache'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-midonet'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mongodb'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mongodb-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-monitor'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-monitors'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mount'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-munin-node'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mysql'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mysql-async'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mysql-async-replication'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mysql-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mysql-monitor'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mysql-root'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mysql-router'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-mysql-shared'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nats'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nedge'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-neutron-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-neutron-load-balancer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-neutron-plugin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-neutron-plugin-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-neutron-plugin-api-subordinate'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nginx-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nova'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nova-ceilometer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nova-cell'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nova-compute'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nova-vgpu'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nova-vmware'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nrpe'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-nrpe-external-master'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ntp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-oathkeeper-info'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-oauth'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-object-storage'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-odl-controller-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-oidc-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-opencti-connector'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-openfga'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-opensearch-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-openstack-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-openstack-loadbalancer'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ovsdb'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ovsdb-cluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ovsdb-cms'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ovsdb-manager'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ovsdb-subordinate'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-pacemaker-remote'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-parca-scrape'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-parca-store'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-peer-cluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-pgsql'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-placement'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-pod-defaults'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-postfix-metrics'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-postgresql-async'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-postgresql-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-prolog-epilog'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-prometheus'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-prometheus-manual'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-prometheus-remote-write'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-prometheus-rules'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-prometheus-scrape'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-public-address'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-quantum'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-rabbitmq'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-radosgw-multisite'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-radosgw-user'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ranger-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-redis'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-register-application'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-rest'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-s3'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-saml'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-script-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-sdcore-config'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-sdn-plugin'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-secrets'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-sentry-metrics'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-service-control'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-service-mesh'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-shards'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-slurmd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-slurmdbd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-slurmrestd'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-smtp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-squid-auth-helper'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ssl-termination'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-statistics'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-stun-server'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-swift'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-swift-global-cluster'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-swift-gw'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-swift-proxy'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-syslog'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-telegraf-exec'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-temporal'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-thruk-agent'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-tls-certificates'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-tokens'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-tracing'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-traefik-route'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-trino-client'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-ubuntu'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-udldap-userdata'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-untrusted-container-runtime'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-user-group'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-vault-autounseal'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-vault-kv'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-vsd-rest-api'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-vsphere-integration'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-web-publish'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-websso-fid-service-provider'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-websso-trusted-dashboard'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-xlc-compiler'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-zookeeper'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['require-zuul'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['saml'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['send-ca-cert'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['smtp'] = 'alpha'
+ .apps['ingress-per-unit-requirer-any'].endpoint_bindings['smtp-legacy'] = 'alpha'
+ .apps['self-signed-certificates'].charm = 'self-signed-certificates'
+ .apps['self-signed-certificates'].charm_origin = 'charmhub'
+ .apps['self-signed-certificates'].charm_name = 'self-signed-certificates'
+ .apps['self-signed-certificates'].charm_rev = 416
+ .apps['self-signed-certificates'].exposed = False
+ .apps['self-signed-certificates'].base.name = 'ubuntu'
+ .apps['self-signed-certificates'].base.channel = '24.04'
+ .apps['self-signed-certificates'].charm_channel = '1/edge'
+ .apps['self-signed-certificates'].app_status.current = 'active'
+ .apps['self-signed-certificates'].relations['certificates'][0].related_app = 'haproxy'
+ .apps['self-signed-certificates'].relations['certificates'][0].interface = 'tls-certificates'
+ .apps['self-signed-certificates'].relations['certificates'][0].scope = 'global'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].workload_status.current = 'active'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.current = 'idle'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].juju_status.version = '3.6.11'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].leader = True
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].machine = '1'
+ .apps['self-signed-certificates'].units['self-signed-certificates/0'].public_address = '10.127.120.236'
+ .apps['self-signed-certificates'].endpoint_bindings[''] = 'alpha'
+ .apps['self-signed-certificates'].endpoint_bindings['certificates'] = 'alpha'
+ .apps['self-signed-certificates'].endpoint_bindings['send-ca-cert'] = 'alpha'
+ .apps['self-signed-certificates'].endpoint_bindings['tracing'] = 'alpha'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['2'].machine_status.current = 'pending'
+ .machines['2'].machine_status.current = 'allocating'
+ .machines['2'].machine_status.message = 'acquiring LXD image'
- .machines['3'].machine_status.current = 'pending'
+ .machines['3'].machine_status.current = 'allocating'
+ .machines['3'].machine_status.message = 'acquiring LXD image'
- .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running start hook'
+ .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running certificates-relation-changed hook'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running certificates-relation-changed hook'
+ .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running certificates-relation-changed hook for self-signed-certificates/0'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['2'].machine_status.message = 'acquiring LXD image'
+ .machines['2'].machine_status.message = 'Retrieving image: metadata: 100% (1.56GB/s)'
- .apps['haproxy'].units['haproxy/0'].juju_status.current = 'executing'
- .apps['haproxy'].units['haproxy/0'].juju_status.message = 'running certificates-relation-changed hook for self-signed-certificates/0'
+ .apps['haproxy'].units['haproxy/0'].juju_status.current = 'idle'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['2'].machine_status.message = 'Retrieving image: metadata: 100% (1.56GB/s)'
+ .machines['2'].machine_status.message = 'Retrieving image: rootfs: 24% (75.95MB/s)'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['2'].machine_status.message = 'Retrieving image: rootfs: 24% (75.95MB/s)'
+ .machines['2'].machine_status.message = 'Retrieving image: rootfs: 47% (84.55MB/s)'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['2'].machine_status.message = 'Retrieving image: rootfs: 47% (84.55MB/s)'
+ .machines['2'].machine_status.message = 'Retrieving image: rootfs: 72% (87.71MB/s)'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['2'].machine_status.message = 'Retrieving image: rootfs: 72% (87.71MB/s)'
+ .machines['2'].machine_status.message = 'Retrieving image: rootfs: 95% (89.39MB/s)'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['2'].machine_status.message = 'Retrieving image: rootfs: 95% (89.39MB/s)'
+ .machines['2'].machine_status.message = 'Creating container'
- .machines['3'].machine_status.message = 'acquiring LXD image'
+ .machines['3'].machine_status.message = 'Failed remote LXD image download. Retrying. Attempt number 2'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['2'].instance_id = 'pending'
- .machines['2'].machine_status.current = 'allocating'
- .machines['2'].machine_status.message = 'Creating container'
- .machines['2'].modification_status.current = 'idle'
+ .machines['2'].instance_id = 'juju-9a7f5d-2'
+ .machines['2'].machine_status.current = 'running'
+ .machines['2'].machine_status.message = 'Running'
+ .machines['2'].modification_status.current = 'applied'
+ .machines['2'].hardware = 'arch=amd64 cores=0 mem=0M availability-zone=github-runner virt-type=container'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
+ .machines['2'].dns_name = '10.127.120.226'
+ .machines['2'].ip_addresses[0] = '10.127.120.226'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/0'].public_address = '10.127.120.226'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['3'].machine_status.message = 'Failed remote LXD image download. Retrying. Attempt number 2'
+ .machines['3'].machine_status.message = 'Failed remote LXD image download. Retrying. Attempt number 3'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['3'].juju_status.current = 'pending'
+ .machines['3'].juju_status.current = 'down'
+ .machines['3'].juju_status.message = 'agent is not communicating with the server'
- .machines['3'].machine_status.message = 'Failed remote LXD image download. Retrying. Attempt number 3'
+ .machines['3'].machine_status.message = 'failed to start machine 3 (attempt count exceeded: Failed remote image download: Alias already exists: juju/ubuntu@22.04/amd64), retrying in 10s (10 more attempts)'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['3'].machine_status.message = 'failed to start machine 3 (attempt count exceeded: Failed remote image download: Alias already exists: juju/ubuntu@22.04/amd64), retrying in 10s (10 more attempts)'
+ .machines['3'].machine_status.message = 'Creating container'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['3'].instance_id = 'pending'
- .machines['3'].machine_status.current = 'allocating'
- .machines['3'].machine_status.message = 'Creating container'
+ .machines['3'].instance_id = 'juju-9a7f5d-3'
+ .machines['3'].machine_status.current = 'running'
+ .machines['3'].machine_status.message = 'Container started'
+ .machines['3'].hardware = 'arch=amd64 cores=0 mem=0M availability-zone=github-runner virt-type=container'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
- .machines['3'].modification_status.current = 'idle'
+ .machines['3'].modification_status.current = 'applied'
[32mINFO    [0m jubilant.wait:_juju.py:982 wait: status changed:
+ .machines['3'].dns_name = '10.127.120.52'
+ .machines['3'].ip_addresses[0] = '10.127.120.52'
- .machines['3'].machine_status.message = 'Container started'
+ .machines['3'].machine_status.message = 'Running'
+ .apps['ingress-per-unit-requirer-any'].units['ingress-per-unit-requirer-any/1'].public_address = '10.127.120.52'