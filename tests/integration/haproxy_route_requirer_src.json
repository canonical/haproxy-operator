{"any_charm.py": "import pathlib\nimport subprocess\nimport ops\nfrom any_charm_base import AnyCharmBase\nimport apt\nimport json\nfrom haproxy_route import HaproxyRouteRequirer, RateLimitPolicy\n\nHAPROXY_ROUTE_RELATION = \"require-haproxy-route\"\n\nclass AnyCharm(AnyCharmBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._haproxy_route = HaproxyRouteRequirer(\n            self, HAPROXY_ROUTE_RELATION\n        )\n\n    def start_server(self):\n        apt.update()\n        apt.add_package(package_names=\"apache2\")\n        www_dir = pathlib.Path(\"/var/www/html\")\n        file_path = www_dir / \"ok\"\n        file_path.parent.mkdir(exist_ok=True)\n        file_path.write_text(\"ok!<br/>\")\n        self.unit.status = ops.ActiveStatus(\"Server ready\")\n\n    def update_relation(self):\n        self._haproxy_route.provide_haproxy_route_requirements(\n            service=\"any\", ports=[80],\n            subdomains=[\"ok\", \"ok2\"],\n            rate_limit_connections_per_minute=1,\n            rate_limit_policy=RateLimitPolicy.DENY,\n            check_interval=600,\n            check_rise=3,\n            check_fall=3,\n            check_path=\"/ok\",\n            path_rewrite_expressions=[\"/ok\"],\n            deny_paths=[\"/private\"]\n        )\n", "apt.py": "# Copyright 2021 Canonical Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Abstractions for the system's Debian/Ubuntu package information and repositories.\n\nThis module contains abstractions and wrappers around Debian/Ubuntu-style repositories and\npackages, in order to easily provide an idiomatic and Pythonic mechanism for adding packages and/or\nrepositories to systems for use in machine charms.\n\nA sane default configuration is attainable through nothing more than instantiation of the\nappropriate classes. `DebianPackage` objects provide information about the architecture, version,\nname, and status of a package.\n\n`DebianPackage` will try to look up a package either from `dpkg -L` or from `apt-cache` when\nprovided with a string indicating the package name. If it cannot be located, `PackageNotFoundError`\nwill be returned, as `apt` and `dpkg` otherwise return `100` for all errors, and a meaningful error\nmessage if the package is not known is desirable.\n\nTo install packages with convenience methods:\n\n```python\ntry:\n    # Run `apt-get update`\n    apt.update()\n    apt.add_package(\"zsh\")\n    apt.add_package([\"vim\", \"htop\", \"wget\"])\nexcept PackageNotFoundError:\n    logger.error(\"a specified package not found in package cache or on system\")\nexcept PackageError as e:\n    logger.error(\"could not install package. Reason: %s\", e.message)\n````\n\nTo find details of a specific package:\n\n```python\ntry:\n    vim = apt.DebianPackage.from_system(\"vim\")\n\n    # To find from the apt cache only\n    # apt.DebianPackage.from_apt_cache(\"vim\")\n\n    # To find from installed packages only\n    # apt.DebianPackage.from_installed_package(\"vim\")\n\n    vim.ensure(PackageState.Latest)\n    logger.info(\"updated vim to version: %s\", vim.fullversion)\nexcept PackageNotFoundError:\n    logger.error(\"a specified package not found in package cache or on system\")\nexcept PackageError as e:\n    logger.error(\"could not install package. Reason: %s\", e.message)\n```\n\n\n`RepositoryMapping` will return a dict-like object containing enabled system repositories\nand their properties (available groups, baseuri. gpg key). This class can add, disable, or\nmanipulate repositories. Items can be retrieved as `DebianRepository` objects.\n\nIn order add a new repository with explicit details for fields, a new `DebianRepository` can\nbe added to `RepositoryMapping`\n\n`RepositoryMapping` provides an abstraction around the existing repositories on the system,\nand can be accessed and iterated over like any `Mapping` object, to retrieve values by key,\niterate, or perform other operations.\n\nKeys are constructed as `{repo_type}-{}-{release}` in order to uniquely identify a repository.\n\nRepositories can be added with explicit values through a Python constructor.\n\nExample:\n```python\nrepositories = apt.RepositoryMapping()\n\nif \"deb-example.com-focal\" not in repositories:\n    repositories.add(DebianRepository(enabled=True, repotype=\"deb\",\n                     uri=\"https://example.com\", release=\"focal\", groups=[\"universe\"]))\n```\n\nAlternatively, any valid `sources.list` line may be used to construct a new\n`DebianRepository`.\n\nExample:\n```python\nrepositories = apt.RepositoryMapping()\n\nif \"deb-us.archive.ubuntu.com-xenial\" not in repositories:\n    line = \"deb http://us.archive.ubuntu.com/ubuntu xenial main restricted\"\n    repo = DebianRepository.from_repo_line(line)\n    repositories.add(repo)\n```\n\"\"\"\n\nimport fileinput\nimport glob\nimport logging\nimport os\nimport re\nimport subprocess\nfrom collections.abc import Mapping\nfrom enum import Enum\nfrom subprocess import PIPE, CalledProcessError, check_output\nfrom typing import Iterable, List, Optional, Tuple, Union\nfrom urllib.parse import urlparse\n\nlogger = logging.getLogger(__name__)\n\n# The unique Charmhub library identifier, never change it\nLIBID = \"7c3dbc9c2ad44a47bd6fcb25caa270e5\"\n\n# Increment this major API version when introducing breaking changes\nLIBAPI = 0\n\n# Increment this PATCH version before using `charmcraft publish-lib` or reset\n# to 0 if you are raising the major API version\nLIBPATCH = 14\n\n\nVALID_SOURCE_TYPES = (\"deb\", \"deb-src\")\nOPTIONS_MATCHER = re.compile(r\"\\[.*?\\]\")\n\n\nclass Error(Exception):\n    \"\"\"Base class of most errors raised by this library.\"\"\"\n\n    def __repr__(self):\n        \"\"\"Represent the Error.\"\"\"\n        return \"<{}.{} {}>\".format(type(self).__module__, type(self).__name__, self.args)\n\n    @property\n    def name(self):\n        \"\"\"Return a string representation of the model plus class.\"\"\"\n        return \"<{}.{}>\".format(type(self).__module__, type(self).__name__)\n\n    @property\n    def message(self):\n        \"\"\"Return the message passed as an argument.\"\"\"\n        return self.args[0]\n\n\nclass PackageError(Error):\n    \"\"\"Raised when there's an error installing or removing a package.\"\"\"\n\n\nclass PackageNotFoundError(Error):\n    \"\"\"Raised when a requested package is not known to the system.\"\"\"\n\n\nclass PackageState(Enum):\n    \"\"\"A class to represent possible package states.\"\"\"\n\n    Present = \"present\"\n    Absent = \"absent\"\n    Latest = \"latest\"\n    Available = \"available\"\n\n\nclass DebianPackage:\n    \"\"\"Represents a traditional Debian package and its utility functions.\n\n    `DebianPackage` wraps information and functionality around a known package, whether installed\n    or available. The version, epoch, name, and architecture can be easily queried and compared\n    against other `DebianPackage` objects to determine the latest version or to install a specific\n    version.\n\n    The representation of this object as a string mimics the output from `dpkg` for familiarity.\n\n    Installation and removal of packages is handled through the `state` property or `ensure`\n    method, with the following options:\n\n        apt.PackageState.Absent\n        apt.PackageState.Available\n        apt.PackageState.Present\n        apt.PackageState.Latest\n\n    When `DebianPackage` is initialized, the state of a given `DebianPackage` object will be set to\n    `Available`, `Present`, or `Latest`, with `Absent` implemented as a convenience for removal\n    (though it operates essentially the same as `Available`).\n    \"\"\"\n\n    def __init__(\n        self, name: str, version: str, epoch: str, arch: str, state: PackageState\n    ) -> None:\n        self._name = name\n        self._arch = arch\n        self._state = state\n        self._version = Version(version, epoch)\n\n    def __eq__(self, other) -> bool:\n        \"\"\"Equality for comparison.\n\n        Args:\n          other: a `DebianPackage` object for comparison\n\n        Returns:\n          A boolean reflecting equality\n        \"\"\"\n        return isinstance(other, self.__class__) and (\n            self._name,\n            self._version.number,\n        ) == (other._name, other._version.number)\n\n    def __hash__(self):\n        \"\"\"Return a hash of this package.\"\"\"\n        return hash((self._name, self._version.number))\n\n    def __repr__(self):\n        \"\"\"Represent the package.\"\"\"\n        return \"<{}.{}: {}>\".format(self.__module__, self.__class__.__name__, self.__dict__)\n\n    def __str__(self):\n        \"\"\"Return a human-readable representation of the package.\"\"\"\n        return \"<{}: {}-{}.{} -- {}>\".format(\n            self.__class__.__name__,\n            self._name,\n            self._version,\n            self._arch,\n            str(self._state),\n        )\n\n    @staticmethod\n    def _apt(\n        command: str,\n        package_names: Union[str, List],\n        optargs: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Wrap package management commands for Debian/Ubuntu systems.\n\n        Args:\n          command: the command given to `apt-get`\n          package_names: a package name or list of package names to operate on\n          optargs: an (Optional) list of additioanl arguments\n\n        Raises:\n          PackageError if an error is encountered\n        \"\"\"\n        optargs = optargs if optargs is not None else []\n        if isinstance(package_names, str):\n            package_names = [package_names]\n        _cmd = [\"apt-get\", \"-y\", *optargs, command, *package_names]\n        try:\n            env = os.environ.copy()\n            env[\"DEBIAN_FRONTEND\"] = \"noninteractive\"\n            subprocess.run(_cmd, capture_output=True, check=True, text=True, env=env)\n        except CalledProcessError as e:\n            raise PackageError(\n                \"Could not {} package(s) [{}]: {}\".format(command, [*package_names], e.stderr)\n            ) from None\n\n    def _add(self) -> None:\n        \"\"\"Add a package to the system.\"\"\"\n        self._apt(\n            \"install\",\n            \"{}={}\".format(self.name, self.version),\n            optargs=[\"--option=Dpkg::Options::=--force-confold\"],\n        )\n\n    def _remove(self) -> None:\n        \"\"\"Remove a package from the system. Implementation-specific.\"\"\"\n        return self._apt(\"remove\", \"{}={}\".format(self.name, self.version))\n\n    @property\n    def name(self) -> str:\n        \"\"\"Returns the name of the package.\"\"\"\n        return self._name\n\n    def ensure(self, state: PackageState):\n        \"\"\"Ensure that a package is in a given state.\n\n        Args:\n          state: a `PackageState` to reconcile the package to\n\n        Raises:\n          PackageError from the underlying call to apt\n        \"\"\"\n        if self._state is not state:\n            if state not in (PackageState.Present, PackageState.Latest):\n                self._remove()\n            else:\n                self._add()\n        self._state = state\n\n    @property\n    def present(self) -> bool:\n        \"\"\"Returns whether or not a package is present.\"\"\"\n        return self._state in (PackageState.Present, PackageState.Latest)\n\n    @property\n    def latest(self) -> bool:\n        \"\"\"Returns whether the package is the most recent version.\"\"\"\n        return self._state is PackageState.Latest\n\n    @property\n    def state(self) -> PackageState:\n        \"\"\"Returns the current package state.\"\"\"\n        return self._state\n\n    @state.setter\n    def state(self, state: PackageState) -> None:\n        \"\"\"Set the package state to a given value.\n\n        Args:\n          state: a `PackageState` to reconcile the package to\n\n        Raises:\n          PackageError from the underlying call to apt\n        \"\"\"\n        if state in (PackageState.Latest, PackageState.Present):\n            self._add()\n        else:\n            self._remove()\n        self._state = state\n\n    @property\n    def version(self) -> \"Version\":\n        \"\"\"Returns the version for a package.\"\"\"\n        return self._version\n\n    @property\n    def epoch(self) -> str:\n        \"\"\"Returns the epoch for a package. May be unset.\"\"\"\n        return self._version.epoch\n\n    @property\n    def arch(self) -> str:\n        \"\"\"Returns the architecture for a package.\"\"\"\n        return self._arch\n\n    @property\n    def fullversion(self) -> str:\n        \"\"\"Returns the name+epoch for a package.\"\"\"\n        return \"{}.{}\".format(self._version, self._arch)\n\n    @staticmethod\n    def _get_epoch_from_version(version: str) -> Tuple[str, str]:\n        \"\"\"Pull the epoch, if any, out of a version string.\"\"\"\n        epoch_matcher = re.compile(r\"^((?P<epoch>\\d+):)?(?P<version>.*)\")\n        matches = epoch_matcher.search(version).groupdict()\n        return matches.get(\"epoch\", \"\"), matches.get(\"version\")\n\n    @classmethod\n    def from_system(\n        cls, package: str, version: Optional[str] = \"\", arch: Optional[str] = \"\"\n    ) -> \"DebianPackage\":\n        \"\"\"Locates a package, either on the system or known to apt, and serializes the information.\n\n        Args:\n            package: a string representing the package\n            version: an optional string if a specific version is requested\n            arch: an optional architecture, defaulting to `dpkg --print-architecture`. If an\n                architecture is not specified, this will be used for selection.\n\n        \"\"\"\n        try:\n            return DebianPackage.from_installed_package(package, version, arch)\n        except PackageNotFoundError:\n            logger.debug(\n                \"package '%s' is not currently installed or has the wrong architecture.\", package\n            )\n\n        # Ok, try `apt-cache ...`\n        try:\n            return DebianPackage.from_apt_cache(package, version, arch)\n        except (PackageNotFoundError, PackageError):\n            # If we get here, it's not known to the systems.\n            # This seems unnecessary, but virtually all `apt` commands have a return code of `100`,\n            # and providing meaningful error messages without this is ugly.\n            raise PackageNotFoundError(\n                \"Package '{}{}' could not be found on the system or in the apt cache!\".format(\n                    package, \".{}\".format(arch) if arch else \"\"\n                )\n            ) from None\n\n    @classmethod\n    def from_installed_package(\n        cls, package: str, version: Optional[str] = \"\", arch: Optional[str] = \"\"\n    ) -> \"DebianPackage\":\n        \"\"\"Check whether the package is already installed and return an instance.\n\n        Args:\n            package: a string representing the package\n            version: an optional string if a specific version is requested\n            arch: an optional architecture, defaulting to `dpkg --print-architecture`.\n                If an architecture is not specified, this will be used for selection.\n        \"\"\"\n        system_arch = check_output(\n            [\"dpkg\", \"--print-architecture\"], universal_newlines=True\n        ).strip()\n        arch = arch if arch else system_arch\n\n        # Regexps are a really terrible way to do this. Thanks dpkg\n        output = \"\"\n        try:\n            output = check_output([\"dpkg\", \"-l\", package], stderr=PIPE, universal_newlines=True)\n        except CalledProcessError:\n            raise PackageNotFoundError(\"Package is not installed: {}\".format(package)) from None\n\n        # Pop off the output from `dpkg -l' because there's no flag to\n        # omit it`\n        lines = str(output).splitlines()[5:]\n\n        dpkg_matcher = re.compile(\n            r\"\"\"\n        ^(?P<package_status>\\w+?)\\s+\n        (?P<package_name>.*?)(?P<throwaway_arch>:\\w+?)?\\s+\n        (?P<version>.*?)\\s+\n        (?P<arch>\\w+?)\\s+\n        (?P<description>.*)\n        \"\"\",\n            re.VERBOSE,\n        )\n\n        for line in lines:\n            try:\n                matches = dpkg_matcher.search(line).groupdict()\n                package_status = matches[\"package_status\"]\n\n                if not package_status.endswith(\"i\"):\n                    logger.debug(\n                        \"package '%s' in dpkg output but not installed, status: '%s'\",\n                        package,\n                        package_status,\n                    )\n                    break\n\n                epoch, split_version = DebianPackage._get_epoch_from_version(matches[\"version\"])\n                pkg = DebianPackage(\n                    matches[\"package_name\"],\n                    split_version,\n                    epoch,\n                    matches[\"arch\"],\n                    PackageState.Present,\n                )\n                if (pkg.arch == \"all\" or pkg.arch == arch) and (\n                    version == \"\" or str(pkg.version) == version\n                ):\n                    return pkg\n            except AttributeError:\n                logger.warning(\"dpkg matcher could not parse line: %s\", line)\n\n        # If we didn't find it, fail through\n        raise PackageNotFoundError(\"Package {}.{} is not installed!\".format(package, arch))\n\n    @classmethod\n    def from_apt_cache(\n        cls, package: str, version: Optional[str] = \"\", arch: Optional[str] = \"\"\n    ) -> \"DebianPackage\":\n        \"\"\"Check whether the package is already installed and return an instance.\n\n        Args:\n            package: a string representing the package\n            version: an optional string if a specific version is requested\n            arch: an optional architecture, defaulting to `dpkg --print-architecture`.\n                If an architecture is not specified, this will be used for selection.\n        \"\"\"\n        system_arch = check_output(\n            [\"dpkg\", \"--print-architecture\"], universal_newlines=True\n        ).strip()\n        arch = arch if arch else system_arch\n\n        # Regexps are a really terrible way to do this. Thanks dpkg\n        keys = (\"Package\", \"Architecture\", \"Version\")\n\n        try:\n            output = check_output(\n                [\"apt-cache\", \"show\", package], stderr=PIPE, universal_newlines=True\n            )\n        except CalledProcessError as e:\n            raise PackageError(\n                \"Could not list packages in apt-cache: {}\".format(e.stderr)\n            ) from None\n\n        pkg_groups = output.strip().split(\"\\n\\n\")\n        keys = (\"Package\", \"Architecture\", \"Version\")\n\n        for pkg_raw in pkg_groups:\n            lines = str(pkg_raw).splitlines()\n            vals = {}\n            for line in lines:\n                if line.startswith(keys):\n                    items = line.split(\":\", 1)\n                    vals[items[0]] = items[1].strip()\n                else:\n                    continue\n\n            epoch, split_version = DebianPackage._get_epoch_from_version(vals[\"Version\"])\n            pkg = DebianPackage(\n                vals[\"Package\"],\n                split_version,\n                epoch,\n                vals[\"Architecture\"],\n                PackageState.Available,\n            )\n\n            if (pkg.arch == \"all\" or pkg.arch == arch) and (\n                version == \"\" or str(pkg.version) == version\n            ):\n                return pkg\n\n        # If we didn't find it, fail through\n        raise PackageNotFoundError(\"Package {}.{} is not in the apt cache!\".format(package, arch))\n\n\nclass Version:\n    \"\"\"An abstraction around package versions.\n\n    This seems like it should be strictly unnecessary, except that `apt_pkg` is not usable inside a\n    venv, and wedging version comparisons into `DebianPackage` would overcomplicate it.\n\n    This class implements the algorithm found here:\n    https://www.debian.org/doc/debian-policy/ch-controlfields.html#version\n    \"\"\"\n\n    def __init__(self, version: str, epoch: str):\n        self._version = version\n        self._epoch = epoch or \"\"\n\n    def __repr__(self):\n        \"\"\"Represent the package.\"\"\"\n        return \"<{}.{}: {}>\".format(self.__module__, self.__class__.__name__, self.__dict__)\n\n    def __str__(self):\n        \"\"\"Return human-readable representation of the package.\"\"\"\n        return \"{}{}\".format(\"{}:\".format(self._epoch) if self._epoch else \"\", self._version)\n\n    @property\n    def epoch(self):\n        \"\"\"Returns the epoch for a package. May be empty.\"\"\"\n        return self._epoch\n\n    @property\n    def number(self) -> str:\n        \"\"\"Returns the version number for a package.\"\"\"\n        return self._version\n\n    def _get_parts(self, version: str) -> Tuple[str, str]:\n        \"\"\"Separate the version into component upstream and Debian pieces.\"\"\"\n        try:\n            version.rindex(\"-\")\n        except ValueError:\n            # No hyphens means no Debian version\n            return version, \"0\"\n\n        upstream, debian = version.rsplit(\"-\", 1)\n        return upstream, debian\n\n    def _listify(self, revision: str) -> List[str]:\n        \"\"\"Split a revision string into a listself.\n\n        This list is comprised of  alternating between strings and numbers,\n        padded on either end to always be \"str, int, str, int...\" and\n        always be of even length.  This allows us to trivially implement the\n        comparison algorithm described.\n        \"\"\"\n        result = []\n        while revision:\n            rev_1, remains = self._get_alphas(revision)\n            rev_2, remains = self._get_digits(remains)\n            result.extend([rev_1, rev_2])\n            revision = remains\n        return result\n\n    def _get_alphas(self, revision: str) -> Tuple[str, str]:\n        \"\"\"Return a tuple of the first non-digit characters of a revision.\"\"\"\n        # get the index of the first digit\n        for i, char in enumerate(revision):\n            if char.isdigit():\n                if i == 0:\n                    return \"\", revision\n                return revision[0:i], revision[i:]\n        # string is entirely alphas\n        return revision, \"\"\n\n    def _get_digits(self, revision: str) -> Tuple[int, str]:\n        \"\"\"Return a tuple of the first integer characters of a revision.\"\"\"\n        # If the string is empty, return (0,'')\n        if not revision:\n            return 0, \"\"\n        # get the index of the first non-digit\n        for i, char in enumerate(revision):\n            if not char.isdigit():\n                if i == 0:\n                    return 0, revision\n                return int(revision[0:i]), revision[i:]\n        # string is entirely digits\n        return int(revision), \"\"\n\n    def _dstringcmp(self, a, b):  # noqa: C901\n        \"\"\"Debian package version string section lexical sort algorithm.\n\n        The lexical comparison is a comparison of ASCII values modified so\n        that all the letters sort earlier than all the non-letters and so that\n        a tilde sorts before anything, even the end of a part.\n        \"\"\"\n        if a == b:\n            return 0\n        try:\n            for i, char in enumerate(a):\n                if char == b[i]:\n                    continue\n                # \"a tilde sorts before anything, even the end of a part\"\n                # (emptyness)\n                if char == \"~\":\n                    return -1\n                if b[i] == \"~\":\n                    return 1\n                # \"all the letters sort earlier than all the non-letters\"\n                if char.isalpha() and not b[i].isalpha():\n                    return -1\n                if not char.isalpha() and b[i].isalpha():\n                    return 1\n                # otherwise lexical sort\n                if ord(char) > ord(b[i]):\n                    return 1\n                if ord(char) < ord(b[i]):\n                    return -1\n        except IndexError:\n            # a is longer than b but otherwise equal, greater unless there are tildes\n            if char == \"~\":\n                return -1\n            return 1\n        # if we get here, a is shorter than b but otherwise equal, so check for tildes...\n        if b[len(a)] == \"~\":\n            return 1\n        return -1\n\n    def _compare_revision_strings(self, first: str, second: str):  # noqa: C901\n        \"\"\"Compare two debian revision strings.\"\"\"\n        if first == second:\n            return 0\n\n        # listify pads results so that we will always be comparing ints to ints\n        # and strings to strings (at least until we fall off the end of a list)\n        first_list = self._listify(first)\n        second_list = self._listify(second)\n        if first_list == second_list:\n            return 0\n        try:\n            for i, item in enumerate(first_list):\n                # explicitly raise IndexError if we've fallen off the edge of list2\n                if i >= len(second_list):\n                    raise IndexError\n                # if the items are equal, next\n                if item == second_list[i]:\n                    continue\n                # numeric comparison\n                if isinstance(item, int):\n                    if item > second_list[i]:\n                        return 1\n                    if item < second_list[i]:\n                        return -1\n                else:\n                    # string comparison\n                    return self._dstringcmp(item, second_list[i])\n        except IndexError:\n            # rev1 is longer than rev2 but otherwise equal, hence greater\n            # ...except for goddamn tildes\n            if first_list[len(second_list)][0][0] == \"~\":\n                return 1\n            return 1\n        # rev1 is shorter than rev2 but otherwise equal, hence lesser\n        # ...except for goddamn tildes\n        if second_list[len(first_list)][0][0] == \"~\":\n            return -1\n        return -1\n\n    def _compare_version(self, other) -> int:\n        if (self.number, self.epoch) == (other.number, other.epoch):\n            return 0\n\n        if self.epoch < other.epoch:\n            return -1\n        if self.epoch > other.epoch:\n            return 1\n\n        # If none of these are true, follow the algorithm\n        upstream_version, debian_version = self._get_parts(self.number)\n        other_upstream_version, other_debian_version = self._get_parts(other.number)\n\n        upstream_cmp = self._compare_revision_strings(upstream_version, other_upstream_version)\n        if upstream_cmp != 0:\n            return upstream_cmp\n\n        debian_cmp = self._compare_revision_strings(debian_version, other_debian_version)\n        if debian_cmp != 0:\n            return debian_cmp\n\n        return 0\n\n    def __lt__(self, other) -> bool:\n        \"\"\"Less than magic method impl.\"\"\"\n        return self._compare_version(other) < 0\n\n    def __eq__(self, other) -> bool:\n        \"\"\"Equality magic method impl.\"\"\"\n        return self._compare_version(other) == 0\n\n    def __gt__(self, other) -> bool:\n        \"\"\"Greater than magic method impl.\"\"\"\n        return self._compare_version(other) > 0\n\n    def __le__(self, other) -> bool:\n        \"\"\"Less than or equal to magic method impl.\"\"\"\n        return self.__eq__(other) or self.__lt__(other)\n\n    def __ge__(self, other) -> bool:\n        \"\"\"Greater than or equal to magic method impl.\"\"\"\n        return self.__gt__(other) or self.__eq__(other)\n\n    def __ne__(self, other) -> bool:\n        \"\"\"Not equal to magic method impl.\"\"\"\n        return not self.__eq__(other)\n\n\ndef add_package(\n    package_names: Union[str, List[str]],\n    version: Optional[str] = \"\",\n    arch: Optional[str] = \"\",\n    update_cache: Optional[bool] = False,\n) -> Union[DebianPackage, List[DebianPackage]]:\n    \"\"\"Add a package or list of packages to the system.\n\n    Args:\n        package_names: single package name, or list of package names\n        name: the name(s) of the package(s)\n        version: an (Optional) version as a string. Defaults to the latest known\n        arch: an optional architecture for the package\n        update_cache: whether or not to run `apt-get update` prior to operating\n\n    Raises:\n        TypeError if no package name is given, or explicit version is set for multiple packages\n        PackageNotFoundError if the package is not in the cache.\n        PackageError if packages fail to install\n    \"\"\"\n    cache_refreshed = False\n    if update_cache:\n        update()\n        cache_refreshed = True\n\n    packages = {\"success\": [], \"retry\": [], \"failed\": []}\n\n    package_names = [package_names] if isinstance(package_names, str) else package_names\n    if not package_names:\n        raise TypeError(\"Expected at least one package name to add, received zero!\")\n\n    if len(package_names) != 1 and version:\n        raise TypeError(\n            \"Explicit version should not be set if more than one package is being added!\"\n        )\n\n    for p in package_names:\n        pkg, success = _add(p, version, arch)\n        if success:\n            packages[\"success\"].append(pkg)\n        else:\n            logger.warning(\"failed to locate and install/update '%s'\", pkg)\n            packages[\"retry\"].append(p)\n\n    if packages[\"retry\"] and not cache_refreshed:\n        logger.info(\"updating the apt-cache and retrying installation of failed packages.\")\n        update()\n\n        for p in packages[\"retry\"]:\n            pkg, success = _add(p, version, arch)\n            if success:\n                packages[\"success\"].append(pkg)\n            else:\n                packages[\"failed\"].append(p)\n\n    if packages[\"failed\"]:\n        raise PackageError(\"Failed to install packages: {}\".format(\", \".join(packages[\"failed\"])))\n\n    return packages[\"success\"] if len(packages[\"success\"]) > 1 else packages[\"success\"][0]\n\n\ndef _add(\n    name: str,\n    version: Optional[str] = \"\",\n    arch: Optional[str] = \"\",\n) -> Tuple[Union[DebianPackage, str], bool]:\n    \"\"\"Add a package to the system.\n\n    Args:\n        name: the name(s) of the package(s)\n        version: an (Optional) version as a string. Defaults to the latest known\n        arch: an optional architecture for the package\n\n    Returns: a tuple of `DebianPackage` if found, or a :str: if it is not, and\n        a boolean indicating success\n    \"\"\"\n    try:\n        pkg = DebianPackage.from_system(name, version, arch)\n        pkg.ensure(state=PackageState.Present)\n        return pkg, True\n    except PackageNotFoundError:\n        return name, False\n\n\ndef remove_package(\n    package_names: Union[str, List[str]]\n) -> Union[DebianPackage, List[DebianPackage]]:\n    \"\"\"Remove package(s) from the system.\n\n    Args:\n        package_names: the name of a package\n\n    Raises:\n        PackageNotFoundError if the package is not found.\n    \"\"\"\n    packages = []\n\n    package_names = [package_names] if isinstance(package_names, str) else package_names\n    if not package_names:\n        raise TypeError(\"Expected at least one package name to add, received zero!\")\n\n    for p in package_names:\n        try:\n            pkg = DebianPackage.from_installed_package(p)\n            pkg.ensure(state=PackageState.Absent)\n            packages.append(pkg)\n        except PackageNotFoundError:\n            logger.info(\"package '%s' was requested for removal, but it was not installed.\", p)\n\n    # the list of packages will be empty when no package is removed\n    logger.debug(\"packages: '%s'\", packages)\n    return packages[0] if len(packages) == 1 else packages\n\n\ndef update() -> None:\n    \"\"\"Update the apt cache via `apt-get update`.\"\"\"\n    subprocess.run([\"apt-get\", \"update\", \"--error-on=any\"], capture_output=True, check=True)\n\n\ndef import_key(key: str) -> str:\n    \"\"\"Import an ASCII Armor key.\n\n    A Radix64 format keyid is also supported for backwards\n    compatibility. In this case Ubuntu keyserver will be\n    queried for a key via HTTPS by its keyid. This method\n    is less preferable because https proxy servers may\n    require traffic decryption which is equivalent to a\n    man-in-the-middle attack (a proxy server impersonates\n    keyserver TLS certificates and has to be explicitly\n    trusted by the system).\n\n    Args:\n        key: A GPG key in ASCII armor format, including BEGIN\n            and END markers or a keyid.\n\n    Returns:\n        The GPG key filename written.\n\n    Raises:\n        GPGKeyError if the key could not be imported\n    \"\"\"\n    key = key.strip()\n    if \"-\" in key or \"\\n\" in key:\n        # Send everything not obviously a keyid to GPG to import, as\n        # we trust its validation better than our own. eg. handling\n        # comments before the key.\n        logger.debug(\"PGP key found (looks like ASCII Armor format)\")\n        if (\n            \"-----BEGIN PGP PUBLIC KEY BLOCK-----\" in key\n            and \"-----END PGP PUBLIC KEY BLOCK-----\" in key\n        ):\n            logger.debug(\"Writing provided PGP key in the binary format\")\n            key_bytes = key.encode(\"utf-8\")\n            key_name = DebianRepository._get_keyid_by_gpg_key(key_bytes)\n            key_gpg = DebianRepository._dearmor_gpg_key(key_bytes)\n            gpg_key_filename = \"/etc/apt/trusted.gpg.d/{}.gpg\".format(key_name)\n            DebianRepository._write_apt_gpg_keyfile(\n                key_name=gpg_key_filename, key_material=key_gpg\n            )\n            return gpg_key_filename\n        else:\n            raise GPGKeyError(\"ASCII armor markers missing from GPG key\")\n    else:\n        logger.warning(\n            \"PGP key found (looks like Radix64 format). \"\n            \"SECURELY importing PGP key from keyserver; \"\n            \"full key not provided.\"\n        )\n        # as of bionic add-apt-repository uses curl with an HTTPS keyserver URL\n        # to retrieve GPG keys. `apt-key adv` command is deprecated as is\n        # apt-key in general as noted in its manpage. See lp:1433761 for more\n        # history. Instead, /etc/apt/trusted.gpg.d is used directly to drop\n        # gpg\n        key_asc = DebianRepository._get_key_by_keyid(key)\n        # write the key in GPG format so that apt-key list shows it\n        key_gpg = DebianRepository._dearmor_gpg_key(key_asc.encode(\"utf-8\"))\n        gpg_key_filename = \"/etc/apt/trusted.gpg.d/{}.gpg\".format(key)\n        DebianRepository._write_apt_gpg_keyfile(key_name=gpg_key_filename, key_material=key_gpg)\n        return gpg_key_filename\n\n\nclass InvalidSourceError(Error):\n    \"\"\"Exceptions for invalid source entries.\"\"\"\n\n\nclass GPGKeyError(Error):\n    \"\"\"Exceptions for GPG keys.\"\"\"\n\n\nclass DebianRepository:\n    \"\"\"An abstraction to represent a repository.\"\"\"\n\n    def __init__(\n        self,\n        enabled: bool,\n        repotype: str,\n        uri: str,\n        release: str,\n        groups: List[str],\n        filename: Optional[str] = \"\",\n        gpg_key_filename: Optional[str] = \"\",\n        options: Optional[dict] = None,\n    ):\n        self._enabled = enabled\n        self._repotype = repotype\n        self._uri = uri\n        self._release = release\n        self._groups = groups\n        self._filename = filename\n        self._gpg_key_filename = gpg_key_filename\n        self._options = options\n\n    @property\n    def enabled(self):\n        \"\"\"Return whether or not the repository is enabled.\"\"\"\n        return self._enabled\n\n    @property\n    def repotype(self):\n        \"\"\"Return whether it is binary or source.\"\"\"\n        return self._repotype\n\n    @property\n    def uri(self):\n        \"\"\"Return the URI.\"\"\"\n        return self._uri\n\n    @property\n    def release(self):\n        \"\"\"Return which Debian/Ubuntu releases it is valid for.\"\"\"\n        return self._release\n\n    @property\n    def groups(self):\n        \"\"\"Return the enabled package groups.\"\"\"\n        return self._groups\n\n    @property\n    def filename(self):\n        \"\"\"Returns the filename for a repository.\"\"\"\n        return self._filename\n\n    @filename.setter\n    def filename(self, fname: str) -> None:\n        \"\"\"Set the filename used when a repo is written back to disk.\n\n        Args:\n            fname: a filename to write the repository information to.\n        \"\"\"\n        if not fname.endswith(\".list\"):\n            raise InvalidSourceError(\"apt source filenames should end in .list!\")\n\n        self._filename = fname\n\n    @property\n    def gpg_key(self):\n        \"\"\"Returns the path to the GPG key for this repository.\"\"\"\n        return self._gpg_key_filename\n\n    @property\n    def options(self):\n        \"\"\"Returns any additional repo options which are set.\"\"\"\n        return self._options\n\n    def make_options_string(self) -> str:\n        \"\"\"Generate the complete options string for a a repository.\n\n        Combining `gpg_key`, if set, and the rest of the options to find\n        a complex repo string.\n        \"\"\"\n        options = self._options if self._options else {}\n        if self._gpg_key_filename:\n            options[\"signed-by\"] = self._gpg_key_filename\n\n        return (\n            \"[{}] \".format(\" \".join([\"{}={}\".format(k, v) for k, v in options.items()]))\n            if options\n            else \"\"\n        )\n\n    @staticmethod\n    def prefix_from_uri(uri: str) -> str:\n        \"\"\"Get a repo list prefix from the uri, depending on whether a path is set.\"\"\"\n        uridetails = urlparse(uri)\n        path = (\n            uridetails.path.lstrip(\"/\").replace(\"/\", \"-\") if uridetails.path else uridetails.netloc\n        )\n        return \"/etc/apt/sources.list.d/{}\".format(path)\n\n    @staticmethod\n    def from_repo_line(repo_line: str, write_file: Optional[bool] = True) -> \"DebianRepository\":\n        \"\"\"Instantiate a new `DebianRepository` a `sources.list` entry line.\n\n        Args:\n            repo_line: a string representing a repository entry\n            write_file: boolean to enable writing the new repo to disk\n        \"\"\"\n        repo = RepositoryMapping._parse(repo_line, \"UserInput\")\n        fname = \"{}-{}.list\".format(\n            DebianRepository.prefix_from_uri(repo.uri), repo.release.replace(\"/\", \"-\")\n        )\n        repo.filename = fname\n\n        options = repo.options if repo.options else {}\n        if repo.gpg_key:\n            options[\"signed-by\"] = repo.gpg_key\n\n        # For Python 3.5 it's required to use sorted in the options dict in order to not have\n        # different results in the order of the options between executions.\n        options_str = (\n            \"[{}] \".format(\" \".join([\"{}={}\".format(k, v) for k, v in sorted(options.items())]))\n            if options\n            else \"\"\n        )\n\n        if write_file:\n            with open(fname, \"wb\") as f:\n                f.write(\n                    (\n                        \"{}\".format(\"#\" if not repo.enabled else \"\")\n                        + \"{} {}{} \".format(repo.repotype, options_str, repo.uri)\n                        + \"{} {}\\n\".format(repo.release, \" \".join(repo.groups))\n                    ).encode(\"utf-8\")\n                )\n\n        return repo\n\n    def disable(self) -> None:\n        \"\"\"Remove this repository from consideration.\n\n        Disable it instead of removing from the repository file.\n        \"\"\"\n        searcher = \"{} {}{} {}\".format(\n            self.repotype, self.make_options_string(), self.uri, self.release\n        )\n        for line in fileinput.input(self._filename, inplace=True):\n            if re.match(r\"^{}\\s\".format(re.escape(searcher)), line):\n                print(\"# {}\".format(line), end=\"\")\n            else:\n                print(line, end=\"\")\n\n    def import_key(self, key: str) -> None:\n        \"\"\"Import an ASCII Armor key.\n\n        A Radix64 format keyid is also supported for backwards\n        compatibility. In this case Ubuntu keyserver will be\n        queried for a key via HTTPS by its keyid. This method\n        is less preferable because https proxy servers may\n        require traffic decryption which is equivalent to a\n        man-in-the-middle attack (a proxy server impersonates\n        keyserver TLS certificates and has to be explicitly\n        trusted by the system).\n\n        Args:\n          key: A GPG key in ASCII armor format,\n                      including BEGIN and END markers or a keyid.\n\n        Raises:\n          GPGKeyError if the key could not be imported\n        \"\"\"\n        self._gpg_key_filename = import_key(key)\n\n    @staticmethod\n    def _get_keyid_by_gpg_key(key_material: bytes) -> str:\n        \"\"\"Get a GPG key fingerprint by GPG key material.\n\n        Gets a GPG key fingerprint (40-digit, 160-bit) by the ASCII armor-encoded\n        or binary GPG key material. Can be used, for example, to generate file\n        names for keys passed via charm options.\n        \"\"\"\n        # Use the same gpg command for both Xenial and Bionic\n        cmd = [\"gpg\", \"--with-colons\", \"--with-fingerprint\"]\n        ps = subprocess.run(\n            cmd,\n            stdout=PIPE,\n            stderr=PIPE,\n            input=key_material,\n        )\n        out, err = ps.stdout.decode(), ps.stderr.decode()\n        if \"gpg: no valid OpenPGP data found.\" in err:\n            raise GPGKeyError(\"Invalid GPG key material provided\")\n        # from gnupg2 docs: fpr :: Fingerprint (fingerprint is in field 10)\n        return re.search(r\"^fpr:{9}([0-9A-F]{40}):$\", out, re.MULTILINE).group(1)\n\n    @staticmethod\n    def _get_key_by_keyid(keyid: str) -> str:\n        \"\"\"Get a key via HTTPS from the Ubuntu keyserver.\n\n        Different key ID formats are supported by SKS keyservers (the longer ones\n        are more secure, see \"dead beef attack\" and https://evil32.com/). Since\n        HTTPS is used, if SSLBump-like HTTPS proxies are in place, they will\n        impersonate keyserver.ubuntu.com and generate a certificate with\n        keyserver.ubuntu.com in the CN field or in SubjAltName fields of a\n        certificate. If such proxy behavior is expected it is necessary to add the\n        CA certificate chain containing the intermediate CA of the SSLBump proxy to\n        every machine that this code runs on via ca-certs cloud-init directive (via\n        cloudinit-userdata model-config) or via other means (such as through a\n        custom charm option). Also note that DNS resolution for the hostname in a\n        URL is done at a proxy server - not at the client side.\n        8-digit (32 bit) key ID\n        https://keyserver.ubuntu.com/pks/lookup?search=0x4652B4E6\n        16-digit (64 bit) key ID\n        https://keyserver.ubuntu.com/pks/lookup?search=0x6E85A86E4652B4E6\n        40-digit key ID:\n        https://keyserver.ubuntu.com/pks/lookup?search=0x35F77D63B5CEC106C577ED856E85A86E4652B4E6\n\n        Args:\n          keyid: An 8, 16 or 40 hex digit keyid to find a key for\n\n        Returns:\n          A string contining key material for the specified GPG key id\n\n\n        Raises:\n          subprocess.CalledProcessError\n        \"\"\"\n        # options=mr - machine-readable output (disables html wrappers)\n        keyserver_url = (\n            \"https://keyserver.ubuntu.com\" \"/pks/lookup?op=get&options=mr&exact=on&search=0x{}\"\n        )\n        curl_cmd = [\"curl\", keyserver_url.format(keyid)]\n        # use proxy server settings in order to retrieve the key\n        return check_output(curl_cmd).decode()\n\n    @staticmethod\n    def _dearmor_gpg_key(key_asc: bytes) -> bytes:\n        \"\"\"Convert a GPG key in the ASCII armor format to the binary format.\n\n        Args:\n          key_asc: A GPG key in ASCII armor format.\n\n        Returns:\n          A GPG key in binary format as a string\n\n        Raises:\n          GPGKeyError\n        \"\"\"\n        ps = subprocess.run([\"gpg\", \"--dearmor\"], stdout=PIPE, stderr=PIPE, input=key_asc)\n        out, err = ps.stdout, ps.stderr.decode()\n        if \"gpg: no valid OpenPGP data found.\" in err:\n            raise GPGKeyError(\n                \"Invalid GPG key material. Check your network setup\"\n                \" (MTU, routing, DNS) and/or proxy server settings\"\n                \" as well as destination keyserver status.\"\n            )\n        else:\n            return out\n\n    @staticmethod\n    def _write_apt_gpg_keyfile(key_name: str, key_material: bytes) -> None:\n        \"\"\"Write GPG key material into a file at a provided path.\n\n        Args:\n          key_name: A key name to use for a key file (could be a fingerprint)\n          key_material: A GPG key material (binary)\n        \"\"\"\n        with open(key_name, \"wb\") as keyf:\n            keyf.write(key_material)\n\n\nclass RepositoryMapping(Mapping):\n    \"\"\"An representation of known repositories.\n\n    Instantiation of `RepositoryMapping` will iterate through the\n    filesystem, parse out repository files in `/etc/apt/...`, and create\n    `DebianRepository` objects in this list.\n\n    Typical usage:\n\n        repositories = apt.RepositoryMapping()\n        repositories.add(DebianRepository(\n            enabled=True, repotype=\"deb\", uri=\"https://example.com\", release=\"focal\",\n            groups=[\"universe\"]\n        ))\n    \"\"\"\n\n    def __init__(self):\n        self._repository_map = {}\n        # Repositories that we're adding -- used to implement mode param\n        self.default_file = \"/etc/apt/sources.list\"\n\n        # read sources.list if it exists\n        if os.path.isfile(self.default_file):\n            self.load(self.default_file)\n\n        # read sources.list.d\n        for file in glob.iglob(\"/etc/apt/sources.list.d/*.list\"):\n            self.load(file)\n\n    def __contains__(self, key: str) -> bool:\n        \"\"\"Magic method for checking presence of repo in mapping.\"\"\"\n        return key in self._repository_map\n\n    def __len__(self) -> int:\n        \"\"\"Return number of repositories in map.\"\"\"\n        return len(self._repository_map)\n\n    def __iter__(self) -> Iterable[DebianRepository]:\n        \"\"\"Return iterator for RepositoryMapping.\"\"\"\n        return iter(self._repository_map.values())\n\n    def __getitem__(self, repository_uri: str) -> DebianRepository:\n        \"\"\"Return a given `DebianRepository`.\"\"\"\n        return self._repository_map[repository_uri]\n\n    def __setitem__(self, repository_uri: str, repository: DebianRepository) -> None:\n        \"\"\"Add a `DebianRepository` to the cache.\"\"\"\n        self._repository_map[repository_uri] = repository\n\n    def load(self, filename: str):\n        \"\"\"Load a repository source file into the cache.\n\n        Args:\n          filename: the path to the repository file\n        \"\"\"\n        parsed = []\n        skipped = []\n        with open(filename, \"r\") as f:\n            for n, line in enumerate(f):\n                try:\n                    repo = self._parse(line, filename)\n                except InvalidSourceError:\n                    skipped.append(n)\n                else:\n                    repo_identifier = \"{}-{}-{}\".format(repo.repotype, repo.uri, repo.release)\n                    self._repository_map[repo_identifier] = repo\n                    parsed.append(n)\n                    logger.debug(\"parsed repo: '%s'\", repo_identifier)\n\n        if skipped:\n            skip_list = \", \".join(str(s) for s in skipped)\n            logger.debug(\"skipped the following lines in file '%s': %s\", filename, skip_list)\n\n        if parsed:\n            logger.info(\"parsed %d apt package repositories\", len(parsed))\n        else:\n            raise InvalidSourceError(\"all repository lines in '{}' were invalid!\".format(filename))\n\n    @staticmethod\n    def _parse(line: str, filename: str) -> DebianRepository:\n        \"\"\"Parse a line in a sources.list file.\n\n        Args:\n          line: a single line from `load` to parse\n          filename: the filename being read\n\n        Raises:\n          InvalidSourceError if the source type is unknown\n        \"\"\"\n        enabled = True\n        repotype = uri = release = gpg_key = \"\"\n        options = {}\n        groups = []\n\n        line = line.strip()\n        if line.startswith(\"#\"):\n            enabled = False\n            line = line[1:]\n\n        # Check for \"#\" in the line and treat a part after it as a comment then strip it off.\n        i = line.find(\"#\")\n        if i > 0:\n            line = line[:i]\n\n        # Split a source into substrings to initialize a new repo.\n        source = line.strip()\n        if source:\n            # Match any repo options, and get a dict representation.\n            for v in re.findall(OPTIONS_MATCHER, source):\n                opts = dict(o.split(\"=\") for o in v.strip(\"[]\").split())\n                # Extract the 'signed-by' option for the gpg_key\n                gpg_key = opts.pop(\"signed-by\", \"\")\n                options = opts\n\n            # Remove any options from the source string and split the string into chunks\n            source = re.sub(OPTIONS_MATCHER, \"\", source)\n            chunks = source.split()\n\n            # Check we've got a valid list of chunks\n            if len(chunks) < 3 or chunks[0] not in VALID_SOURCE_TYPES:\n                raise InvalidSourceError(\"An invalid sources line was found in %s!\", filename)\n\n            repotype = chunks[0]\n            uri = chunks[1]\n            release = chunks[2]\n            groups = chunks[3:]\n\n            return DebianRepository(\n                enabled, repotype, uri, release, groups, filename, gpg_key, options\n            )\n        else:\n            raise InvalidSourceError(\"An invalid sources line was found in %s!\", filename)\n\n    def add(self, repo: DebianRepository, default_filename: Optional[bool] = False) -> None:\n        \"\"\"Add a new repository to the system.\n\n        Args:\n          repo: a `DebianRepository` object\n          default_filename: an (Optional) filename if the default is not desirable\n        \"\"\"\n        new_filename = \"{}-{}.list\".format(\n            DebianRepository.prefix_from_uri(repo.uri), repo.release.replace(\"/\", \"-\")\n        )\n\n        fname = repo.filename or new_filename\n\n        options = repo.options if repo.options else {}\n        if repo.gpg_key:\n            options[\"signed-by\"] = repo.gpg_key\n\n        with open(fname, \"wb\") as f:\n            f.write(\n                (\n                    \"{}\".format(\"#\" if not repo.enabled else \"\")\n                    + \"{} {}{} \".format(repo.repotype, repo.make_options_string(), repo.uri)\n                    + \"{} {}\\n\".format(repo.release, \" \".join(repo.groups))\n                ).encode(\"utf-8\")\n            )\n\n        self._repository_map[\"{}-{}-{}\".format(repo.repotype, repo.uri, repo.release)] = repo\n\n    def disable(self, repo: DebianRepository) -> None:\n        \"\"\"Remove a repository. Disable by default.\n\n        Args:\n          repo: a `DebianRepository` to disable\n        \"\"\"\n        searcher = \"{} {}{} {}\".format(\n            repo.repotype, repo.make_options_string(), repo.uri, repo.release\n        )\n\n        for line in fileinput.input(repo.filename, inplace=True):\n            if re.match(r\"^{}\\s\".format(re.escape(searcher)), line):\n                print(\"# {}\".format(line), end=\"\")\n            else:\n                print(line, end=\"\")\n\n        self._repository_map[\"{}-{}-{}\".format(repo.repotype, repo.uri, repo.release)] = repo\n", "haproxy_route.py": "# pylint: disable=too-many-lines\n\"\"\"Haproxy-route interface library.\n\n## Getting Started\n\nTo get started using the library, you just need to fetch the library using `charmcraft`.\n\n```shell\ncd some-charm\ncharmcraft fetch-lib charms.haproxy.v0.haproxy_route\n```\n\nIn the `metadata.yaml` of the charm, add the following:\n\n```yaml\nrequires:\n    backend:\n        interface: haproxy-route\n        limit: 1\n```\n\nThen, to initialise the library:\n\n```python\nfrom charms.haproxy.v0.haproxy_route import HaproxyRouteRequirer\n\nclass SomeCharm(CharmBase):\n  def __init__(self, *args):\n    # ...\n\n    # There are 2 ways you can use the requirer implementation:\n    # 1. To initialize the requirer with parameters:\n    self.haproxy_route_requirer = HaproxyRouteRequirer(self,\n        address=<required>,\n        port=<required>,\n        paths=<optional>,\n        subdomains=<optional>,\n        path_rewrite_expressions=<optional>, list of path rewrite expressions,\n        query_rewrite_expressions=<optional>, list of query rewrite expressions,\n        header_rewrites=<optional>, map of {<header_name>: <list of rewrite_expressions>,\n        check_interval=<optional>,\n        check_rise=<optional>,\n        check_fall=<optional>,\n        check_paths=<optional>,\n        load_balancing_algorithm=<optional>, defaults to \"leastconn\",\n        load_balancing_cookie=<optional>, only used when load_balancing_algorithm is cookie\n        rate_limit_connections_per_minutes=<optional>,\n        rate_limit_policy=<optional>,\n        upload_limit=<optional>,\n        download_limit=<optional>,\n        retry_count=<optional>,\n        retry_interval=<optional>,\n        retry_redispatch=<optional>,\n        deny_paths=<optional>,\n        server_timeout=<optional>,\n        connect_timeout=<optional>,\n        queue_timeout=<optional>,\n        server_maxconn=<optional>,\n    )\n\n    # 2.To initialize the requirer with no parameters, i.e\n    # self.haproxy_route_requirer = HaproxyRouteRequirer(self)\n    # This will simply initialize the requirer class and it won't perfom any action.\n\n    # Afterwards regardless of how you initialized the requirer you can call the\n    # provide_haproxy_route_requirements method anywhere in your charm to update the requirer data.\n    # The method takes the same number of parameters as the requirer class.\n    # provide_haproxy_route_requirements(address=, port=, ...)\n\n    self.framework.observe(\n        self.framework.on.config_changed, self._on_config_changed\n    )\n    self.framework.observe(\n        self.haproxy_route_requirer.on.ready, self._on_endpoints_ready\n    )\n    self.framework.observe(\n        self.haproxy_route_requirer.on.removed, self._on_endpoints_removed\n    )\n\n    def _on_config_changed(self, event: ConfigChangedEvent) -> None:\n        self.haproxy_route_requirer.provide_haproxy_route_requirements(...)\n\n    def _on_endpoints_ready(self, _: EventBase) -> None:\n        # Handle endpoints ready event\n        ...\n\n    def _on_endpoints_removed(self, _: EventBase) -> None:\n        # Handle endpoints removed event\n        ...\n\n## Using the library as the provider\nThe provider charm should expose the interface as shown below:\n```yaml\nprovides:\n    haproxy-route:\n        interface: haproxy-route\n```\nNote that this interface supports relating to multiple endpoints.\n\nThen, to initialise the library:\n```python\nfrom charms.haproxy.v0.haproxy_route import HaproxyRouteRequirer\n\nclass SomeCharm(CharmBase):\n    self.haproxy_route_provider = HaproxyRouteProvider(self)\n    self.framework.observe(\n        self.haproxy_route_provider.on.data_available, self._on_haproxy_route_data_available\n    )\n\n    def _on_haproxy_route_data_available(self, event: EventBase) -> None:\n        data = self.haproxy_route_provider.get_data(self.haproxy_route_provider.relations)\n        ...\n\"\"\"\n\nimport json\nimport logging\nfrom enum import Enum\nfrom typing import Any, MutableMapping, Optional, cast\n\nfrom ops import CharmBase, ModelError, RelationBrokenEvent\nfrom ops.charm import CharmEvents\nfrom ops.framework import EventBase, EventSource, Object\nfrom ops.model import Relation\nfrom pydantic import (\n    AnyHttpUrl,\n    BaseModel,\n    ConfigDict,\n    Field,\n    IPvAnyAddress,\n    ValidationError,\n    field_validator,\n    model_validator,\n)\nfrom pydantic.dataclasses import dataclass\nfrom typing_extensions import Self\n\n# The unique Charmhub library identifier, never change it\nLIBID = \"08b6347482f6455486b5f5bb4dc4e6cf\"\n\n# Increment this major API version when introducing breaking changes\nLIBAPI = 0\n\n# Increment this PATCH version before using `charmcraft publish-lib` or reset\n# to 0 if you are raising the major API version\nLIBPATCH = 1\n\nlogger = logging.getLogger(__name__)\nHAPROXY_ROUTE_RELATION_NAME = \"haproxy-route\"\n\n\nclass DataValidationError(Exception):\n    \"\"\"Raised when data validation fails.\"\"\"\n\n\nclass HaproxyRouteInvalidRelationDataError(Exception):\n    \"\"\"Rasied when data validation of the haproxy-route relation fails.\"\"\"\n\n\nclass _DatabagModel(BaseModel):\n    \"\"\"Base databag model.\n\n    Attrs:\n        model_config: pydantic model configuration.\n    \"\"\"\n\n    model_config = ConfigDict(\n        # tolerate additional keys in databag\n        extra=\"ignore\",\n        # Allow instantiating this class by field name (instead of forcing alias).\n        populate_by_name=True,\n        # Custom config key: whether to nest the whole datastructure (as json)\n        # under a field or spread it out at the toplevel.\n        _NEST_UNDER=None,\n    )  # type: ignore\n    \"\"\"Pydantic config.\"\"\"\n\n    @classmethod\n    def load(cls, databag: MutableMapping) -> \"_DatabagModel\":\n        \"\"\"Load this model from a Juju json databag.\n\n        Args:\n            databag: Databag content.\n\n        Raises:\n            DataValidationError: When model validation failed.\n\n        Returns:\n            _DatabagModel: The validated model.\n        \"\"\"\n        nest_under = cls.model_config.get(\"_NEST_UNDER\")\n        if nest_under:\n            return cls.model_validate(json.loads(databag[nest_under]))\n\n        try:\n            data = {\n                k: json.loads(v)\n                for k, v in databag.items()\n                # Don't attempt to parse model-external values\n                if k in {(f.alias or n) for n, f in cls.model_fields.items()}\n            }\n        except json.JSONDecodeError as e:\n            msg = f\"invalid databag contents: expecting json. {databag}\"\n            logger.error(msg)\n            raise DataValidationError(msg) from e\n\n        try:\n            return cls.model_validate_json(json.dumps(data))\n        except ValidationError as e:\n            msg = f\"failed to validate databag: {databag}\"\n            logger.error(msg, exc_info=True)\n            raise DataValidationError(msg) from e\n\n    @classmethod\n    def from_dict(cls, values: dict) -> \"_DatabagModel\":\n        \"\"\"Load this model from a dict.\n\n        Args:\n            values: Dict values.\n\n        Raises:\n            DataValidationError: When model validation failed.\n\n        Returns:\n            _DatabagModel: The validated model.\n        \"\"\"\n        try:\n            logger.info(\"Loading values from dictionary: %s\", values)\n            return cls.model_validate(values)\n        except ValidationError as e:\n            msg = f\"failed to validate: {values}\"\n            logger.debug(msg, exc_info=True)\n            raise DataValidationError(msg) from e\n\n    def dump(\n        self, databag: Optional[MutableMapping] = None, clear: bool = True\n    ) -> Optional[MutableMapping]:\n        \"\"\"Write the contents of this model to Juju databag.\n\n        Args:\n            databag: The databag to write to.\n            clear: Whether to clear the databag before writing.\n\n        Returns:\n            MutableMapping: The databag.\n        \"\"\"\n        if clear and databag:\n            databag.clear()\n\n        if databag is None:\n            databag = {}\n        nest_under = self.model_config.get(\"_NEST_UNDER\")\n        if nest_under:\n            databag[nest_under] = self.model_dump_json(\n                by_alias=True,\n                # skip keys whose values are default\n                exclude_defaults=True,\n            )\n            return databag\n\n        dct = self.model_dump(mode=\"json\", by_alias=True, exclude_defaults=True)\n        databag.update({k: json.dumps(v) for k, v in dct.items()})\n        return databag\n\n\nclass ServerHealthCheck(BaseModel):\n    \"\"\"Configuration model for backend server health checks.\n\n    Attributes:\n        interval: Number of seconds between consecutive health check attempts.\n        rise: Number of consecutive successful health checks required for up.\n        fall: Number of consecutive failed health checks required for DOWN.\n        path: List of URL paths to use for HTTP health checks.\n    \"\"\"\n\n    interval: int = Field(\n        description=\"The interval (in seconds) between health checks.\", default=60\n    )\n    rise: int = Field(\n        description=\"How many successful health checks before server is considered up.\", default=2\n    )\n    fall: int = Field(\n        description=\"How many failed health checks before server is considered down.\", default=3\n    )\n    path: Optional[str] = Field(description=\"The health check path.\", default=None)\n\n\n# tarpit is not yet implemented\nclass RateLimitPolicy(Enum):\n    \"\"\"Enum of possible rate limiting policies.\n\n    Attrs:\n        DENY: deny a client's HTTP request to return a 403 Forbidden error.\n        REJECT: closes the connection immediately without sending a response.\n        SILENT: disconnects immediately without notifying the client\n            that the connection has been closed.\n    \"\"\"\n\n    DENY = \"deny\"\n    REJECT = \"reject\"\n    SILENT = \"silent-drop\"\n\n\nclass RateLimit(BaseModel):\n    \"\"\"Configuration model for connection rate limiting.\n\n    Attributes:\n        connections_per_minute: Number of connections allowed per minute for a client.\n        policy: Action to take when the rate limit is exceeded.\n    \"\"\"\n\n    connections_per_minute: int = Field(description=\"How many connections are allowed per minute.\")\n    policy: RateLimitPolicy = Field(\n        description=\"Configure the rate limit policy.\", default=RateLimitPolicy.DENY\n    )\n\n\nclass LoadBalancingAlgorithm(Enum):\n    \"\"\"Enum of possible http_route types.\n\n    Attrs:\n        LEASTCONN: The server with the lowest number of connections receives the connection.\n        SRCIP: Load balance using the hash of The source IP address.\n        ROUNDROBIN: Each server is used in turns, according to their weights.\n        COOKIE: Load balance using hash req.cookie(clientid).\n    \"\"\"\n\n    LEASTCONN = \"leastconn\"\n    SRCIP = \"source\"\n    ROUNDROBIN = \"roundrobin\"\n    COOKIE = \"cookie\"\n\n\nclass LoadBalancingConfiguration(BaseModel):\n    \"\"\"Configuration model for load balancing.\n\n    Attributes:\n        algorithm: Algorithm to use for load balancing.\n        cookie: Cookie name to use when algorithm is set to cookie.\n    \"\"\"\n\n    algorithm: LoadBalancingAlgorithm = Field(\n        description=\"Configure the load balancing algorithm for the service.\",\n        default=LoadBalancingAlgorithm.LEASTCONN,\n    )\n    cookie: Optional[str] = Field(\n        description=\"Only used when algorithm is COOKIE. Define the cookie to load balance on.\",\n        default=None,\n    )\n\n\nclass BandwidthLimit(BaseModel):\n    \"\"\"Configuration model for bandwidth rate limiting.\n\n    Attributes:\n        upload: Limit upload speed (bytes per second).\n        download: Limit download speed (bytes per second).\n    \"\"\"\n\n    upload: Optional[int] = Field(description=\"Upload limit (bytes per seconds).\", default=None)\n    download: Optional[int] = Field(\n        description=\"Download limit (bytes per seconds).\", default=None\n    )\n\n\n# retry-on is not yet implemented\nclass Retry(BaseModel):\n    \"\"\"Configuration model for retry.\n\n    Attributes:\n        count: How many times should a request retry.\n        interval: Interval (in seconds) between retries.\n        redispatch: Whether to redispatch failed requests to another server.\n    \"\"\"\n\n    count: int = Field(description=\"How many times should a request retry.\")\n    interval: int = Field(description=\"Interval (in seconds) between retries.\")\n    redispatch: bool = Field(\n        description=\"Whether to redispatch failed requests to another server.\", default=False\n    )\n\n\nclass TimeoutConfiguration(BaseModel):\n    \"\"\"Configuration model for timeout.\n\n    Attributes:\n        server: Timeout for requests from haproxy to backend servers.\n        connect: Timeout for client requests to haproxy.\n        queue: Timeout for requests waiting in the queue after server-maxconn is reached.\n    \"\"\"\n\n    server: int = Field(\n        description=\"Timeout (in seconds) for requests from haproxy to backend servers.\",\n        default=60,\n    )\n    connect: int = Field(\n        description=\"Timeout (in seconds) for client requests to haproxy.\", default=60\n    )\n    queue: int = Field(\n        description=\"Timeout (in seconds) for requests in the queue.\",\n        default=60,\n    )\n\n\nclass HaproxyRewriteMethod(Enum):\n    \"\"\"Enum of possible HTTP rewrite methods.\n\n    Attrs:\n        SET_PATH: The server with the lowest number of connections receives the connection.\n        SET_QUERY: Load balance using the hash of The source IP address.\n        SET_HEADER: Each server is used in turns, according to their weights.\n    \"\"\"\n\n    SET_PATH = \"set-path\"\n    SET_QUERY = \"set-query\"\n    SET_HEADER = \"set-header\"\n\n\nclass RewriteConfiguration(BaseModel):\n    \"\"\"Configuration model for HTTP rewrite.\n\n    Attributes:\n        method: Which rewrite method to apply.One of set-path, set-query, set-header.\n        expression: Regular expression to use with the rewrite method.\n        header: The name of the header to rewrited.\n    \"\"\"\n\n    method: HaproxyRewriteMethod = Field(\n        description=\"Which rewrite method to apply.One of set-path, set-query, set-header.\"\n    )\n    expression: str = Field(description=\"Regular expression to use with the rewrite method.\")\n    header: Optional[str] = Field(description=\"The name of the header to rewrite.\", default=None)\n\n\nclass RequirerApplicationData(_DatabagModel):\n    \"\"\"Configuration model for HAProxy route requirer application data.\n\n    Attributes:\n        service: Name of the service requesting HAProxy routing.\n        ports: List of port numbers on which the service is listening.\n        paths: List of URL paths to route to this service. Defaults to an empty list.\n        subdomains: List of subdomains to route to this service. Defaults to an empty list.\n        rewrites: List of RewriteConfiguration objects defining path, query, or header\n            rewrite rules.\n        check: ServerHealthCheck configuration for monitoring backend health.\n        load_balancing: Configuration for the load balancing strategy.\n        rate_limit: Optional configuration for limiting connection rates.\n        bandwidth_limit: Optional configuration for limiting upload and download bandwidth.\n        retry: Optional configuration for request retry behavior.\n        deny_paths: List of URL paths that should not be routed to the backend.\n        timeout: Configuration for server, client, and queue timeouts.\n        server_maxconn: Optional maximum number of connections per server.\n    \"\"\"\n\n    service: str = Field(description=\"The name of the service.\")\n    ports: list[int] = Field(description=\"The list of ports listening for this service.\")\n    paths: list[str] = Field(description=\"The list of paths to route to this service.\", default=[])\n    subdomains: list[str] = Field(\n        description=\"The list of subdomains to route to this service.\", default=[]\n    )\n    rewrites: list[RewriteConfiguration] = Field(\n        description=\"The list of path rewrite rules.\", default=[]\n    )\n    check: ServerHealthCheck = Field(\n        description=\"Configure health check for the service.\",\n        default=ServerHealthCheck(),\n    )\n    load_balancing: LoadBalancingConfiguration = Field(\n        description=\"Configure loadbalancing.\", default=LoadBalancingConfiguration()\n    )\n    rate_limit: Optional[RateLimit] = Field(\n        description=\"Configure rate limit for the service.\", default=None\n    )\n    bandwidth_limit: BandwidthLimit = Field(\n        description=\"Configure bandwidth limit for the service.\", default=BandwidthLimit()\n    )\n    retry: Optional[Retry] = Field(\n        description=\"Configure retry for incoming requests.\", default=None\n    )\n    deny_paths: list[str] = Field(\n        description=\"Configure path that should not be routed to the backend\", default=[]\n    )\n    timeout: TimeoutConfiguration = Field(\n        description=\"Configure timeout\",\n        default=TimeoutConfiguration(),\n    )\n    server_maxconn: Optional[int] = Field(\n        description=\"Configure maximum connection per server\", default=None\n    )\n\n    @field_validator(\"load_balancing\")\n    @classmethod\n    def validate_load_balancing_configuration(\n        cls, configuration: LoadBalancingConfiguration\n    ) -> LoadBalancingConfiguration:\n        \"\"\"Validate the parsed load balancing configuration.\n\n        Args:\n            configuration: The configuration to validate.\n\n        Raises:\n            ValueError: When cookie is not set under COOKIE load balancing mode.\n\n        Returns:\n            LoadBalancingConfiguration: The validated configuration.\n        \"\"\"\n        if configuration.algorithm == LoadBalancingAlgorithm.COOKIE and not configuration.cookie:\n            raise ValueError(\"cookie must be set if load balacing algorithm is COOKIE.\")\n        return configuration\n\n    @field_validator(\"rewrites\")\n    @classmethod\n    def validate_rewrites(cls, rewrites: list[RewriteConfiguration]) -> list[RewriteConfiguration]:\n        \"\"\"Validate the parsed list of rewrite configurations.\n\n        Args:\n            rewrites: The configurations to validate.\n\n        Raises:\n            ValueError: When header is not set under SET_HEADER rewrite method.\n\n        Returns:\n            list[RewriteConfiguration]: The validated configurations.\n        \"\"\"\n        for rewrite in rewrites:\n            if rewrite.method == HaproxyRewriteMethod.SET_HEADER and not rewrite.method:\n                raise ValueError(\"header must be set if rewrite method is SET_HEADER.\")\n        return rewrites\n\n\nclass HaproxyRouteProviderAppData(_DatabagModel):\n    \"\"\"haproxy-route provider databag schema.\n\n    Attributes:\n        endpoints: The list of proxied endpoints that maps to the backend.\n    \"\"\"\n\n    endpoints: list[AnyHttpUrl]\n\n\nclass RequirerUnitData(_DatabagModel):\n    \"\"\"haproxy-route requirer unit data.\n\n    Attributes:\n        address: IP address of the unit.\n    \"\"\"\n\n    address: IPvAnyAddress = Field(description=\"IP address of the unit.\")\n\n\n@dataclass\nclass HaproxyRouteRequirerData:\n    \"\"\"haproxy-route requirer data.\n\n    Attributes:\n        relation: Relation instance.\n        application_data: Application data.\n        units_data: Units data\n    \"\"\"\n\n    relation: Relation\n    application_data: RequirerApplicationData\n    units_data: list[RequirerUnitData]\n\n\n@dataclass\nclass HaproxyRouteRequirersData:\n    \"\"\"haproxy-route requirers data.\n\n    Attributes:\n        requirers_data: List of requirer data.\n    \"\"\"\n\n    requirers_data: list[HaproxyRouteRequirerData]\n\n    @model_validator(mode=\"after\")\n    def check_services_unique(self) -> Self:\n        \"\"\"Check that requirers define unique services.\n\n        Raises:\n            ValidationError: When requirers declared duplicate services.\n\n        Returns:\n            The validated model.\n        \"\"\"\n        services = [\n            requirer_data.application_data.service for requirer_data in self.requirers_data\n        ]\n        if len(services) != len(set(services)):\n            raise ValidationError(\"Services declaration by requirers must be unique.\")\n\n        return self\n\n\nclass HaproxyRouteDataAvailableEvent(EventBase):\n    \"\"\"HaproxyRouteDataAvailableEvent custom event.\n\n    This event indicates that the requirers data are available.\n    \"\"\"\n\n\nclass HaproxyRouteDataRemovedEvent(EventBase):\n    \"\"\"HaproxyRouteDataRemovedEvent custom event.\n\n    This event indicates that one of the endpoints was removed.\n    \"\"\"\n\n\nclass HaproxyRouteProviderEvents(CharmEvents):\n    \"\"\"List of events that the TLS Certificates requirer charm can leverage.\n\n    Attributes:\n        data_available: This event indicates that\n            the haproxy-route endpoints are available.\n        data_removed: This event indicates that one of the endpoints was removed.\n    \"\"\"\n\n    data_available = EventSource(HaproxyRouteDataAvailableEvent)\n    data_removed = EventSource(HaproxyRouteDataRemovedEvent)\n\n\nclass HaproxyRouteProvider(Object):\n    \"\"\"Haproxy-route interface provider implementation.\n\n    Attributes:\n        on: Custom events of the provider.\n        relations: Related appliations.\n    \"\"\"\n\n    on = HaproxyRouteProviderEvents()\n\n    def __init__(\n        self,\n        charm: CharmBase,\n        relation_name: str = HAPROXY_ROUTE_RELATION_NAME,\n        raise_on_validation_error: bool = False,\n    ) -> None:\n        \"\"\"Initialize the HaproxyRouteProvider.\n\n        Args:\n            charm: The charm that is instantiating the library.\n            relation_name: The name of the relation.\n            raise_on_validation_error: Whether the library should raise\n                HaproxyRouteInvalidRelationDataError when requirer data validation fails.\n                If this is set to True the provider charm needs to also catch and handle the\n                thrown exception.\n        \"\"\"\n        super().__init__(charm, relation_name)\n\n        self._relation_name = relation_name\n        self.charm = charm\n        self.raise_on_validation_error = raise_on_validation_error\n        on = self.charm.on\n        self.framework.observe(on[self._relation_name].relation_created, self._configure)\n        self.framework.observe(on[self._relation_name].relation_changed, self._configure)\n        self.framework.observe(on[self._relation_name].relation_broken, self._on_endpoint_removed)\n        self.framework.observe(\n            on[self._relation_name].relation_departed, self._on_endpoint_removed\n        )\n\n    @property\n    def relations(self) -> list[Relation]:\n        \"\"\"The list of Relation instances associated with this endpoint.\"\"\"\n        return list(self.charm.model.relations[self._relation_name])\n\n    def _configure(self, _event: EventBase) -> None:\n        \"\"\"Handle relation events.\"\"\"\n        if relations := self.relations:\n            # Only for data validation\n            _ = self.get_data(relations)\n            self.on.data_available.emit()\n\n    def _on_endpoint_removed(self, _: EventBase) -> None:\n        \"\"\"Handle relation broken/departed events.\"\"\"\n        self.on.data_removed.emit()\n\n    def get_data(self, relations: list[Relation]) -> HaproxyRouteRequirersData:\n        \"\"\"Fetch requirer data.\n\n        Args:\n            relations: A list of Relation instances to fetch data from.\n\n        Raises:\n            HaproxyRouteInvalidRelationDataError: When requirer data validation fails.\n\n        Returns:\n            HaproxyRouteRequirersData: Validated data from all haproxy-route requirers.\n        \"\"\"\n        requirers_data: list[HaproxyRouteRequirerData] = []\n        for relation in relations:\n            try:\n                application_data = self._get_requirer_application_data(relation)\n                units_data = self._get_requirer_units_data(relation)\n                haproxy_route_requirer_data = HaproxyRouteRequirerData(\n                    application_data=application_data, units_data=units_data, relation=relation\n                )\n                requirers_data.append(haproxy_route_requirer_data)\n            except DataValidationError as exc:\n                if self.raise_on_validation_error:\n                    logger.error(\n                        \"haproxy-route data validation failed for relation %s: %s\",\n                        relation,\n                        str(exc),\n                    )\n                    raise HaproxyRouteInvalidRelationDataError(\n                        f\"haproxy-route data validation failed for relation: {relation}\"\n                    ) from exc\n                continue\n        return HaproxyRouteRequirersData(requirers_data=requirers_data)\n\n    def _get_requirer_units_data(self, relation: Relation) -> list[RequirerUnitData]:\n        \"\"\"Fetch and validate the requirer's units data.\n\n        Args:\n            relation: The relation to fetch unit data from.\n\n        Raises:\n            DataValidationError: When unit data validation fails.\n\n        Returns:\n            list[RequirerUnitData]: List of validated unit data from the requirer.\n        \"\"\"\n        requirer_units_data: list[RequirerUnitData] = []\n\n        for unit in relation.units:\n            databag = relation.data[unit]\n            try:\n                data = cast(RequirerUnitData, RequirerUnitData.load(databag))\n                requirer_units_data.append(data)\n            except DataValidationError:\n                logger.error(\"Invalid requirer application data for %s\", unit)\n                raise\n        return requirer_units_data\n\n    def _get_requirer_application_data(self, relation: Relation) -> RequirerApplicationData:\n        \"\"\"Fetch and validate the requirer's application databag.\n\n        Args:\n            relation: The relation to fetch application data from.\n\n        Raises:\n            DataValidationError: When requirer application data validation fails.\n\n        Returns:\n            RequirerApplicationData: Validated application data from the requirer.\n        \"\"\"\n        try:\n            return cast(\n                RequirerApplicationData, RequirerApplicationData.load(relation.data[relation.app])\n            )\n        except DataValidationError:\n            logger.error(\"Invalid requirer application data for %s\", relation.app.name)\n            raise\n\n    def publish_proxied_endpoints(self, endpoints: list[str], relation: Relation) -> None:\n        \"\"\"Publish to the app databag the proxied endpoints.\n\n        Args:\n            endpoints: The list of proxied endpoints to publish.\n            relation: The relation with the requirer application.\n        \"\"\"\n        HaproxyRouteProviderAppData(\n            endpoints=list(map(lambda x: cast(AnyHttpUrl, x), endpoints))\n        ).dump(relation.data[self.charm.app], clear=True)\n\n\nclass HaproxyRouteEnpointsReadyEvent(EventBase):\n    \"\"\"HaproxyRouteEnpointsReadyEvent custom event.\"\"\"\n\n\nclass HaproxyRouteEndpointsRemovedEvent(EventBase):\n    \"\"\"HaproxyRouteEndpointsRemovedEvent custom event.\"\"\"\n\n\nclass HaproxyRouteRequirerEvents(CharmEvents):\n    \"\"\"List of events that the TLS Certificates requirer charm can leverage.\n\n    Attributes:\n        ready: when the provider proxied endpoints are ready.\n        removed: when the provider\n    \"\"\"\n\n    ready = EventSource(HaproxyRouteEnpointsReadyEvent)\n    removed = EventSource(HaproxyRouteEndpointsRemovedEvent)\n\n\nclass HaproxyRouteRequirer(Object):\n    \"\"\"haproxy-route interface requirer implementation.\n\n    Attributes:\n        on: Custom events of the requirer.\n    \"\"\"\n\n    on = HaproxyRouteRequirerEvents()\n\n    # pylint: disable=too-many-arguments,too-many-positional-arguments,too-many-locals\n    def __init__(\n        self,\n        charm: CharmBase,\n        relation_name: str,\n        service: Optional[str] = None,\n        ports: Optional[list[int]] = None,\n        paths: Optional[list[str]] = None,\n        subdomains: Optional[list[str]] = None,\n        check_interval: Optional[int] = None,\n        check_rise: Optional[int] = None,\n        check_fall: Optional[int] = None,\n        check_path: Optional[str] = None,\n        path_rewrite_expressions: Optional[list[str]] = None,\n        query_rewrite_expressions: Optional[list[str]] = None,\n        header_rewrite_expressions: Optional[list[tuple[str, str]]] = None,\n        load_balancing_algorithm: LoadBalancingAlgorithm = LoadBalancingAlgorithm.LEASTCONN,\n        load_balancing_cookie: Optional[str] = None,\n        rate_limit_connections_per_minute: Optional[int] = None,\n        rate_limit_policy: RateLimitPolicy = RateLimitPolicy.DENY,\n        upload_limit: Optional[int] = None,\n        download_limit: Optional[int] = None,\n        retry_count: Optional[int] = None,\n        retry_interval: Optional[int] = None,\n        retry_redispatch: bool = False,\n        deny_paths: Optional[list[str]] = None,\n        server_timeout: int = 60,\n        connect_timeout: int = 60,\n        queue_timeout: int = 60,\n        server_maxconn: Optional[int] = None,\n        unit_address: Optional[str] = None,\n    ) -> None:\n        \"\"\"Initialize the HaproxyRouteRequirer.\n\n        Args:\n            charm: The charm that is instantiating the library.\n            relation_name: The name of the relation to bind to.\n            service: The name of the service to route traffic to.\n            ports: List of ports the service is listening on.\n            paths: List of URL paths to route to this service.\n            subdomains: List of subdomains to route to this service.\n            check_interval: Interval between health checks in seconds.\n            check_rise: Number of successful health checks before server is considered up.\n            check_fall: Number of failed health checks before server is considered down.\n            check_path: The path to use for server health checks.\n            path_rewrite_expressions: List of regex expressions for path rewrites.\n            query_rewrite_expressions: List of regex expressions for query rewrites.\n            header_rewrite_expressions: List of tuples containing header name\n                and rewrite expression.\n            load_balancing_algorithm: Algorithm to use for load balancing.\n            load_balancing_cookie: Cookie name to use when algorithm is set to cookie.\n            rate_limit_connections_per_minute: Maximum connections allowed per minute.\n            rate_limit_policy: Policy to apply when rate limit is reached.\n            upload_limit: Maximum upload bandwidth in bytes per second.\n            download_limit: Maximum download bandwidth in bytes per second.\n            retry_count: Number of times to retry failed requests.\n            retry_interval: Interval between retries in seconds.\n            retry_redispatch: Whether to redispatch failed requests to another server.\n            deny_paths: List of paths that should not be routed to the backend.\n            server_timeout: Timeout for requests from haproxy to backend servers in seconds.\n            connect_timeout: Timeout for client requests to haproxy in seconds.\n            queue_timeout: Timeout for requests waiting in queue in seconds.\n            server_maxconn: Maximum connections per server.\n            unit_address: IP address of the unit (if not provided, will use binding address).\n        \"\"\"\n        super().__init__(charm, relation_name)\n\n        self._relation_name = relation_name\n        self.relation = self.model.get_relation(self._relation_name)\n        self.charm = charm\n        self.app = self.charm.app\n\n        # build the full application data\n        self._application_data = self._generate_application_data(\n            service,\n            ports,\n            paths,\n            subdomains,\n            check_interval,\n            check_rise,\n            check_fall,\n            check_path,\n            path_rewrite_expressions,\n            query_rewrite_expressions,\n            header_rewrite_expressions,\n            load_balancing_algorithm,\n            load_balancing_cookie,\n            rate_limit_connections_per_minute,\n            rate_limit_policy,\n            upload_limit,\n            download_limit,\n            retry_count,\n            retry_interval,\n            retry_redispatch,\n            deny_paths,\n            server_timeout,\n            connect_timeout,\n            queue_timeout,\n            server_maxconn,\n        )\n        self._unit_address = unit_address\n\n        on = self.charm.on\n        self.framework.observe(on[self._relation_name].relation_created, self._configure)\n        self.framework.observe(on[self._relation_name].relation_changed, self._configure)\n        self.framework.observe(on[self._relation_name].relation_broken, self._on_relation_broken)\n\n    def _configure(self, _: EventBase) -> None:\n        \"\"\"Handle relation events.\"\"\"\n        self.update_relation_data()\n        if self.relation and self.get_proxied_endpoints():\n            # This event is only emitted when the provider databag changes\n            # which only happens when relevant changes happened\n            # Additionally this event is purely informational and it's up to the requirer to\n            # fetch the proxied endpoints in their code using get_proxied_endpoints\n            self.on.ready.emit()\n\n    def _on_relation_broken(self, _: RelationBrokenEvent) -> None:\n        \"\"\"Handle relation broken event.\"\"\"\n        self.on.removed.emit()\n\n    # pylint: disable=too-many-arguments,too-many-positional-arguments\n    def provide_haproxy_route_requirements(\n        self,\n        service: str,\n        ports: list[int],\n        paths: Optional[list[str]] = None,\n        subdomains: Optional[list[str]] = None,\n        check_interval: Optional[int] = None,\n        check_rise: Optional[int] = None,\n        check_fall: Optional[int] = None,\n        check_path: Optional[str] = None,\n        path_rewrite_expressions: Optional[list[str]] = None,\n        query_rewrite_expressions: Optional[list[str]] = None,\n        header_rewrite_expressions: Optional[list[tuple[str, str]]] = None,\n        load_balancing_algorithm: LoadBalancingAlgorithm = LoadBalancingAlgorithm.LEASTCONN,\n        load_balancing_cookie: Optional[str] = None,\n        rate_limit_connections_per_minute: Optional[int] = None,\n        rate_limit_policy: RateLimitPolicy = RateLimitPolicy.DENY,\n        upload_limit: Optional[int] = None,\n        download_limit: Optional[int] = None,\n        retry_count: Optional[int] = None,\n        retry_interval: Optional[int] = None,\n        retry_redispatch: bool = False,\n        deny_paths: Optional[list[str]] = None,\n        server_timeout: int = 60,\n        connect_timeout: int = 60,\n        queue_timeout: int = 60,\n        server_maxconn: Optional[int] = None,\n    ) -> None:\n        \"\"\"Update haproxy-route requirements data in the relation.\n\n        Args:\n            service: The name of the service to route traffic to.\n            ports: List of ports the service is listening on.\n            paths: List of URL paths to route to this service.\n            subdomains: List of subdomains to route to this service.\n            check_interval: Interval between health checks in seconds.\n            check_rise: Number of successful health checks before server is considered up.\n            check_fall: Number of failed health checks before server is considered down.\n            check_path: The path to use for server health checks.\n            path_rewrite_expressions: List of regex expressions for path rewrites.\n            query_rewrite_expressions: List of regex expressions for query rewrites.\n            header_rewrite_expressions: List of tuples containing header name\n                and rewrite expression.\n            load_balancing_algorithm: Algorithm to use for load balancing.\n            load_balancing_cookie: Cookie name to use when algorithm is set to cookie.\n            rate_limit_connections_per_minute: Maximum connections allowed per minute.\n            rate_limit_policy: Policy to apply when rate limit is reached.\n            upload_limit: Maximum upload bandwidth in bytes per second.\n            download_limit: Maximum download bandwidth in bytes per second.\n            retry_count: Number of times to retry failed requests.\n            retry_interval: Interval between retries in seconds.\n            retry_redispatch: Whether to redispatch failed requests to another server.\n            deny_paths: List of paths that should not be routed to the backend.\n            server_timeout: Timeout for requests from haproxy to backend servers in seconds.\n            connect_timeout: Timeout for client requests to haproxy in seconds.\n            queue_timeout: Timeout for requests waiting in queue in seconds.\n            server_maxconn: Maximum connections per server.\n        \"\"\"\n        self._application_data = self._generate_application_data(\n            service,\n            ports,\n            paths,\n            subdomains,\n            check_interval,\n            check_rise,\n            check_fall,\n            check_path,\n            path_rewrite_expressions,\n            query_rewrite_expressions,\n            header_rewrite_expressions,\n            load_balancing_algorithm,\n            load_balancing_cookie,\n            rate_limit_connections_per_minute,\n            rate_limit_policy,\n            upload_limit,\n            download_limit,\n            retry_count,\n            retry_interval,\n            retry_redispatch,\n            deny_paths,\n            server_timeout,\n            connect_timeout,\n            queue_timeout,\n            server_maxconn,\n        )\n        self.update_relation_data()\n\n    # pylint: disable=too-many-arguments,too-many-positional-arguments,too-many-locals\n    def _generate_application_data(  # noqa: C901\n        self,\n        service: Optional[str] = None,\n        ports: Optional[list[int]] = None,\n        paths: Optional[list[str]] = None,\n        subdomains: Optional[list[str]] = None,\n        check_interval: Optional[int] = None,\n        check_rise: Optional[int] = None,\n        check_fall: Optional[int] = None,\n        check_path: Optional[str] = None,\n        path_rewrite_expressions: Optional[list[str]] = None,\n        query_rewrite_expressions: Optional[list[str]] = None,\n        header_rewrite_expressions: Optional[list[tuple[str, str]]] = None,\n        load_balancing_algorithm: LoadBalancingAlgorithm = LoadBalancingAlgorithm.LEASTCONN,\n        load_balancing_cookie: Optional[str] = None,\n        rate_limit_connections_per_minute: Optional[int] = None,\n        rate_limit_policy: RateLimitPolicy = RateLimitPolicy.DENY,\n        upload_limit: Optional[int] = None,\n        download_limit: Optional[int] = None,\n        retry_count: Optional[int] = None,\n        retry_interval: Optional[int] = None,\n        retry_redispatch: bool = False,\n        deny_paths: Optional[list[str]] = None,\n        server_timeout: int = 60,\n        connect_timeout: int = 60,\n        queue_timeout: int = 60,\n        server_maxconn: Optional[int] = None,\n    ) -> dict[str, Any]:\n        \"\"\"Generate the complete application data structure.\n\n        Args:\n            service: The name of the service to route traffic to.\n            ports: List of ports the service is listening on.\n            paths: List of URL paths to route to this service.\n            subdomains: List of subdomains to route to this service.\n            check_interval: Interval between health checks in seconds.\n            check_rise: Number of successful health checks before server is considered up.\n            check_fall: Number of failed health checks before server is considered down.\n            check_path: The path to use for server health checks.\n            path_rewrite_expressions: List of regex expressions for path rewrites.\n            query_rewrite_expressions: List of regex expressions for query rewrites.\n            header_rewrite_expressions: List of tuples containing header name and\n                rewrite expression.\n            load_balancing_algorithm: Algorithm to use for load balancing.\n            load_balancing_cookie: Cookie name to use when algorithm is set to cookie.\n            rate_limit_connections_per_minute: Maximum connections allowed per minute.\n            rate_limit_policy: Policy to apply when rate limit is reached.\n            upload_limit: Maximum upload bandwidth in bytes per second.\n            download_limit: Maximum download bandwidth in bytes per second.\n            retry_count: Number of times to retry failed requests.\n            retry_interval: Interval between retries in seconds.\n            retry_redispatch: Whether to redispatch failed requests to another server.\n            deny_paths: List of paths that should not be routed to the backend.\n            server_timeout: Timeout for requests from haproxy to backend servers in seconds.\n            connect_timeout: Timeout for client requests to haproxy in seconds.\n            queue_timeout: Timeout for requests waiting in queue in seconds.\n            server_maxconn: Maximum connections per server.\n\n        Returns:\n            dict: A dictionary containing the complete application data structure.\n        \"\"\"\n        # Apply default value to list parameters to avoid problems with mutable default args.\n        if not ports:\n            ports = []\n        if not paths:\n            paths = []\n        if not subdomains:\n            subdomains = []\n        if not path_rewrite_expressions:\n            path_rewrite_expressions = []\n        if not query_rewrite_expressions:\n            query_rewrite_expressions = []\n        if not header_rewrite_expressions:\n            header_rewrite_expressions = []\n        if not deny_paths:\n            deny_paths = []\n\n        application_data: dict[str, Any] = {\n            \"service\": service,\n            \"ports\": ports,\n            \"paths\": paths,\n            \"subdomains\": subdomains,\n            \"load_balancing\": {\n                \"algorithm\": load_balancing_algorithm,\n                \"cookie\": load_balancing_cookie,\n            },\n            \"timeout\": {\n                \"server\": server_timeout,\n                \"connect\": connect_timeout,\n                \"queue\": queue_timeout,\n            },\n            \"bandwidth_limit\": {\n                \"download\": download_limit,\n                \"upload\": upload_limit,\n            },\n            \"deny_paths\": deny_paths,\n            \"server_maxconn\": server_maxconn,\n            \"rewrites\": self._generate_rewrite_configuration(\n                path_rewrite_expressions,\n                query_rewrite_expressions,\n                header_rewrite_expressions,\n            ),\n        }\n\n        if check := self._generate_server_healthcheck_configuration(\n            check_interval, check_rise, check_fall, check_path\n        ):\n            application_data[\"check\"] = check\n\n        if rate_limit := self._generate_rate_limit_configuration(\n            rate_limit_connections_per_minute, rate_limit_policy\n        ):\n            application_data[\"rate_limit\"] = rate_limit\n\n        if retry := self._generate_retry_configuration(\n            retry_count, retry_interval, retry_redispatch\n        ):\n            application_data[\"retry\"] = retry\n        return application_data\n\n    def _generate_server_healthcheck_configuration(\n        self,\n        interval: Optional[int],\n        rise: Optional[int],\n        fall: Optional[int],\n        path: Optional[str],\n    ) -> dict[str, int | Optional[str]]:\n        \"\"\"Generate configuration for server health checks.\n\n        Args:\n            interval: Time between health checks in seconds.\n            rise: Number of successful checks before marking server as up.\n            fall: Number of failed checks before marking server as down.\n            path: the path to use for health checks.\n\n        Returns:\n            dict[str, int | Optional[str]]: Health check configuration dictionary.\n        \"\"\"\n        server_healthcheck_configuration: dict[str, int | Optional[str]] = {}\n        if interval and rise and fall:\n            server_healthcheck_configuration = {\n                \"interval\": interval,\n                \"rise\": rise,\n                \"fall\": fall,\n                \"path\": path,\n            }\n        return server_healthcheck_configuration\n\n    def _generate_rewrite_configuration(\n        self,\n        path_rewrite_expressions: list[str],\n        query_rewrite_expressions: list[str],\n        header_rewrite_expressions: list[tuple[str, str]],\n    ) -> list[dict[str, str | HaproxyRewriteMethod]]:\n        \"\"\"Generate rewrite configuration from provided expressions.\n\n        Args:\n            path_rewrite_expressions: List of path rewrite expressions.\n            query_rewrite_expressions: List of query rewrite expressions.\n            header_rewrite_expressions: List of header name and expression tuples.\n\n        Returns:\n            list[dict[str, str]]: List of generated rewrite configurations.\n        \"\"\"\n        # rewrite configuration\n        rewrite_configurations: list[dict[str, str | HaproxyRewriteMethod]] = []\n        for expression in path_rewrite_expressions:\n            rewrite_configurations.append(\n                {\"method\": HaproxyRewriteMethod.SET_PATH, \"expression\": expression}\n            )\n        for expression in query_rewrite_expressions:\n            rewrite_configurations.append(\n                {\"method\": HaproxyRewriteMethod.SET_QUERY, \"expression\": expression}\n            )\n        for header, expression in header_rewrite_expressions:\n            rewrite_configurations.append(\n                {\n                    \"method\": HaproxyRewriteMethod.SET_HEADER,\n                    \"expression\": expression,\n                    \"header\": header,\n                }\n            )\n        return rewrite_configurations\n\n    def _generate_rate_limit_configuration(\n        self, rate_limit_connections_per_minute: Optional[int], rate_limit_policy: RateLimitPolicy\n    ) -> dict[str, Any]:\n        \"\"\"Generate rate limit configuration.\n\n        Args:\n            rate_limit_connections_per_minute: Maximum connections allowed per minute.\n            rate_limit_policy: Policy to apply when rate limit is reached.\n\n        Returns:\n            dict[str, Any]: Rate limit configuration, or empty dict if no limits are set.\n        \"\"\"\n        rate_limit_configuration = {}\n        if rate_limit_connections_per_minute:\n            rate_limit_configuration = {\n                \"connections_per_minute\": rate_limit_connections_per_minute,\n                \"policy\": rate_limit_policy,\n            }\n        return rate_limit_configuration\n\n    def _generate_retry_configuration(\n        self, count: Optional[int], interval: Optional[int], redispatch: bool\n    ) -> dict[str, Any]:\n        \"\"\"Generate retry configuration.\n\n        Args:\n            count: Number of times to retry failed requests.\n            interval: Interval between retries in seconds.\n            redispatch: Whether to redispatch failed requests to another server.\n\n        Returns:\n            dict[str, Any]: Retry configuration dictionary, or empty dict if retry not configured.\n        \"\"\"\n        retry_configuration = {}\n        if count and interval:\n            retry_configuration = {\n                \"count\": count,\n                \"interval\": interval,\n                \"redispatch\": redispatch,\n            }\n        return retry_configuration\n\n    def update_relation_data(self) -> None:\n        \"\"\"Update both application and unit data in the relation.\"\"\"\n        if not self._application_data.get(\"service\") and not self._application_data.get(\"ports\"):\n            logger.warning(\"Required field(s) are missing, skipping update of the relation data.\")\n            return\n\n        if relation := self.relation:\n            self._update_application_data(relation)\n            self._update_unit_data(relation)\n\n    def _update_application_data(self, relation: Relation) -> None:\n        \"\"\"Update application data in the relation databag.\n\n        Args:\n            relation: The relation instance.\n        \"\"\"\n        if self.charm.unit.is_leader():\n            application_data = self._prepare_application_data()\n            application_data.dump(relation.data[self.app], clear=True)\n\n    def _update_unit_data(self, relation: Relation) -> None:\n        \"\"\"Prepare and update the unit data in the relation databag.\n\n        Args:\n            relation: The relation instance.\n        \"\"\"\n        unit_data = self._prepare_unit_data()\n        unit_data.dump(relation.data[self.charm.unit], clear=True)\n\n    def _prepare_application_data(self) -> RequirerApplicationData:\n        \"\"\"Prepare and validate the application data.\n\n        Raises:\n            DataValidationError: When validation of application data fails.\n\n        Returns:\n            RequirerApplicationData: The validated application data model.\n        \"\"\"\n        try:\n            return cast(\n                RequirerApplicationData, RequirerApplicationData.from_dict(self._application_data)\n            )\n        except ValidationError as exc:\n            logger.error(\"Validation error when preparing requirer application data.\")\n            raise DataValidationError(\n                \"Validation error when preparing requirer application data.\"\n            ) from exc\n\n    def _prepare_unit_data(self) -> RequirerUnitData:\n        \"\"\"Prepare and validate unit data.\n\n        Raises:\n            DataValidationError: When no address or unit IP is available.\n\n        Returns:\n            RequirerUnitData: The validated unit data model.\n        \"\"\"\n        address = self._unit_address\n        if not address:\n            network_binding = self.charm.model.get_binding(\"juju-info\")\n            if (\n                network_binding is not None\n                and (bind_address := network_binding.network.bind_address) is not None\n            ):\n                address = str(bind_address)\n            else:\n                logger.error(\"No unit IP available.\")\n                raise DataValidationError(\"No unit IP available.\")\n        return RequirerUnitData(address=cast(IPvAnyAddress, address))\n\n    def get_proxied_endpoints(self) -> list[AnyHttpUrl]:\n        \"\"\"The full ingress URL to reach the current unit.\n\n        Returns:\n            The provider URL or None if the URL isn't available yet or is not valid.\n        \"\"\"\n        relation = self.relation\n        if not relation or not relation.app:\n            return []\n\n        # Fetch the provider's app databag\n        try:\n            databag = relation.data[relation.app]\n        except ModelError:\n            logger.exception(\"Error reading remote app data.\")\n            return []\n\n        if not databag:  # not ready yet\n            return []\n\n        try:\n            provider_data = cast(\n                HaproxyRouteProviderAppData, HaproxyRouteProviderAppData.load(databag)\n            )\n            return provider_data.endpoints\n        except DataValidationError:\n            logger.exception(\"Invalid provider url.\")\n            return []\n"}
